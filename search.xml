<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[UMA是如何动态生成Unity的Avatar的]]></title>
    <url>%2F2019%2F11%2F16%2FUMA%E6%98%AF%E5%A6%82%E4%BD%95%E5%8A%A8%E6%80%81%E7%94%9F%E6%88%90Unity%E7%9A%84Avatar%E7%9A%84%2F</url>
    <content type="text"><![CDATA[名词解释 UnityEngine.SkeletonBone: 是一个Struct Details of the Transform name mapped to the skeleton bone of a model and its default position and rotation in the T-pose. 表示模型中的骨骼，其中带有骨骼的名字，还有在TPose时的平移缩放旋转信息。 UnityEngine.HumanBone: 是一个Struct The mapping between a bone in the model and the conceptual bone in the Mecanim human anatomy. 表示模型中的骨骼与Mecanim人体解剖学中的概念性骨骼之间的映射，其中模型中骨骼的名字，和Mecanim骨骼的名字，还有这个定义了这个骨骼的肌肉的旋转极限的HumanLimit。 UnityEngine.HumanLimit: 是一个Struct This class stores the rotation limits that define the muscle for a single human bone. 表示一根骨骼的肌肉的旋转极限。 UMA.UmaTPose: UMA-&gt;Extract T-Pose生成的文件，就是由这个类的实例序列化来的。 是一个Classbaok UMA用来表示和存储Unity中的Avatar的一个类，其中包含了： SkeletonBone的数组，用来记录在TPose时所有模型骨骼的信息（包括在config unity avatar时所有的节点，比如Model节点等，不光光是代表骨骼的节点）。 HumanBone的数组，用来记录模型骨骼和Mecanim骨骼的一一映射关系。 还包含了Muscles设置中的一些配置信息。 UnityEngine.HumanDescription: 是一个Struct 其实就是Unity版的UmaTPose，其中包含了SkeletonBone的数组和HumanBone的数组和其他的一些avatar配置，可以用这个来创建一个新的avatar。 生成UMA用的TPose文件 ExtractTPose()函数中包含了两种提取TPose的方法，这个函数是在Unity的Config Avatar界面执行的。 把选中的asset变成一个ModelImporter，然后从modelImporter.humanDescription中读取HumanDescription信息。用这种方法只能提取出avatar的其他配置信息，而关键的SkeletonBone的数组和HumanBone的数组是得不到的。而且实际运行中，这部分代码并没有执行到。 全局找带有Animator的GameObject，并且从这个GameObject和它的下属层次结构中得到SkeletonBone的数组和HumanBone的数组。这个方法不能得到avatar的其他配置信息，但是没关系，这些信息但用到之前会赋予和unity中一样的默认值。实际运行中，UMA是用了这个方法来提取TPose的。因为在Config Avatar界面，模型就是在TPose的。 运行时动态生成Avatar 当UMA检测到人物的骨架发生变化时（通过DNA调整），就会重建Avatar。 UMA会将骨架重置到没有运用DNA修改的状态，就是去掉DNA的功能。 UMA会利用提取出来的TPose文件，让骨架回到TPose的状态。 然后再运用DNA的修改。 最后，在这个状态下，读取当前的模型骨骼的信息，填入到SkeletonBone的数组，HumanBone的数组保持TPose文件中的样子。然后利用这些信息生成新的avatar，最后设置给Animator。]]></content>
      <categories>
        <category>Unity</category>
      </categories>
      <tags>
        <tag>Unity</tag>
        <tag>UMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UMA 301视频教程备忘录]]></title>
    <url>%2F2019%2F11%2F14%2FUMA-301%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B%E5%A4%87%E5%BF%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[制作一个Base Model 用Adobe Fuse生成一个光头T pose模型，导出上传到Mixamo，Mixamo会自动为这个模型绑定骨骼，还要生成一些脸部表情的Blend Shapes。 导入Blender，删除一些多余的骨骼，可以看到有很多面部表情的Blend Shapes。 还可以利用其他的工具来生成模型，比如Daz。 UMA 301 - Part 1 Creating a Base Model 修正一些不对的地方 Fuse导入Blender的模型，normal map没有起作用，要调整一下的。 模型的全局Scale不是1，Rotation也不都是0，需要apply scale &amp; rotation，把他们烘培到mesh顶点上，让scale和rotation归一化。 要在原有的以Hip为root的骨架上，在加两个骨骼Global-&gt;Position，这是UMA需要的骨架结构。 UMA 301 - Part 2 Fixing Scale &amp; Global Bones 添加一些调整用的骨骼来改变人物外形 因为现有的骨骼，你在调整上级的scale的时候，所有的下级都会受到影响，这不是我们想要的，所以我们需要在调整的地方复制一个骨骼（没有下级）。 然后把复制的骨骼作为原骨骼的子（本来他们应该是同一级的），并且把原骨骼的权重全部转移到复制的骨骼上（原骨骼不在影响任何顶点，所有的影响都到复制的骨骼上了）。 以上步骤是针对现有的骨骼的，如果要新建一个调整骨骼（一般都是叶子节点了），那么先新建好骨骼，放在对应的层级下面，然后再刷顶点权重。 UMA 301 - Part 3 Creating Adjustment Bones 创建BlendShape来改变人物外形 每个BlendShape（morph target）都是原始mesh的一个copy，所以如果有很多blendshape的话，会占用很多内存的 在Blender中新建一个ShapeKey，然后进入编辑模式，改变一些顶点，就可以创建一个新的BlendShape了。 UMA 301 - Part 4 Creating Blendshapes 把用Fuse制作的Blender文件直接导入Unity中遇到的问题 把带有BlendShape的文件导入Unity，通过BlendShape改变过的部位的法线会有问题，造成奇怪的光照效果。 人物的脚骨骼也有问题，歪掉了，导致人物的脚变形了。 上面的问题是在Unity中直接用Blender文件导致的，因为其实Unity在导入Blender文件的时候，用了Blender的默认FBX Exporter。 所以我们就不能在Unity中直接用Blender文件，而是通过Blender的FBX Export手动导出。在Blender中默认是试用FBX 7.4 binary来导出，这就是问题所在，所以我们要改成用FBX 6.1 ASCII来导出。 FBX6.1也有自己的问题，首先Scale不起作用了。然后设置Forward为Z Forward，除了Armature和Mesh需要导出外，其他的都去掉，然后导出。 到Unity中发现，Unity会用0.01的Scale来导入，导致人物非常小，所以要去掉Unity中Use File Scale的导入选项，然后法线就对了，但是脚还是不对。 到Blender中，需要删除两只脚的最后一根骨头，然后自己在新建同样的骨头，最后在导出到Unity就对了。 UMA 301 - Part 5 Troubleshooting Blender Exports 拆分并最终确定我们的模型 在老版的UMA中，必须拆分模型为几个部分，方便穿衣服的时候，不显示相应的身体部分（避免穿模）。因为新版UMA支持Mesh Hide Asset，可以解决穿模问题，所以不是必须拆分模型的。但是我觉得还是拆分模型的runtime性能要高些吧。 还有如果有BlendShape的话，把使用BlendShape的部分Mesh拆分出来，那么只有那部分Mesh会有多个BlendShape的顶点copy，其他部分就没有copy了，可以节省很多内存。但是到Unity中UMA还是会合并成一个Mesh，所以这些BlendShape还是会变成整个Mesh的BlendShape，同样浪费比较多的内存。（只是我的猜测） 在拆分之前，UMA需要一个Unified Mesh用来在缝合拆分Mesh时，修正缝合处的法线方向。所以需要先copy身体和眼球的Mesh，然后把copy的mesh合成一个Mesh，命名为Unified，还要把这个mesh上的BlendShape全部删除，因为没作用。 然后就可以拆分Mesh了，把头拆分出来一个新的Mesh（其中的BlendShape也会保留着一起拆分了），命名为Head。 拆分后，剩下的身体Mesh中，还是保留了BlendShape的（因为这些BlendShape是脸部表情，所以没有实际作用），需要把它们全部删除。 UMA 301 - Part 6 Splitting and Finalizing our Model 创建一个基本种族（Base Race） 先从原始的Fuse模型中，把我们需要用的Textures取出来，然后删除原始Fuse模型。 确保我们导入的模型的transform中scale都是1。 到模型的设置界面，选择Rig-&gt;Animation Type-&gt;Humanoid，然后设置Avatar。 在Avatar设置界面，需要通过UMA-&gt;Extract T-Pose取出TPose给UMA备用，这会在UMA/Content/Generated/TPoses目录下生成一个UMA用的TPose文件，我们需要把这个文件copy到我们新建race的文件夹下面。 打开UMA-&gt;Slot Builder，创建新的slots（方法见以前的备忘录），注意不需要创建Wardrode Recipe，还有需要缝合的slot，要设置Seams Mesh为Unified，不需要缝合的就把这选项置空。 设置新创建的Overlay，需要指定贴图。 通过UMA-&gt;Core-&gt;Text Recipe新建一个Recipe，这个是base race需要的一个recipe。 通过UMA-&gt;Core-&gt;Race Data新建一个Race Data。 把整个新建的文件夹拖入UMA Global Library。 设置RaceData，把text recipe设置到Base Race Recipe字段中，TPose文件设置到T-Pose字段，设置好名字，可以添加和删除Wardrobe Slots。 然后设置Text Recipe，把它命名为BaseRecipe，把RaceData设置到RaceData字段，点击Add to Global Index，把身体slot和对应的overlay加入其中。 因为这个FuseRace，和UMA自带的Race的骨架结构不相同，所以需要勾选Dynamic Character Avatar-&gt;Active Race-&gt;Race Change Options-&gt;Rebuild Skeleton选项，才能在各个race间正常切换。 只有在运行时需要切换race，而且race的骨架不一样，才需要勾选Rebuild Skeleton选项，其他时候不需要，因为有效率影响。 如果需要BlendShape起作用，需要勾选Dynamic Character Avatar-&gt;Advanced Options-&gt;Load BlendShapes选项。 UMA 301 - Part 7 Creating a Basic Race 设置骨骼DNA 新建UMA-&gt;DNA-&gt;Dynamic DNA Converter，打开新建的文件，通过Dynamic DNA-&gt;Create Dynamic DNA Asset新建Dynamic DNA Asset。 打开Dynamic DNA Asset，新建一些DNA Name（名字有两部分组成，身体部位和这个部分的属性，比如overallHeight）。 把Dynamic DNA Converter设置到RaceData的Dna Converter List中。 打开UMA-&gt;Examples-&gt;Extensions Examples-&gt;DynamicCharacterSystem-&gt;Scenes-&gt;UMA DCS Tool - DNAConverterBehaviour，选择我们新建的race。 然后设置不同的DNA，选择需要调整哪个骨骼的哪些属性。还可以增加很多操作到一起。最后调整完了，需要save的。 UMA 301 - Part 8 Setting up bone DNA 设置BlendShape DNA 新建一个FuseMorphConverter的GameObject，在上面添加Morph Set Dna Converter Behaviour脚本，把这个GameObject变成一个Prefab，命名为FuseMaleMorphConverter。 通过UMA-&gt;DNA-&gt;Dynamic DNA Asset新建DNA Asset，命名为FuseMaleMorphUMADnaAsset。 通过UMA-&gt;DNA-&gt;Morph Set DNA新建Morph Set DNA Asset，命名为FuseMaleMorphSetDnaAsset。 把FuseMaleMorphUMADnaAsset中的Dna Type Hash拷贝到FuseMaleMorphSetDnaAsset中的Dna Type Hash中，还要拷贝到FuseMaleMorphConverter中的Dna Type Hash中。 把FuseMaleMorphUMADnaAsset设置到FuseMaleMorphConverter的Dna Asset中，把FuseMaleMorphSetDnaAsset设置到FuseMaleMorphConverter的Morph Set中。 在FuseMaleMorphUMADnaAsset中新增DNA Name。 在FuseMaleMorphSetDnaAsset中设置哪个DNA影响哪个BlendShape。BlendShape的名字可以从UMARenderer-&gt;SkinnedMeshRenderer-&gt;BlendShapes中查看。 把FuseMaleMorphConverter加入到RaceData的Dna Converter List中。 把新进的这些Assets加入到UMA Global Library中。 注意FuseMaleMorphUMADnaAsset中的DNA名字的顺序，和FuseMaleMorphSetDnaAsset中Dna Morphs的顺序必须相同。 UMA 301 - Part 9 Creating Blendshape DNA 创建非人形UMA种族 把需要的FBX导入blender，修改其骨架结构，加入Global和Position骨骼，并且把armature的名字改成Root，这样到unity中就有Root-&gt;Global-&gt;Position这样的骨架了。 这样修改后，原本的动画就不能用了，因为骨架变化了，所以需要把所有的动画导入Blender，然后做同样的骨架修改。 接下来的步骤和human race是一样的。 UMA 301 - Part 10 Non-Humanoid UMA Races]]></content>
      <categories>
        <category>Unity</category>
      </categories>
      <tags>
        <tag>Unity</tag>
        <tag>UMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UMA 201视频教程备忘录]]></title>
    <url>%2F2019%2F11%2F14%2FUMA-201%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B%E5%A4%87%E5%BF%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[制作Overlay 先准备好Overlay用到的贴图，每个Overlay单独一个文件夹，把贴图放在文件夹下面。 菜单UMA-&gt;Core-&gt;Overlay Asset，设置好名字，然后设置Material（是UMA专用的Material），一般选择UMA_Diffuse_Normal_Metallic，选择对应的贴图数量，这个材质是3个贴图。 把创建好的Overlay放入到UMA Global Library中。 创建对应的Wardrobe Recipe，菜单UMA-&gt;DCS-&gt;Wardrobe Recipe，选择对应的race，选择Wardrobe Slot，选择对应的Base Slot，最后Add Overlay。 把创建好的Wardrobe Recipe放入到UMA Global Library中。 PS：如果发现做好的Overlay分辨率比原始素材要低，那是因为UMA在生成最终贴图的时候，会把分辨率减半。这个是可以设置的，在UMA_DCS-&gt;UMAGenerator-&gt;Initial Scale Factor，默认是2，就是贴图尺寸减半，如果改成1，就是保持原来的贴图大小。这样分辨率就高了，当然会占用更多的资源。 UMA 201 - Part 2a Skin Overlays 优化Overlay 如果制作的Overlay是一张很大的图，但是只用到了其中很小的一部分，其他部分都是空白，那么会浪费很多最后游戏包的空间（runtime内存是不浪费的，因为UMA最终会合成一张贴图）。 到ps中，找到一个小的长方形区域（2的幂次方），可以完全包含需要的那部分贴图，记住这个区域和整张图的左上角的偏移量。 然后把原图切小成小的长方形区域大小，并保存放回unity中覆盖原图。 回到Overlay的设置界面，设置其中的Rect参数。因为ps中偏移量是从原图左上角到小区域的左上角，而unity需要的是原图左下角，到小区域的左下角，所以这里的偏移量需要做计算的，如下图所示。最后还要填上小图的尺寸大小。 然后要更新一下对应的Wardrobe Recipe，在其中删除老的Overlay，重新添加修改的Overlay，就大功告成了。 UMA 201 - Part 2b Optimising Overlays 把其他的模型适配到UMA第一节 把fbx导入到blender中，注意导入的选项，保证骨骼的方向是正确的。 通过骨骼的旋转和缩放等，把导入的模型和UMA模型基本重叠起来。 把旋转和缩放都bake到模型的顶点上，确保骨骼和模型没有旋转和缩放。 把模型和骨骼解除绑定，并删除其中的权重信息。 UMA 201 - Part 3a Preparing Pre-Rigged Clothing 把其他的模型适配到UMA第二节，编辑mesh 选中需要编辑的mesh，进入Mesh Edit Mode，可以编辑面，边，点。 删除没用的面，把分离出来面创建成新的mesh。 打开比例编辑，移动面，使得被编辑的mesh和UMA的人物很好的适配。 UMA 201 - Part 3b Vertex Wrangling 把mesh和UMA的骨骼绑定 把UMA身体上的顶点权重信息project到新的mesh上 把mesh和UMA的骨架绑定到一起。 绑定后，还可以编辑mesh，去掉没用的面和顶点。 对于某些部分，映射的权重可能不好，需要手动调整。 UMA 201 - Part 4 Rigging Clothing Models 把mesh导入unity变成slots 因为fbx版本比较低，不稳定，所以直接用blend的格式导入到unity中，当创建完slot后，可以把这个blend文件删除，UMA不再需要它。 打开UMA-&gt;Slot Builder，把对应的mesh设置到Slot Mesh上，选择合适的UMAMaterial，选择Slot Destination Folder。还可以选择自动生成Overlay和Wardrobe Recipe和自动加入到Global Library。 自动生成的Overlay没有设置对应的贴图，需要找到这个Overlay，并设置对应的贴图。 自动生成的Wardrobe Recipe也是空的，需要手动设置race，Wardrobe Slot。Add Slot和设置Overlay。 把整个新建的文件夹拖入到UMA Global Library中注册。 UMA 201 - Part 5 Converting Clothing Models 创建自定义shader 创建一个普通的Material，选择自己需要的shader，调整好shader的一些默认参数。 通过菜单UMA-&gt;Core-&gt;Material常见UMA Material，把需要的贴图的类型按顺序编码到名字中。 把unity material设置到Material字段中，定义需要用到的Texture Channels，设置好每个贴图的属性（比如shader中贴图的名字）。 UMA 201 - Part 6 Using Custom Shaders]]></content>
      <categories>
        <category>Unity</category>
      </categories>
      <tags>
        <tag>Unity</tag>
        <tag>UMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UMA 101视频教程备忘录]]></title>
    <url>%2F2019%2F11%2F14%2FUMA-101%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B%E5%A4%87%E5%BF%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[UMA_DCS prefab 加上 UMA Dynamic Character Avatar, 选择一个race，然后设置好默认的animator controller就可以运行了。 UMA 101 - Part 2 Up and Running base recipe –&gt; 确定一个裸体的人物模型。wardrobe recipe –&gt; 确定人物的衣服，鞋子，饰品等；这里可以设置A物体覆盖B物体。 UMA 101 - Part 3 Introducing Recipes Dynamic Character Avatar中的Default Recipes可以为这个人物添加默认的衣服裤子等。每个Wardrobe recipe都只能和相对应的race一起才能正常工作。Wardrobe slot是设置这个物体是放在人身上的哪个部位的。还可以设置需要覆盖哪个部位的其他物件和base物件。 UMA 101 - Part 4 Default Recipes Dynamic Character Avatar中的Character Colors可以重载recipes中的shared colors，然后在slot中可以用shared color来影响overlay的颜色。这样就可以通过设置Character Colors来方便的设置人物的整体颜色。 UMA 101 - Part 5 Shared Colours 所有的recipes和他们所用到的components都需要到UMA Global Library中去注册才能使用。制作DCS-&gt;wardrobe recipe UMA 101 - Part 7 Creating Custom Recipes Misc-&gt;Mesh Hide Asset，可以用来隐藏某个slot上的某些三角形，已解决衣服和身体部分重叠的情况下的穿模问题。UMA内建工具可以编辑需要去掉那些三角形。在编辑的时候还可以把衣服覆盖在身体上，方便查看需要隐藏哪些三角形。 UMA 101 - Part 8 Advanced Occlusion 通过代码来改变人物的外貌 得到DynamicCharacterAvatar实例：1234using UMA;using UMA.CharacterSystem;DynamicCharacterAvatar avatar = GetComponent&lt;DynamicCharacterAvatar&gt;(); UMA 101 - Part 9 Modify Characters with Code 设置Dna：12345678//得到所有的DnaSetterDictionary&lt;string, DnaSetter&gt; dna = avatar.GetDNA();//设置height dnadna["height"].Set(1f);//得到height dnadna["height"].Get();//重建人物avatar.BuildCharacter(); 改变种族：12345// 把race改成HumanMaleDCS，改变后不需要调用BuildCharacter，// race改变后所对应的dnaSetter的字典也会改变。avatar.ChangeRace("HumanMaleDCS");// 得到当前种族avatar.activeRace; 监听avatar的事件：123// DynamicCharacterAvatar的一些时间可以通过下面类似的方法添加和删除。avatar.CharacterUpdated.AddListener()avatar.CharacterUpdated.RemoveListener() UMA 101 - Part 10 Simple Character Creator (1 of 3) 设置颜色：1234avatar.SetColor("Skin", Color.Black);// 设置名字为Skin的sharedColor的颜色为black// true就是函数内部马上调用BuildCharacter，颜色马上就显示出来// false则把新颜色缓存者，到下次BuildCharacter才会显示在人物上。avatar.UpdateColors(true); 得到颜色：1avatar.GetColor("Skin").color; 操作slot：123456789101112// 把名字为Hair的slot设置成MaleHair1这个配件。avatar.SetSlot("Hair", "MaleHair1");//重建人物avatar.BuildCharacter();// 把名字为Hair的slot清空avatar.ClearSlot("Hair");//重建人物avatar.BuildCharacter();// 得到名字为Hair的slot的item的名字avatar.GetWardrobeItemName("Hair"); UMA 101 - Part 11 Simple Character Creator (2 of 3)UMA 101 - Part 11a Simple Character Creator (2a of 3) Save/Load一个人物 1234567// 这里可以得到一个json编码的字符串，保存了所有和当前人物相关的配置信息。string recipeToBeSaved = avatar.GetCurrentRecipe();// load之前需要清空所有的slots，不然如果load出来的人物没有相对应的slot的信息，// 那么之前的slot将会被laod出来的人物保留着。avatar.ClearSlots();// 从一个string的recipe中load一个人物avatar.LoadFromRecipeString(recipeToBeSaved); UMA 101 - Part 12 Character Creator (3 of 3) Utility Recipes就是类似于wardrobe recipes的东西，但是他不是给人物添加衣服的，而是给人物添加一个功能性的脚本，这个脚本可以自定义做很多事情。UMA里面内建了两个比较有用的Utility Recipes。 ForearmTwistRecipe，用来解决手腕在绕着手臂旋转时，手腕和手臂连接处会不自然的扭曲到一起的问题。用了这Recipe后，旋转手腕会和正常人那样自然。 ExpressionsRecipe，用来解决人物下巴往下掉的问题。因为如果动画中没有下巴的动画，那么下巴会在建模时的位置，一般都是在下面，让嘴巴张开的。加了这个Recipe，就可以调整下巴，让嘴巴比起来。当然还可以调整很多五官的位置的。 UMA 101 - Part 13 Utility Slots Expression Player可以控制人物的面部表情。内建的Enable Blinking可以让人物随机间隔的眨眼，Enable Saccades可以让眼球像真人那样转动。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798using UMA;using UMA.CharacterSystem;using UMA.PoseTools; // for Expression Playerprivate ExpressionPlayer expression;// OnUMACreatedexpression = GetComponent&lt;ExpressionPlayer&gt;();expression.enableBlinking = true; // 激活眨眼expression.enableSaccades = true; // 激活眼球运动// void Update，实现各种表情float delta = 10 * Time.deltaTime;switch(mood)&#123;case 0: // Normal expression.leftMouthSmile_Frown = Mathf.Lerp(expression.leftMouthSmile_Frown,0,delta); expression.rightMouthSmile_Frown = Mathf.Lerp(expression.rightMouthSmile_Frown,0,delta); expression.midBrowUp_Down = Mathf.Lerp(expression.midBrowUp_Down,0,delta); expression.leftBrowUp_Down = Mathf.Lerp(expression.leftBrowUp_Down,0,delta); expression.rightBrowUp_Down = Mathf.Lerp(expression.rightBrowUp_Down,0,delta); expression.rightUpperLipUp_Down = Mathf.Lerp(expression.rightUpperLipUp_Down,0,delta); expression.leftUpperLipUp_Down = Mathf.Lerp(expression.leftUpperLipUp_Down,0,delta); expression.rightLowerLipUp_Down = Mathf.Lerp(expression.rightLowerLipUp_Down,0,delta); expression.leftLowerLipUp_Down = Mathf.Lerp(expression.leftLowerLipUp_Down,0,delta); expression.mouthNarrow_Pucker = Mathf.Lerp(expression.mouthNarrow_Pucker,0,delta); expression.jawOpen_Close = Mathf.Lerp(expression.jawOpen_Close,0,delta); expression.noseSneer = Mathf.Lerp(expression.noseSneer,0,delta); expression.leftEyeOpen_Close = Mathf.Lerp(expression.leftEyeOpen_Close,0,delta); expression.rightEyeOpen_Close = Mathf.Lerp(expression.rightEyeOpen_Close,0,delta); break;case 1: // Happy expression.leftMouthSmile_Frown = Mathf.Lerp(expression.leftMouthSmile_Frown,0.7f,delta); expression.rightMouthSmile_Frown = Mathf.Lerp(expression.rightMouthSmile_Frown,0.7f,delta); expression.midBrowUp_Down = Mathf.Lerp(expression.midBrowUp_Down,-0.7f,delta); expression.leftBrowUp_Down = Mathf.Lerp(expression.leftBrowUp_Down,0f,delta); expression.rightBrowUp_Down = Mathf.Lerp(expression.rightBrowUp_Down,0f,delta); expression.rightUpperLipUp_Down = Mathf.Lerp(expression.rightUpperLipUp_Down,0f,delta); expression.leftUpperLipUp_Down = Mathf.Lerp(expression.leftUpperLipUp_Down,0f,delta); expression.rightLowerLipUp_Down = Mathf.Lerp(expression.rightLowerLipUp_Down,-0f,delta); expression.leftLowerLipUp_Down = Mathf.Lerp(expression.leftLowerLipUp_Down,-0f,delta); expression.mouthNarrow_Pucker = Mathf.Lerp(expression.mouthNarrow_Pucker,0f,delta); expression.jawOpen_Close = Mathf.Lerp(expression.jawOpen_Close,0f,delta); expression.noseSneer = Mathf.Lerp(expression.noseSneer,0.1f,delta); expression.leftEyeOpen_Close = Mathf.Lerp(expression.leftEyeOpen_Close,-0.2f,delta); expression.rightEyeOpen_Close = Mathf.Lerp(expression.rightEyeOpen_Close,-0.2f,delta); break;case 2: // Sad expression.leftMouthSmile_Frown = Mathf.Lerp(expression.leftMouthSmile_Frown,-0.8f,delta); expression.rightMouthSmile_Frown = Mathf.Lerp(expression.rightMouthSmile_Frown,-0.8f,delta); expression.midBrowUp_Down = Mathf.Lerp(expression.midBrowUp_Down,0.7f,delta); expression.leftBrowUp_Down = Mathf.Lerp(expression.leftBrowUp_Down,-0.3f,delta); expression.rightBrowUp_Down = Mathf.Lerp(expression.rightBrowUp_Down,-0.3f,delta); expression.rightUpperLipUp_Down = Mathf.Lerp(expression.rightUpperLipUp_Down,0f,delta); expression.leftUpperLipUp_Down = Mathf.Lerp(expression.leftUpperLipUp_Down,0,delta); expression.rightLowerLipUp_Down = Mathf.Lerp(expression.rightLowerLipUp_Down,0f,delta); expression.leftLowerLipUp_Down = Mathf.Lerp(expression.leftLowerLipUp_Down,0f,delta); expression.mouthNarrow_Pucker = Mathf.Lerp(expression.mouthNarrow_Pucker,-0.7f,delta); expression.jawOpen_Close = Mathf.Lerp(expression.jawOpen_Close,0f,delta); expression.noseSneer = Mathf.Lerp(expression.noseSneer,-0.1f,delta); expression.leftEyeOpen_Close = Mathf.Lerp(expression.leftEyeOpen_Close,0.5f,delta); expression.rightEyeOpen_Close = Mathf.Lerp(expression.rightEyeOpen_Close,0.5f,delta); break;case 3: // Angry expression.leftMouthSmile_Frown = Mathf.Lerp(expression.leftMouthSmile_Frown,-0.3f,delta); expression.rightMouthSmile_Frown = Mathf.Lerp(expression.rightMouthSmile_Frown,-0.3f,delta); expression.midBrowUp_Down = Mathf.Lerp(expression.midBrowUp_Down,-1f,delta); expression.leftBrowUp_Down = Mathf.Lerp(expression.leftBrowUp_Down,1f,delta); expression.rightBrowUp_Down = Mathf.Lerp(expression.rightBrowUp_Down,1f,delta); expression.rightUpperLipUp_Down = Mathf.Lerp(expression.rightUpperLipUp_Down,0.7f,delta); expression.leftUpperLipUp_Down = Mathf.Lerp(expression.leftUpperLipUp_Down,0.7f,delta); expression.rightLowerLipUp_Down = Mathf.Lerp(expression.rightLowerLipUp_Down,-0.7f,delta); expression.leftLowerLipUp_Down = Mathf.Lerp(expression.leftLowerLipUp_Down,-0.7f,delta); expression.mouthNarrow_Pucker = Mathf.Lerp(expression.mouthNarrow_Pucker,0.7f,delta); expression.jawOpen_Close = Mathf.Lerp(expression.jawOpen_Close,-0.3f,delta); expression.noseSneer = Mathf.Lerp(expression.noseSneer,0.3f,delta); expression.leftEyeOpen_Close = Mathf.Lerp(expression.leftEyeOpen_Close,-0.2f,delta); expression.rightEyeOpen_Close = Mathf.Lerp(expression.rightEyeOpen_Close,-0.2f,delta); break;case 4: // Surprised expression.leftMouthSmile_Frown = Mathf.Lerp(expression.leftMouthSmile_Frown,0f,delta); expression.rightMouthSmile_Frown = Mathf.Lerp(expression.rightMouthSmile_Frown,0f,delta); expression.midBrowUp_Down = Mathf.Lerp(expression.midBrowUp_Down,1f,delta); expression.leftBrowUp_Down = Mathf.Lerp(expression.leftBrowUp_Down,1f,delta); expression.rightBrowUp_Down = Mathf.Lerp(expression.rightBrowUp_Down,1f,delta); expression.rightUpperLipUp_Down = Mathf.Lerp(expression.rightUpperLipUp_Down,0f,delta); expression.leftUpperLipUp_Down = Mathf.Lerp(expression.leftUpperLipUp_Down,0f,delta); expression.rightLowerLipUp_Down = Mathf.Lerp(expression.rightLowerLipUp_Down,-0f,delta); expression.leftLowerLipUp_Down = Mathf.Lerp(expression.leftLowerLipUp_Down,-0f,delta); expression.mouthNarrow_Pucker = Mathf.Lerp(expression.mouthNarrow_Pucker,-1f,delta); expression.jawOpen_Close = Mathf.Lerp(expression.jawOpen_Close,0.8f,delta); expression.noseSneer = Mathf.Lerp(expression.noseSneer,-0.3f,delta); expression.leftEyeOpen_Close = Mathf.Lerp(expression.leftEyeOpen_Close,1f,delta); expression.rightEyeOpen_Close = Mathf.Lerp(expression.rightEyeOpen_Close,1f,delta); break;default: break;&#125; 在AssetStore上有个插件LipSync Pro可以更好利用ExpressionPlayer来控制表情 UMA 101 - Part 14 Expression Player UMA和其他插件的整合 许多插件要求人物上必须有Animator组件，但是UMA的Aniamtor组件是运行时动态生成的，这就会有问题。但是其实可以先在人物上放置一个空的Animator组件，UMA运行后会找个这个Animator并正确初始化它。 许多插件要求人物上必须有骨骼结构，但是UMA的骨骼结构同样是运行时才生成的，这又是个问题。但是现在可以通过UMA-&gt;Bone Builder菜单，预先生成静态的骨骼结构在人物上，然后运行时UMA会找到这个静态骨骼结构，并和UMA关联起来，而不会重新生成一个新的骨骼结构。 有个小问题，这个预先生成的骨架是从UMA的原始FBX文件中得到的，和预览用的dummy人物不相符。UMA Bone Visualizer可以在没有运行的时候在scene中显示骨架。 可以在这个预先生成的骨架上绑定新的骨头，用来绑定武器等附件，但是需要注意在骨架结构中不能出现相同名字的骨头，哪怕在不同的层次下。 UMA 101 - Part 15 Integration Tips]]></content>
      <categories>
        <category>Unity</category>
      </categories>
      <tags>
        <tag>Unity</tag>
        <tag>UMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Math Magician - Lerp, Slerp, and Nlerp]]></title>
    <url>%2F2019%2F07%2F23%2FMath-Magician-Lerp-Slerp-and-Nlerp%2F</url>
    <content type="text"><![CDATA[Math Magician – Lerp, Slerp, and Nlerp原文出处 Illustration of linear interpolation on a data set. The same data set is used for other interpolation methods in the interpolation article. (Photo credit: Wikipedia) Man, I wish I had this blog going through school, because this place has become my online notebook. Game development is like riding a bike – but math, for me, can have a hard time stickin’. I’ve been working in databases so long that I have a lot of bit-shifting down, but my matrix and vector math is starting to lack. So, a creative way for me to remember all these algorithms is to try to explain them. Today, I’m going through Linear Interpolation, Spherical Linear Interpolation, and Normalized Linear Interpolation. The Code I’m starting off with the code to show the similarities and differences with each. One thing similar right off the bat is the parameter list: each function takes the same 3 – A start, an end, and a percent. The start and end can be Vectors, Matrices, or Quaternions. For now, lets use Vectors. The percent is a scalar value between 0 and 1. If percent == 0, then the functions will return start, if percent == 1, the functions return end, and if percent == 0.5, it returns a nice average midpoint between the 2 vectors. Illustration of linear interpolation. (Photo credit: Wikipedia) Lerp: The basic Linear Interpolation function. Lets break this down, right to left. First, we have end - start, which will pretty much return a vector the same distance as start and end, but represented as if it’s start vector was {0, 0, 0}. We multiply that vector by percent, which will give us a vector only percent as long (so if percent is 0.5, the vector now is only half as long as it was). We then add start back to it to return it to its normal position. Done. 1234Vector3 Lerp(Vector3 start, Vector3 end, float percent) &#123; return (start + percent*(end - start)); &#125; Lerp is useful for: Transitions from place to place over time, implementing a Health Bar, etc. Pretty much going from point A to point B. Oblique vector rectifies to Slerp factor. (Photo credit: Wikipedia) Slerp: Slerp is a very powerful function. It’s become very popular in the game industry for it’s ease of use and significant result, however, people tend to ignore its drawbacks. Slerp travels the torque-minimal path, which means it travels along the straightest path the rounded surface of a sphere, which is a major plus and part of the reason it’s so powerful, as well as the fact that it maintains constant velocity. But Slerp is non-commutative, meaning the order of how the vectors/matrices/quaternions are passed in will affect the result. A call to Slerp(A, B, delta) will yield a different result as compared to Slerp(B, A, delta). Also, Slerp is computationally expensive because of the use of mathematical functions like Sin(), Cos() and Acos(). 12345678910111213141516171819// Special Thanks to Johnathan, Shaun and Geof! Vector3 Slerp(Vector3 start, Vector3 end, float percent) &#123; // [Dot product](http://en.wikipedia.org/wiki/Dot_product) - the cosine of the angle between 2 vectors. float dot = Vector3.Dot(start, end); // Clamp it to be in the range of Acos() // This may be unnecessary, but floating point // precision can be a fickle mistress. Mathf.Clamp(dot, -1.0f, 1.0f); // Acos(dot) returns the angle between start and end, // And multiplying that by percent returns the angle between // start and the final result. float theta = Mathf.Acos(dot)*percent; Vector3 RelativeVec = end - start*dot; RelativeVec.Normalize(); // [Orthonormal basis](http://en.wikipedia.org/wiki/Orthonormal_basis) // The final result. return ((start*Mathf.Cos(theta)) + (RelativeVec*Mathf.Sin(theta))); &#125; Slerp is useful for: Rotation, mostly. Nlerp: Nlerp is our solution to Slerp’s computational cost. Nlerp also handles rotation and is much less computationally expensive, however it, too has it’s drawbacks. Both travel a torque-minimal path, but Nlerp _is_ commutative where Slerp is not, and Nlerp also does not maintain a constant velocity, which, in some cases, may be a desired effect. Implementing Nlerp in place of some Slerp calls may produce the same effect and even save on some FPS. However, with every optimization, using this improperly may cause undesired effects. Nlerp should be used more, but it doesn’t mean cut out Slerp all together. Nlerp is very easy, too. Just normalize the result from Lerp()! 1234Vector3 Nlerp(Vector3 start, Vector3 end, float percent) &#123; return Lerp(start,end,percent).normalized(); &#125; Nlerp is useful for: Animation (in regards to rotation and such), optimized rotation. The trouble with Lerp, Slerp and Nlerp to most people is that they don’t truly understand the functions. Most people will post “Hey, I have a problem with this” and someone will say use one form of Linear Interpolation, and just patch it in. It’ll work, but will they understand why it works? Could it be improved? I would recommend taking some time and reading articles on the different Linear Interpolations – there are more out there I didn’t mention, and all with different purposes. That’s all for now. I’ll find more math to put up in the weeks to come. Till then, take care! This post is dedicated to the staff at ZeeGee Games. I may have lost the battle, but I will not lose the war! I will be a Game Dev Jedi! Also, if you’re part of the IGDA, Vote For Grant! (This link is really old by now, though…) References:The Inner Product – Understanding Slerp, Then Not Using It, by Jonathan BlowGameDev.Net – Nlerp/Slerp with 2 vectors. [Solved], Post by ChaosPhoenix, Reply from jyk]]></content>
      <categories>
        <category>GameEngine</category>
      </categories>
      <tags>
        <tag>Unity</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBEngine源码解读五--转载二]]></title>
    <url>%2F2019%2F06%2F28%2FKBEngine%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%BA%94-%E8%BD%AC%E8%BD%BD%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[原文地址 此笔记是本人在开发及研读的过程中记录下来的，由于没整理，会看得有些吃力，请读者视能力而读，个人理解，如有问题，悉心接受。 部分引用KBEngine官网的一些句段，感谢kbe。 每个cell在被创建时当被观察者观察到时（也就是进入到别人的AOI里）那么会被调用kbe底层回调python层onWitnessed方法， 当然只在增加观察者时第一次被观察到或者删除观察者时观察者个数为0时才回调，通过参数1或者0进行区别，看以下代码 controlledBy:kbe底层提供了可以把一个非客户端持有的cell上的entity给客户端控制，由客户端来进行可以控制的改变权限是改变方向和位置， 但是这个entity必须是在控制者的entity（demo里的avatar的entity）的Aoi范围内，而且这个控制者是必须被客户端控制的一个entity， 具有aoi也就是还必须是具有cell的，操作如下： 在cell上self.controlledBy = base（操作者的base） Kbe具体实现代码如下： 当被控制了，那么客户端可以直接发送onUpdateDataFromClient消息对被控制者进行update，在cellApp上这条消息回调方法里会进行判断的 BaseAppMgr与CellAppMgr如何做到负载均衡？ CellApp或者BaseApp，每次在handleGameTick，也就是每一帧都会调用updateLoad，updateLoad会计算出一个可以表现负载的一个值， 然后再调用onUpdateLoad()，onUpdateLoad里会将此值和APP的标记同步和当前app的entities的数量同步到相关的AppMgr上，例如下： Appmgr接收方： KBE的H5插件是客户端的实体ID读取方式是以智能优化的方式，而不是根据服务端的配置进行相应方式读取， 所以服务端为了配合客户端的智能优化，需要将aliasEntityID和entitydefAliasID开启 所有Cell上的entity都拥有EntityCoordinateNode，coordinateNode的flag是COORDINATE_NODE_FLAG_ENTITY 只有当entity被调用installCoordinateNodes时，才会给entity的EntityCoordinateNode设置坐标系统 问题：假如avatar位置设置为0、0、0，space创建了cell的entity，为什么在avatar的onEnterAoI没有space这个entity？ 因为space没有手动设置hasClient为True或者space的属性没有跟CELL的client相关的标志。代码中的判断如下 服务端关服： 信号回调函数这里指的信号是一种软中断，它即可以作为进程间通信的一种机制，更重要的是，信号总是中断一个进程的正常运行， 它更多的用于处理一些非正常情况。例如 ctrl+c 就是一个向进程发一个信号，信号是异步的，进程并不知道信号什么时候到达， 进程既可以处理信号，也可以发送信号给特定的进程，每个信号都有一个名字，这些名字以SIG开头，例如SIGABRT是进程异常终止信号。 硬件异常产生的信号有例如除数为0、无效的存储访问等，这些条件通常由硬件检测到，并将其通知到内核， 然后内核为该条件发生时正在运行的进程产生相应的信号 软件产生的异常信号可以用kill、raise、alarm、settimer、sigqueue产生信号 信号的种类：不可靠信号：Linux信号机制基本上是从Unix系统中继承过来的。早期Unix系统中的信号机制比较简单和原始，后来在实践中暴露出一些问题， 因此，把那些建立在早期机制上的信号叫做”不可靠信号”，信号值小于SIGRTMIN的叫不可靠信号(1~31)。 每次信号处理后，该信号对应的处理函数会恢复到默认值。但现在的Linux已经对其进行了改进，信号处理函数一直是用户指定的或者是系统默认的。 信号可能丢失。 不可靠信号不支持信号排队，同一个信号产生多次，只要程序还未处理该信号，那么实际只处理此信号一次。 可靠信号：信号值位于SIGRTMIN和SIGRTMAX之间的信号都是可靠信号，可靠信号克服了信号可能丢失的问题 。 实时信号与非实时信号：Linux目前定义了64种信号（将来可能会扩展），前面32种为非实时信号，后32种为实时信号。非实时信号都不支持排队， 都是不可靠信号，实时信号都支持排队，都是可靠信号。 信号排队意味着无论产生多少次信号，信号处理函数就会被调用同样的次数。 Kbe这里监听的SIGINT和SIGHUP是系统信号，SIGINT信号当终端输入了中断字符ctrl+c，默认系统处理是进程终止掉，SIGHUP信号也是差不多， 终端关闭时会产生这个信号，默认处理一样，还有其他系统信号，剩下想了解，自己百度， Kbe这里就是用的信号注册，当接收到这个信号，可以做自己的关服处理，linux终端下 输入kill–l 就有列出所有的信号了 Machine:是机器服，每一台物理机都要开一个machine进程，machine是负责本物理服的所有进程的管理，如果需要配局域网内一组多物理服的话， 那么*mgr、interface、logger的进程在这一组服内只需要各进程开一个即可， 第三方登录接口：如果在kbengine.xml或kbengine_def.xml里配置了interfaces的host port那么，dbmgr在initInterfacesHandler()时 会有两个类来对应登录时选择哪个类 如果接了第三方，那么需要在kbengine.xml或者kbengine_def.xml配置host和post，那么dbmgr在loginAccount时会先请求interface，代码如下： Interfaces收到onAccountLogin()后调用了脚本层的onRequestAccountLogin,也将需要的参数传给脚本层的方法中，代码如下 在脚本层中的处理完相关数据后要调用引擎层的accountLoginResponse， 脚本层调用方式： 引擎层代码： 在interfaces返回给dmgr的onLoginAccountCBBFromInterfaces接口调用的onAccountLoginCB函数里， 如果success错误码不是由SERVER_ERR_LOCAL_PROCESSING，那么DBTaskAccountLogin不会检查密码，看needCheckPassword变量，代码如下 interfaces那边返回的错误码（suceess）是成功的话，那么在DBTaskAccountLogin的db_thread_process中 如果没有此帐号的话，DBTaskAccountLogin的db_thread_process中调用的queryAccount返回false时，那么会判断kbengine.xml或者kbengine_def.xml中 有没有配置loginAutoCreate为true 或者 （有没有配置第三方地址并且不需要检查密码），两个条件一个为true，都会自动创建帐号，代码如下 Dbmgr处理login服发送过来的帐号登陆查询时，会查询entitylog表，是否有在线记录，有的话，会将componentID和entityID记录下 代码如下： Login服收到onLoginAccountQueryResultFromDbmgr，假如帐号没问题的话，会根据发送过来的componentID和entityID进行判断， 这里的componentID不是客户端的，而是BaseApp的componentID，意味着这个账户是登录在哪个BaseApp上，已经在线与不在线有两个处理方式，代码如下： onLogOnAttempt：当玩家在别处登录时，会通知脚本异常登录请求有脚本决定是否允许这个通道强制登录，调用与Account entity的onLogOnAttempt 与客户端的心跳超时： 因为Channel初始化的时候，不管是客户端还是服进程的channel，都会启动不活跃检测，注册检测定时器，超时参数的话，读kbengine配置的， 定时器回调会判断是否是超时，判断条件是这个channel的lastReceivedTime_成员变量是否超出超时参数，如果超时，那么将做超时处理， 将此条channel销毁掉，代码如下： 当然kbe底层，包括客户端插件也做了心跳处理，会每隔n时间向目标发送onAppActiveTick消息，ServerApp接收到这条消息后， 更新了接收数据时间，代码如下：]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
        <tag>KBEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBEngine源码解读四--转载一]]></title>
    <url>%2F2019%2F06%2F28%2FKBEngine%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E5%9B%9B-%E8%BD%AC%E8%BD%BD%E4%B8%80%2F</url>
    <content type="text"><![CDATA[原文地址 此笔记是本人在开发及研读的过程中记录下来的，由于没整理，会看得有些吃力，请读者视能力而读，个人理解，如有问题，悉心接受。部分引用KBEngine官网的一些句段，感谢kbe。 下列符号解释 =：继承 ==：等同 +：与上具有子关系 CellAppMgr:管理多个CellApp +CellApp:管理多个区域 + Cell：一个区域 ==Space：等同Cell +此区域上的玩家entity：代表玩家 +Witness（对象）：监视周围的玩家entity，将发生的事件消息同步给客户端 +AOI（兴趣范围）：默认500M +GhostEntitys（list）：存取此区域边沿外界一定距离内的玩家entity的列表 +GhostEntity：从邻近的Cell的对应的entity的部分数据的拷贝的实体 +属性数据：只读，如果某个属性对于客户端是可见的，那么该属性必须是可以存在 Ghost的，例如：当前的武器、等级、名称 +范围：默认500M，可配置，大于等于玩家的AOI +负载平衡：CellApp会告诉它们的Cell的边界应该在哪里 +新建的玩家Entity加入到正确的Cell上 +一个服务器组一个Mgr实例 DBMgr：管理Entity数据的数据库存储 +存数据：在BaseApp间轮流调度处理，BaseApp向CellApp要entity的cell部分的数据再定时转给DBMgr存储 Machine：监视服务器进程信息，每个服务器机器上有一个machine +作用：启动/停止服务器进程，通知服务器群组各个进程的存活状态，监视机器的使用状态：cpu/内存/带宽 +machine不会tcp连接的 ObjectPool：对象池，一些对象频繁的被创建，例如：moneystream，bundle，tcppacket等等，这个对象池通过服务端峰值有效的预估提前创建一些对象缓存起来，在用到的时候直接从对象池中提取 SmartPoolObject：智能池对象，创建一个智能池对象，与相应的对象池绑定，只要此对象析构时，那么会调用绑定的对象池进行回收 Components：组件类，记录当前组件信息 ServerApp：App基类，每个app都需继承这个类，基本的C++框架模块 都会存在这个类，其中有继承通道超时操作，继承通道取消注册操作， Resmgr：资源类，存有一切环境信息，例如bin或者资源目录，和一些已打开文件 ThreadPool：线程池类 NetWork：网络 +Bundle：发送的包裹 +Packet：接收的包裹 +NetworkInterface：理解为网络上下文对象。 +EndPoint：对端，可以理解成与对端的会话信息及与对端网络的解决方案。硬封装 创建socket，绑定，监听，recv，send函数 +ListenerRecviver：保存有对端对象，特性，网络上下文对象（NetworkInterface） +EventDispatcher：事件分配，封装着一个poller和一个Task（Task可以add，所以这个类又是一个可以作为多任务分配的任务集），接口基本都是用poller，注册事件。异步IO的体现 +Channel：代表着一个连接，成员有EndPoint，TcpPacketReceiver，TcpPacketSender，Init时会将fd注册到poller，接收到的包裹经过过滤器存放到bufferPacket，然后bufferPacket会被NetworkInterface类调用processChannels时调用本类的processPackets进行处理 +onPacketReceived方法：这个方法里是用来判断及计数的，例如lastTickBytesRecevied 这个成员变量是用来对一帧之内recv多次直至为空产生的数据量进行计数，查看TcpPacketReceiver这个类的processRecv方法与PacketReceiver::handleInputNotification方法，计数时要进行与kbengine.def配置的参数进行判断及处理 +PacketReader： 包裹阅读器，具有解单包功能，和操作单包，是在Channel被调用processPackets时被调用本类的processMessages函数 +TcpPacketReceiver = PacketReceiver：封装了接收行为，是驱动者，操作接受行为时是被调用processRecv这个方法，方法里操作：创建packet，调用EndPoint的recv，再调用本类的processPacket，processPacket中调用channel的onPacketReceived，过滤包裹形成可用包；processRecv被PacketReceiver::handleInputNotification(intfd)这个方法循环调用 +TcpPacketSender = PacketReceiver：封装了发送行为，是驱动者，创建包裹，最终是调用EndPoint的发送函数 +TcpPacket = Packet：tcp包裹，一个Channel会存放多个Packet， +PacketFilter：包裹过滤器，将Packet存放到Channel，被PacketReceiver调用processPacket函数时调用了本类的recv函数，是在传输层recv后，立即调用的，也就是没放到buffer，还没调用到Channel类的processPackets函数之前 +WebSocketPacketFilter=PacketFilter：继承于PacketFilter，重写PacketFiler的recv和send，因为websocket会有web格式包头信息，所以要先过滤掉 EpollPoller =EventPoller：网络模型类，使用epoll封装的消息事件类 SelectPoller=EventPoller：网络模型类，使用select封装的消息事件类 PyFileDescriptior：python层的文件描述类，用于设置Python的相对应的网络消息回调方法，网络事件触发时，会调用相对应的回调 内存结构： MoneyStream：内存流 函数的认识： QueryPerformanceCounter()：获取高性能计时器从开机到调用的tick数 QueryPerformanceFrequency()：获取高性能计时器的赫兹 也就是每秒多少次tick 以上两个函数可以计算出高精度的时间 误差看机器 一般是微妙级 KBEngine.calcStampsPerSecond()：获取每秒多少次cpu tick Python C API： PyImport_GetModuleDict：获取模块管理的（python层面）字典？ Int PyDict_SetItemString(PyObject p, const char key, PyObject* val)：插入值val到字典以key为key，python底层会调用 PyUnicode_FromString()创建一个unicode的PyObject对象。 PyUnicode_FromString(const char* u)：创建一个unicode的PyObject对象。 PyModule_GetDef ServerApp：最底层，C++服务端模型都在这层 PythonApp = ServerApp：Python模型在这层 KBE +C++层 +服务端底层框架 +游戏服务端逻辑框架 +python层 +配合C++服务端逻辑框架层 +游戏逻辑框架 XMLXml：kbengine的类，完全操作一个xml文件，封装了tinyxml库的接口。 XML：使用了tinyxml解析库 Tinyxml库： +TiXmlNode = TiXmlBase：一个xml节点，一个xml文件一般begin节点是document，节点成员有parent，prev，next，LastChild，FirstChild；parant就是这一条链表的父亲节点，下图的描述只存在于除了是属性信息TxXmlAttribute的情况 +TiXmlDocument = TiXmlNode：文档节点，读取xml文件的最开端，文档节点的parse函数就是将xml里面的内容解析成一个个节点链接在文件节点之后， +TiXmlComment = TiXmlNode：注释节点，标志：begin：&lt;!-- end：--&gt; +TiXmlDelcaration = TiXmlNode：声明节点，标志：begin：&lt;?xml end：&gt; +TiXmlText = TiXmlNode：文本节点，有两种情况会创建；标志1：begin：&lt;![CDATA[ end：]]&gt;; 标志2：例子：&lt;Type&gt; AVATAR &lt;/Type&gt;，当读到&lt;Type&gt;的&gt;时，继续跳过些空格，如果遇到字符不是’&lt;’，那么会创建这个类来保存后面的文本，直到遇到’&lt;’。 +TiXmlElement = TiXmlNode：元素节点，保存有多个属性节点，标志：begin：&lt; + alpha end：&gt;，例：&lt;Account&gt;&lt;/Account&gt; +TiXmAttribute = TiXmlBase：属性信息，属于元素节点的成员，例：&lt;Account hasClient=&quot;true&quot;&gt;&lt;/Account&gt;这里面的 hasClient=&quot;true&quot;，name和value都有保存 +TiXmlParsingData：存有当前文件游标位置 ScriptDefModule：描述一个脚本def模块 DataType：存有不同和有别名的数据类型。有别名的数据类型是通过读取assets\scripts\entity_defs\alias.xml文件进行保存的 PropertyDescription：属性类 EntityDef：静态类，加载xml文件保存方法名，根据xml文件名搜索相关模块的python脚本，然后再根据已经保存下来的方法名检查此python脚本有没有保存的方法名；entity_app、dbmgr和ClientApp会调用EntityDef初始化。 PyMemoryStream：调用pickler静态方法将自己的方法注册到Pickler相关的python Pickler：静态类，存有pickler方法系列注册到python的模块， EntityApp = ServerApp： isInitialize：调用了installPyScript，installPyModules，installEntityDef installPyScript：将python空间目录路径设置成组件通用和当前组件的脚本路径，getscript().install()添加、初始化脚本基础模块kbengine， BaseApp和CellApp在installPyModules有调用EntityApp：：installPyModules BaseApp = ServerApp：在installPyModules时将一些关于entity方法注册到相对应的python模块，例如： 在installAllScriptModules时根据entities.xml里的每个字段在python目录空间（也就是设置的脚本目录）里导入跟这个字段同名的模块，例如&lt;Account hasClient=&quot;true&quot;&gt;&lt;/Account&gt;，那么就把Account.py导入进来。 Base = ScrptObject：代表着一个实体entity \kbe\res：是系统资源目录server/kbengine_defs.xml \assets\res\server：是用户资源目录server/kbengine.xml \assets\scripts：是用户脚本目录entities.xml FixedMessages：姑且叫做固定消息，程序启动时会加载kbe\res\server\messages_fixed.xml，然后保存在自己的_infomap，每次程序开始时在给MessageHandlers添加app里已经定义好的消息时会判断是否是固定消息的，如果是那么会赋值消息ID，而且消息里的消息ID是不能重复的； 继承MessageHandle类，然后加入MessageHandles类的其中一个成员。 MessageHandlers：消息操作集类， MessageHandler:消息操作类，所有的C++层的网络消息操作句柄都继承于这个类 对C++层的网络消息定义不懂，可以仔细看看interface_defs和loginapp_interface_macros文件，因为这两个文件配合利用宏定义而巧妙的产生各个消息类的声明及定义，消息操作集类的声明及定义 划黄线地方是操作消息类型，这里是添加这个类型对象到消息集合，定义及声明的地方在如下 调用消息句柄是在PacketReader::processMessages里调用pMsgHandlers进行查找，操作，如图 \kbe\res\server\kbengine_defs.xml：服务系统默认配置这个最好不好更改，如果要更改配置，更改服务用户配置文件；如果你需要改变引擎设置,请在({assets}/res/server/kbengine.xml)中覆盖kbe/res/server/kbengine_defs.xml的对应参数来修改,这样的好处是不会破坏引擎的默认设置，在你更新引擎时也不会产生冲突，以及在多个逻辑项目时不会影响到其他的项目设置。 \asserts\res\server\kbengine.xml：服务用户配置 任何一个App都会读取/assets/scripts/common；/assets/scripts/data；/assets/scripts/user_type；kbe/res/scripts/common这四个目录，如果是BaseApp/CellApp/DBMgr那么会读取这个 任何一个App都会读取设置python路径： /assets/scripts/server_common；/assets/scripts/common；assets/scripts/data；/assets/scripts/user_type；/kbe/res/script；/kbe/res/scripts/common；/kbe/res/scripts/common/lib-dynload；/kbe/res/scripts/common/DLLs； /kbe/res/scripts/common/Lib 这些目录 如果是BaseApp/CellApp/DBMgr那么还会加上assets/scripts/server_common； DB：DBUtil：有个静态成员变量pThreadPoolMaps_，这个变量是根据xml表配的数据库接口名称为key添加的线程池（默认为default），线程池类是DBThreadPool DBThreadPool = ThreadPool：保存着DBThread；kbengine_defs.xml中配置的databaseInterface配置多少个就创建多少个。 DBThread = TPThread：TPThread是封装 ThreadPool的其中的一个线程，也就是说ThreadPool有多个TPThread（有多少这个对象就有多少条线程），TPThread的workfunc执行时在处理Task时会调用自身的onProcessTaskStart(Task)，processTask(Task)，onProcessTaskEnd(Task)，所以就会调用到DBThread重写的虚函数；而DBThread在onProcessTaskStart (Task) 时是有开始mysql的事务，onProcessTaskEnd (Task)是有提交事务。而且DBThread在启动轮询处理开始时onStart()会创建DBInterface留做此线程用，onEnd()会删除掉DBInterface DBInterfaceMysql = DBInterface：是与mysql直接操作的接口类，保存有ip、端口等详细信息；kbengine_defs.xml中配置的databaseInterface配置多少个就创建多少个，配置里一般都有个default DBUtil：数据库控制单元，创建、初始化、删除DBInterface， EntityTables：存有多个表（EntityTableMysql）；同步到数据库时会为每个EntityTableMysql会增加任务到线程池内 EntityTableMysql：mysql表实体，存有表的属性数据：字段类型，字段名，表名； Component：App会去主动搜寻需要告知本组件信息的App，如果有别的组件广播到本组件来，那么也会保存下来；Commponent类的process会发送udp消息组广播给machine，如果发送成功并且达到是machine的条件，然后直到没回消息，那么证明machine存在，设置下状态，再次process时会搜索本组件需要连接的，所有需要连接的组件都询问下machine，如果有的话，那么都添加，然后更改状态，下次process时就是连接这些组件（connectComponent）；state是0时是寻找machine，state是1时是寻找需要连接的组件，state是2时是连接在state是1时添加的组件；连接需要连接的组件完成后，发送注册组件消息给连接完成的组件，对端组件就会 entity_defs下面的文件的Properties里面的每个字段的Flags值会影响到ScriptDefModule的，也会影响到读取并创建PropertyDescription存在ScriptDefModule的哪个PropertyDescription 需要persistent的，ScriptDefMoudle的persistentPropertyDescr_保存下这个属性 entitiy_defs下面的文件的Implement字段有值的话，那么读取assets\scripts\entity_defs\interface相关字段值的文件 entity_defs下面的每个文件是一个ScriptDefModule（模块），也就是说每个模块都会添加相应的文件里面的属性、方法和相应的Implement的Interface下的文件。调用这个loadAllScriptModules方法进行加载所有python类 FixedDictType = DataType：举例这么一个用户自定义类型；这个类型需要工具，会在alias.xml读取此类型出来，会读取implementedBy这个字段，例如：&lt;implementedBy&gt;AVATAR_DATA.inst&lt;/implementedBy&gt;，会根据 “ .”分割字符串，分割出AVATAR_DATA、inst，然后读取AVATAR_DATA模块（也就是py文件），然后从模块中返回inst对象，py文件中例：inst = AVATAR_DATA_PICKLER()，会从模块中返回一些方法保存在此类型类中，此类中成员moduleName是这implementedBy字段的值，即是AVATAR_DATA.inst SyncEntityStreamTemplateHandler：是属于baseapp的类；主要是同步持久化数据，会将dbAccountEntityScriptType相对应的ScriptDefModule模块的属性默认数据转换成python对象（在PropertyDescription::addPersistentToStream这里面） Component组件如果不是guconsole（也就是说是console）的话，那么相连的组件的uid必须是一样的 BundleBroadcast：广播发送包裹，是udp协议栈，构造时会绑定监听端口，epListen是用来监听的socket，epBroastcast是用来广播的socket CellApp、BaseApp组件与DBMgr连接并注册到DBMgr后，DBMgr会将需注册的组件的信息（也就是前面的CellApp、BaseApp）告诉其他已经和DBMgr连接的CellApp、BaseApp组件，被告知的与CellApp或BaseApp同类型的组件会注册最开始的CellApp或BaseApp的组件并连接 也就是说，所有的BaseApp和CellApp是有互联的 Components::groupOrderid或者 globalOrderid：App组内的排序ID，例如给CellApp组内的排序ID，BaseApp组内的排序ID，LoginApp组内的排序ID，这些ID的分配是在App连接DBMgr时，由DBMgr调用ServerApp::onRegisterNewApp()，onRegisterNewApp再调用addComponent方法，addComponent分配的ID，再由DBMgr的SyncAppDatasHandler类调用process时同步这些ID给相关的App，会发送onDbmgrInitCompleted消息给相关的APP，相关的App收到消息后，会设置id，（BaseApp会多做一步，设置在App所在python环境中），会调用python层的onInit，还会在创建InitProgressHandler类，InitProgressHandler是定时器类，会调用process做些初始化的工作，并且会发送消息给相关的AppMgr，AppMgr进行相关的工作，InitProgressHandler的process会一直循环执行到App的初始化完成度大于1，可以看C++层调用python层的onReadyForLogin方法 AppMgr的initProgress：（完成度）是指App类型的所有App初始化完成除以所有App的值，完成初始化的App 50 / 总共App 100 = 0.5 App也有自己的完成度 InitProgressHandler：是在BaseApp上创建并调用的话，如果是第一个BaseApp，会向DBMgr获取所有模块的一些实体 在做上面的工作的同时，DBMgr充当着GlobalDataServer的角色，所以会将GlobalDataServer相关的数据发送给新的App 组件的uid是根据环境变量的uid设置的 CellApp同上图 BaseApp和 BaseApp和BaseAppMgr连接后，BaseApp有个onUpdateLoad方法在同步BaseApp信息给BaseAppMgr FixedDictType：这个类型有个成员moduleName，保存alias.xml中是FIXED_DICT FixedDict类型修改值是没有调用entity重载的set，所以没有同步属性 alias.xml：这个文件不仅仅是设置别名，如果别名的类型是复杂类型，那么还会再延伸出属性，具有与Account.def等文件的Property字段的同样功能 遇到是Array类型，那么增加表，表名取父表+父字段名字+当前字段名 Base启动时会询问 只要有python类继承base或cell，都会有__py_onTimer添加到模块中 每个ServerDefModule都有保存着所代表的python对象类型（PyTypeObject*scriptType_），例如Account，那就是Account类型的对象，CreateObject函数就是创建所代表类型的python对象，这个对象的成员是根据Account.xml的属性来创建的，然后在EntityApp&lt;&gt;中的onCreateEntity()里用new去重新构造python对象，例如new(pyEntity)E(eid,sm)，这里E是Base或Cell，因为保存着所代表的python对象类型都是是继承Base或Cell，不明白为什么都是继承Base或Cell，请看对ServerDefModule的解释 nitializeEntity kbe\src\lib\entitydef\entity_macro.h：定义entity与python C++扩展的宏 kbe\src\server\baseapp\base_interface_macros.h：定义base与python通信的消息参数宏（通信指的其实就是调用函数） 继承Base的python类的cellData是在Base构造时创建的cellData，详情请看Base构造 在Base目录下的Space.py的__init__初始化时调用Base的CreateInNewSpace CellApp的Entity是Entity类 BaseApp的Entity是Base类 Entity类和Base类在初始化时会调用CreateNamespace，然后在调用子类python类的__init__ 基本上python模块里的KBEngine模块是存在EntityApp的getScript().getModule() Kbe初始化好后，BaseApp调用了kbengine.py里的onBaseAppReady，如果是第一个的话，那么调用了kbengine.createBaseLoaclly(&quot;Spaces&quot;, {}),与kbengine.createBaseLoaclly绑定的C++函数Baseapp::__py_createBase调用了Baseapp::getSingleton().createEntity，创建了一个继承与Base的python类Spaces，Spaces是整个kbe创建地图的起点，看spaces的__init__方法时你就懂了，注意Spaces在初始化时会也用C++提供的定时器注册定时器。 在CellApp创建python类Space的Entity时，Space的__init__会调用addSpaceGeometryMapping创建地图 玩家在网关登录（也就是BaseApp），BaseApp会调用idClient.alloc()分配一个EntityID给玩家 Base的initializeScript就是调用的子python类的__init__ 当BaseApp要创建CellApp，也就是调用createCellEntity，就会在CellApp创建Cell，调用onCreateCellEntityFromBaseapp，然后创建完会发送onEntityGetCell消息给BaseApp，然后BaseApp会调用python类的onGetCell；而在最开始创建Space调用CreateInNewSpace会发送消息给CellAppMgr，CellAppMgr再发送给CellApp创建NewSpace并创建Entity，然后会给BaseApp发送onEntityGetCell消息； 总结（猜测）：只要是BaseApp通知CellApp的消息中会导致创建Entity都会给BaseApp发送onEntityGetCell消息，在Base::onGetCell() 给Base创建EntityMailBox给成员ceilMailBox，也就是说所有的Base或Proxy最终都会创建mailBox只不过类型是多选项的， Base:: onEntityGetCell ()：在这个方法里，会设置base的spaceID，在initClientCellPropertys会将spaceID发送给客户端 CellApp的Entity会保存有baseMailBox、clientMailBox，baseMailBox会记住在BaseApp的ID BaseApp的Base或Proxy会保存有cellMailBox、clientMailBox，cellMailBox会记住在CellApp上所在的spaceID和CellApp的ID， Baseapp::createCellEntity(EntityMailboxAbstract*createToCellMailbox, Base* base)：将base创建到这个createToCellMailbox相对应 CellApp上的Enitity 1234567891011121314151617KBEngine.createBaseAnywhere("SpawnPoint", &#123;"spawnEntityNO" : datas[0], "position" : datas[1], "direction" : datas[2], "modelScale" : datas[3], "createToCell" : self.cell&#125;);-&gt; BaseApp:: createBaseAnywhere–&gt; BaseAppMgr –&gt; BaseApp:: onCreateBaseAnywhere–&gt; BaseApp::CreateEntity –&gt; SpacePoint::__init__ -&gt; self.createCellEntity(self.createToCell) //（这里的createToCell与最开始的createToCell是一样的）-&gt; Base::createCellEntity() //（这里的createToCellMailbox是最开始的createToCell, base是前面的SpawnPoint类）-&gt; Baseapp::getSingleton().createCellEntity (EntityMailboxAbstract* createToCellMailbox,Base* base) -&gt; Cellapp::onCreateCellEntityFromBaseapp // 告诉BaseApp:: onEntityGetCell和创建Entity-&gt; BaseApp::onEntityGetCell Client如何远程调用服务端entity的方法（其实就是client发送消息给服务端，服务端根据方法ID调用实体的相关方法而已）：Client发送给Base消息onRemoteMethodCall，BaseApp根据发来的EntityID(ProxyID/BaseID)，因为是调用base的方法，发来的entityID必须是客户端的proxyID一致，也就是不能调用别的proxy，只能调用客户端的代理base/proxy,调用Base的onRemoteMethodCall,根据消息的utype查找出相应的BaseMethon，调用相关的python方法 Server如何远程调用客户端的entity方法（从服务端的角度）：server是通过mailbox与client交互的，我也不知道为什么非要弄个mailbox，mailbox是存在entity（Base/Proxy）上的，server怎么保证通过mailbox与client交互呢？是通过python层级对client操作，都要用sefl.client这个行为操作，例如：self.client.onReqAvatarList(self.characters)， client指的就是mailbox，client调用onReqAvatarList时，调用了onScriptGetAttribute()（这是在写mailbox类时重写的方法）,在这个方法里面，查找了这个方法名（onReqAvatarList），将查找出来的方法进行创建一个能操作的python类createRemoteMethod(pMethodDescription)，再将创建的类对象返回给python，python执行到onReqAvatarList()的()时调用了前面创建的类的方法tp_call()，然后Base* pEntity=Baseapp::getSingleton().findEntity(mailbox-&gt;id()); methodDescription-&gt;checkArgs(args)进行检查，检查参数是否一致，就这个self.characters，检查会true的话，就创建一个mail：mailbox-&gt;newMail((*pBundle));，然后再执行methodDescription-&gt;addToStream(mstream,args);这行代码是将methodDescription里的方法ID和传进去的args进行序列化到mstream，然后再(*pBundle).append(mstream-&gt;data(), (int)mstream-&gt;wpos());，进行send给client 数据库的数据保存：特殊处理，数据库保存方向和位置，只要模块有cell属性，sm-&gt;hasCell()，那么都会有position和direction BaseApp在创建Avatar时Avatar类仅仅只是辅助创建下，write db data后就destroy 其实不管是BaseApp下的Base/Proxy的CellMailBox还是对应在CellApp下的Entity的 ID都是一样，也就是说只要BaseApp才生产entityID，而且不管是Entity还是在Base/Proxy还是MailBox ID必须是一致的，同个entity类型 CellApp::Witness类：监视拥有者玩家内的视野范围的玩家（AOI），负责同步视野范围其他玩家的客户端数据及位置信息。有三种状态，一（将要进入视野范围内）：其他玩家进入拥有者玩家的视野范围，那么将其他玩家在拥有者玩家的Witness里的引用的状态更改成普通状态，并同步其他玩家的客户端信息与位置信息给拥有者玩家，通知拥有者玩家有其他玩家进入视野范围；二（将要离开视1野范围内）：其他玩家离开拥有者玩家的视野范围，那么通知拥有者玩家有其他玩家要离开，并删除其他玩家在拥有者玩家的Witness里的引用；三（普通状态，还在视野范围内），同步还在视野范围的其他玩家的volatile信息给拥有者玩家。一直在update EntityCoordinateNode = CoordinateNode： AOITrigger = RangeTrigger： 一个玩家都有一个witness成员，和一个EntityCoordinateNode；witness类又包含着AOITrigger类，EntityCoordinateNode又会影响到AOITrigger， AOITrigger 也就是EntityCoordinateNode的数值一改变 CoordinateSystem：update（node）函数功能，更新node在x链，y链，z链上的位置，使其符合一定顺序位置 每次服务端收到客户端的同步位置，都会调用node的update() AllClients：会调用onScriptGetAttribute方法，然后会newClientsRemoteEntityMethod(pMethodDescription, otherClients_, id_) ClientsRemoteEntityMethod：这个类主要是做远程远程方法给entityAOI内的玩家，例如self.otherClients.onJump()，otherClients是AllClients类型，python层调用onJump()，那么就C++层otherClients就调用了onScriptGetAttribute（）；具体操作类似Server如何远程调用客户端的entity方法；只不过这个是AOI内玩家都远程调用一遍 Kbe的tools：Xls的表头符号$代表着这个字段是有映射值，需要替换 !代表着是key .代表着是不为空 数据库保存ARRAY类型属性的方法就是新建一张子表，表名结构：父表名_属性字段名_被ARRAY定义类型的值，例如： Alias.xml： 123456789&lt;AVATAR_INFO_LIST&gt; FIXED_DICT &lt;implementedBy&gt;AVATAR_INFO.avatar_info_list_inst&lt;/implementedBy&gt; &lt;Properties&gt; &lt;values&gt; &lt;Type&gt; ARRAY &lt;of&gt; AVATAR_INFO &lt;/of&gt; &lt;/Type&gt; &lt;/values&gt; &lt;/Properties&gt;&lt;/AVATAR_INFO_LIST&gt; Account.def 12345678910&lt;root&gt; &lt;Properties&gt; &lt;characters&gt; &lt;Type&gt; AVATAR_INFOS_LIST &lt;/Type&gt; &lt;Flags&gt; BASE &lt;/Flags&gt; &lt;Default&gt; &lt;/Default&gt; &lt;Persistent&gt; true &lt;/Persistent&gt; &lt;/characters&gt; &lt;/Properties&gt;&lt;/root&gt; 例如这个属性【characters】保存到数据库，那么首先已经有Account表了，这张表咋们就不说了，【characters】的类型是AVATAR_INFOS_LIST，这个类型固定字典的保存有values这个值，但是这个值是ARRAY类型，所以此时会创建一张子表以对应这个ARRAY定义的值，表名是父表名Account_属性名characters_被ARRAY定义类型的值values，【Account_characters_values】；将ARRAY类型看作一个张表的数据，ARRAY是数组，一张表的多个数据也是数组，所以kbe属性里凡是遇到ARRAY类型的，都会另创建一张表；由于【Account_characters_values】是子表，所以在这个表内会创建一个字段parentID，这个parentID对应父表的ID。 BaseApp持久化数据到DBMgr：一个entity（Base/Proxy），调用writeToDB（）函数，如果entity（Base/Proxy）有cell那么发条消息给CellApp，获取下Cell上的数据，再返回到BaseApp给entity（Base/Proxy），然后entity（Base/Proxy）再调用onCellWriteToDBCompleted()函数，函数里获取entity（Base/Proxy）需要持久化的数据发送给DBMgr；如果此Entity没有dbid，那么DBMgr认为是insert类型，反之，update类型；如果是insert类型 且 Entity有个属性是ARRAY类型，那么这个会在Content的optable循环插入需要insert或update的数据。 每个entity的数据不会因为被改变而实时改变数据库，但是有个定时存档，最好还是改变数据了，手动存档 Entity数据写入数据库是将所有需要持久化的数据，也就是只要entity脏了，那么不管它的属性有没有脏 只要entity的某个需要持久化的属性改变了，那么该entity都会置脏 定时存档：Archiver::archive(Base base), 调用base.writeToDB(NULL, NULL, NULL) 定时备份：Backuper::backup() 其实这两个的区别就是backup是将entity置脏，而writetodb是将数据写入db，如果没有entity没有置脏，那么不会写入db，可以去追溯entity的两个成员变量：shouldAutoArchive_、shouldAutoBackup_ Cellapp：非玩家entity都是不会同步entity所在的地图信息 帐号登录，在返回DB信息后，创建Base时会添加个clientMailBox，然后会同步些客户端数据给客户端，这时还没有cell 在创建Base之后，创建cell后会返回onEntityGetCell，base会根据如果有clientmailbox，那么会调用initClientCellPropertys这个方法，同步base本身的celldata属性给客户端 CellApp上面的cell也有clientMailBox，但是cell上的clientmailbox是MAILBOX_TYPE_CLIENT_VIA_BASE这个类型的，也就是要通过base转发到client 只有hasClient（是否有clientChannal）才会有witness观察者对象 hasClient在kbengine引擎有多次出现，有不同出现地方不同的含义：1.在ServerDefModule里的hasClient是指这个模块有没有需要同步给客户端的数据，有这个类型ENTITY_CLIENT_DATA_FLAGS；2.是否有clientChannal，出现了hasClient，可以顺着往前推，会有看到判断是否有clientChannal 在cell，有两种AOI trigger，一种是TrapTrigger，这种保存着一个控制器pProximityController_，一种是AOITrigger，这种保存着一个withness， 两个Trigger都继承RangeTrigger，每个trigger 都会有个节点RangeTriggerNode插入在坐标系统里，这个RangeTriggerNode节点是用来比较附近的非自己的EntityCoordinateNode，以此来判断自己是否在其他节点的AOI范围内。也就是自己的EnittyCoordinateNode是否在其他RangeTriggerNode的AOI范围内，感兴趣的话，会调用RangerTriggerNode的Trigger来调用 RangeTrigger：有两个RangeTriggerNode（positiveBoundary, nagativeBoundary），都会将RangeTriggerNode添加到原点的entityNode，这两个node代表着 EntityCoordinateNode：是实体的node，只watcherNode gameUpdateHertz：是一秒内多少tick 玩家entity会通过是否是client添加witness对象（有witness对象，那么就添加AOI对象，有AOI对象就会在coordinate系统添加范围节点），而npc/monster entity没有，只能通过脚本层，调用entity::addProximity来添加范围触发器，而范围触发器ProximityController在构造时会添加TrapTrigger，并且调用TrapTrigger::install，而TrapTrigger是继承RangeTrigger，RangeTrigger::install()的功能是安装两个范围节点到coordinateSystem，添加范围触发器ProximityController到pControllers-&gt;add(p)，也就是说entity里有个witness对象，也有pControllers用来存放多个触发器； coordiante的update里调用了onNodePass系列，这个node是RangeTriggerNode类型，那么是调用了RangeTriggerNode::onNodePass系列，RangeTriggerNode::onNodePass系列又调用了RangeTrigger::onNodePass系列，条件允许调用了this-&gt;onEnter(pNode)，而这个RangeTrigger的子类型是TrapTrigger，则调用了TrapTrigger::onEnter，子类型是AOITrigger，则调用了AOITrigger::onEnter；TrapTrigger::onEnter调用了ProximityController::onEnter，ProximityController::onEnter调用了Entity::onEnterTrap，AOITrigger::onEnter调用了Witness::onEnterAOI GlobalDataClient：是继承于kbe自定义的map类型（姑且称为DIYmap），这个DIYmap类型初始化对象设置成重写好的一些python map的API：123456789SCRIPT_INIT(Map, 0, &amp;Map::mappingSequenceMethods, &amp;Map::mappingMethods, 0, 0)，mappingMethods/** python map操作所需要的方法表 */PyMappingMethodsMap::mappingMethods =&#123; (lenfunc)Map::mp_length, // mp_length (binaryfunc)Map::mp_subscript, // mp_subscript 下标取值 (objobjargproc)Map::mp_ass_subscript // mp_ass_subscript 下标 赋值&#125;; 在mp_ass_subscript调用了一个虚函数onDataChanged；在GlobalDataClient类也重写了onDataChanged，所以在脚本层改变了GlobalDataClient类型的数据，就会调用onDataChanged，onDataChanged调用pickler::pickle(value, 0)，此时value是一个entity（base/proxy），那么会回调entity（base/proxy）的__reduce_ex__，__reduce_ex__方法返回了一个tuple类型，参数（mailbox的unpickle方法， tuple类型参数（一些参数要传给unpickle的参数，这里面有个参数非常重要，ENTITY_MAILBOX_TYPE这个类型的参数， 因为要让别的app获得这个mailbox，所以要设置好这个mailbox的类型）），pickler方法结束后，发送onBroadcastGlobalDataChanged消息给DBMgr； DBMgr接收到后，DBMgr将数据发送给相关全局数据类型的组件，并保存数据在DBMgr 并且每个App类型也可以有各自的AppData，可以参考BaseApp或者CellApp C++层的onInstallPyModules方法，例如只管理Base的全局BaseApp上，看下图，可以直接在python层，这样子调用KBEngine.baseAppData 还有一个是所有的继承于EntityApp的App都可以获取到数据，看下图，可以直接在python层，这样子调用KBEngine.globalData, 因为有保存mailbox并且cellApp和baseApp之间都是有互通的，所以可以通过保存在GlobalData的相关模块的mailbox进行远程调用方法 baseAppData和cellAppData同理 1234567891011121314151617enum &#123; MAILBOX_TYPE_CELL = 0, // mailbox的类型是cell MAILBOX_TYPE_BASE = 1, // mailbox的类型是base MAILBOX_TYPE_CLIENT = 2, // mailbox的类型是 通过base的cell 也就是说调用cell这个mailbox上的方法， // 会先将消息和参数发给属于有这个base实体的baseapp上，再由目标baseapp将消息和参数发送给对应的cell MAILBOX_TYPE_CELL_VIA_BASE = 3, MAILBOX_TYPE_BASE_VIA_CELL = 4, // 同上理 MAILBOX_TYPE_CLIENT_VIA_CELL = 5, MAILBOX_TYPE_CLIENT_VIA_BASE = 6,&#125; GhostManager：是CellApp的一个类，作用1：在teleport的时候real的entity切换成ghost时，任何发给Entity消息都存放在这个类 BaseMessagesForwardHandler：主要是用于保存发送给关于Base的Cell的消息，一般要创建这个类时，表示CellApp上的Entity在进行Teleport操作 ghostCell 和 realCell：这两个存在Entity类里，主要针对Entity进行Teleport操作时的一些辅助标志；Entity在操作Teleport时，会通知相关的base调用onMigrationCellappStart方法增加BaseMessagesForwardHandler类；然后设置entity的realCell（ComponentID）为要migration的Cell的ComponentID，然后调用Entity的addToStream方法，发送Entity的数据（包括witness、controller、Timer等）到目标Cell， 在Cell上被观察者，一改变属性，那么会将自身属性同步给观察者，会在Entity类onDefDataChanged里对witnesses进行 createInNewSpace：创建了一个 调用这个接口的python类的类型的对象 entity的destroy：python层的destroy调用的是C++层的__py_pyDestroyEntity，这个方法调用的了entity的C++层的onDestroyEntity函数，和entity的C++层的destroyEntity()， 这个C++层的onDestroyEntity是在__py_pyDestroyEntity传进来的参数，可以传两个参数（deleteFromDB，writeToDB），就第一个deleteFromDB意思就是要删除这个entity在db的数据， 判断了是否要与db数据关联与这个deleteFromDB的标志，如果都为True的话，那么通知dbmgr，removeEntity； destroyEntity再调用的是APP的destroyEntity，然后在APP的destroyEntity里的entity列表会删除掉此entity，然后再调用entity的C++层的destroy，然后entity的destroy方法其中调用了entity的onDestroy， 并且调用了scriptTimers_.cancelAll();，取消了所有定时器，然后entity的onDestroy里，如果callScript是为True的话，那么回调python层的onDestroy，然后执行了跟实体类型相关的销毁，有调用python层的onDestroy； cellapp的entity销毁看entity.cpp entity_macro.h entity_app.h；baseapp的base/proxy销毁看base.cpp entity_macro.h entity_app.h； 看demo里，是包装一个方法destroySelf，没有直接调用python层的destroy，而是在里面进行判断了是否有cell，先调用C++提供的python的destroyCellEntity，当cell被销毁后会调用base的python层的onLoseCell，然后再调用destroySelf，这次才直接调用C++提供给python层的destroy。Demo这样写是为了更安全的destroy demo的cellapp的entity的离开场景设计：在avatar的python层的teleport的onDestroy方法中调用base的space的logoutSpace方法， logoutSpace方法又调用了cell上的entity的onLeave 报错Components::findComponents: find {}({})…，原因是在Components::findComponents方法里请求machine找某组件时，没回应，循环多次而引起的报错 报错BundleBroadcast::receive: failed! It can be caused by the firewall, the broadcastaddr, etc.””Maybe broadcastaddr is not a LAN ADDR, or the Machine process is not running. 原因是组播后，调用BundleBroadcast::receive时select超时多次而引起的报错 ScriptTimers类：存在于Entity上成员变量，主要是保存无数个Entity在py层创建的定时器；addTimer成员函数最终还是getApp()-&gt;addTimer EntityScriptTimerHandler类继承于TimerHandler：是在Entity addTimer操作时，new出来时将entity的this存进去（详细请查看entity_macro.h的pyAddTimer,python层的timer可以循环触发，有repeat参数），随着addTimer时存放在g_pApp-&gt;timers().add里面，这样子app在processTimer时就可以调用这个对象的handler里再调用entity的onTimer方法，这里定时器的时间是按照自定义帧为单位的 addTimer:第一个参数是第一次执行距离现在的时间，第二个参数是执行的间隔时间（除第一次外），如果是0那么代表只执行一次 在这里代码的意思是如果参数repeatOffset也就是第二个参数，计算出来没达到一帧，那么就算是一帧，因为服务端是按照一秒几帧进行循环的，不能比一帧还小的单位，假如一秒是执行10帧，第二个参数是0.05秒（50毫秒），那么0.05 * 10等于0.5，0.5没达到一帧，那么算成一帧 g_kbetime:是按照自定义帧数来计算的，默认是一秒10帧的话，那么在1秒内会update10次，每update一次，g_kbetime就会增加一次，想要获取当前是g_kbetime总共经历update多少秒，可以g_kbetime除以自定义每秒多少帧，按照默认，g_kbetime / 10，得出经历了多少秒，kbe提供了一个方法gameTimeInSeconds。这个是用来给逻辑timer进行判断 与timer相关的： isOnGroud：这个就是服务器发过来的位置坐标可能不是在地面上的。这时直接去渲染的话人物就会离地，但是通过这个标志位可以避勉这种错误的渲染，自己在客户端处理一下不让渲染出来的人物飞起来。 onLogOnAttempt：这个是dbmgr有在线信息，所以在BaseApp上调用的一个方法 登录流程：client连接loginApp，然后loginApp向dbmgr查询accountinfos，并且查询是否已有在线记录，dbmgr回loginApp查询结果信息，loginApp根据结果将信息发送给baseAppMgr，baseAppMgr就能发送相关信息给BaseApp，如果有在线信息，那么表示已存在，否则向dbmgr查询account的entity信息 initClientBasePropertys：（method）Proxy初始化client的信息，并发送onUpdatePropertys entity_def的interfaces是的继承不能是多个相同的，例如npcobject继承gameobject，但是monster继承了npcobject，却又继承了gameobject PyObject_CallFunction：是调用一个函数对象 PyObject_CallObject：是调用一个python类的__call__ Python的__getattr__与__getattribute__的区别：当访问某个实例属性时， getattribute会被无条件调用，如未实现自己的getattr方法，会抛出AttributeError提示找不到这个属性，如果自定义了自己getattr方法的话，方法会在这种找不到属性的情况下被调用， 日志管理：kbe的日志管理，在进行写入文件是用了第三方库log4，各个服未进行连接之前是在写入各自服的日志，与Log服连接之后，服将日志数据发送到log服，由log服管理 –cid:(必须设置) 类型为uint64,全名component id,每个进程都有一个唯一ID，唯一ID在合适的时候用于区分他们之间的身份。 –gus:(可选设置) 类型为uint16,全名genUUID64 sections，这个值会被引擎KBEngine.genUUID64函数用到，设置为不同的值genUUID64将在 不同的区间产生唯一uuid。 这个值如果能在多个服务组进程之间保持唯一性，那么在合服的时候能够带来一定的便利性。 例如：游戏服A和游戏服B中的物品在数据库中存储的ID都使用genUUID64生成，那么在合服的时候能够直接向一张表中合并数据。 (注意：如果gus超过65535或者小于等于0，genUUID64将只能保证当前进程唯一) 定义文件的格式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191&lt;root&gt; //该实体的父类def //这个标签只在Entity.def中有效，如果本身就是一个接口def则该标签被忽略 &lt;Parent&gt; Avatar &lt;/Parent&gt; //易变属性同步控制 &lt;Volatile&gt; //这样设置则总是同步到客户端 &lt;position/&gt; //没有显式的设置则总是同步到客户端 &lt;!-- &lt;yaw/&gt; --&gt; //设置为0则不同步到客户端 &lt;pitch&gt; 0 &lt;/pitch&gt; //距离10米及以内同步到客户端 &lt;roll&gt; 10 &lt;/roll&gt; &lt;/Volatile&gt; //注册接口def，类似于C#中的接口 //这个标签只在Entity.def中有效，如果本身就是一个接口def则该标签被忽略 &lt;Implements&gt; //所有的接口def必须写在entity_defs/interfaces中 &lt;Interface&gt; GameObject &lt;/Interface&gt; &lt;/Implements&gt; &lt;Properties&gt; //属性名称 &lt;accountName&gt; //属性类型 &lt;Type&gt; UNICODE &lt;/Type&gt; // (可选) //属性的自定义协议ID，如果客户端不使用kbe配套的SDK来开发，客户端需要开发跟kbe对接的协议, //开发者可以定义属性的ID便于识别，c++协议层使用一个uint16来描述，如果不定义ID则引擎会使用 //自身规则所生成的协议ID,这个ID必须所有def文件中唯一 &lt;Utype&gt; 1000 &lt;/Utype&gt; //属性的作用域 (参考下方:属性作用域章节) &lt;Flags&gt; BASE &lt;/Flags&gt; // (可选) //是否存储到数据库 &lt;Persistent&gt; true &lt;/Persistent&gt; // (可选) //存储到数据库中的最大长度 &lt;DatabaseLength&gt; 100 &lt;/DatabaseLength&gt; // (可选，不清楚最好不要设置) //默认值 &lt;Default&gt; kbengine &lt;/Default&gt; // (可选) //数据库索引，支持UNIQUE与INDEX &lt;Index&gt; UNIQUE &lt;/Index&gt; &lt;/accountName&gt; ... ... &lt;/Properties&gt; &lt;ClientMethods&gt; //客户端暴露的远程方法名称 &lt;onReqAvatarList&gt; //远程方法的参数 &lt;Arg&gt; AVATAR_INFOS_LIST &lt;/Arg&gt; // (可选) //方法的自定义协议ID，如果客户端不使用kbe配套的SDK来开发，客户端需要开发跟kbe对接的协议, //开发者可以定义属性的ID便于识别，c++协议层使用一个uint16来描述，如果不定义ID则引擎会使用 //自身规则所生成的协议ID,这个ID必须所有def文件中唯一 &lt;Utype&gt; 1001 &lt;/Utype&gt; &lt;/onReqAvatarList&gt; ... ... &lt;/ClientMethods&gt; &lt;BaseMethods&gt; // Baseapp暴露的远程方法名称 &lt;reqAvatarList&gt; // (可选) //定义了此标记则允许客户端调用,否则仅服务端内部暴露 &lt;Exposed/&gt; // (可选) //方法的自定义协议ID，如果客户端不使用kbe配套的SDK来开发，客户端需要开发跟kbe对接的协议, //开发者可以定义属性的ID便于识别，c++协议层使用一个uint16来描述，如果不定义ID则引擎会使用 //自身规则所生成的协议ID,这个ID必须所有def文件中唯一 &lt;Utype&gt; 1002 &lt;/Utype&gt; &lt;/reqAvatarList&gt; ... ... &lt;/BaseMethods&gt; &lt;CellMethods&gt; // Cellapp暴露的远程方法名称 &lt;hello&gt; // (可选) //定义了此标记则允许客户端调用,否则仅服务端内部暴露 &lt;Exposed/&gt; // (可选) //方法的自定义协议ID，如果客户端不使用kbe配套的SDK来开发，客户端需要开发跟kbe对接的协议, //开发者可以定义属性的ID便于识别，c++协议层使用一个uint16来描述，如果不定义ID则引擎会使用 //自身规则所生成的协议ID,这个ID必须所有def文件中唯一 &lt;Utype&gt; 1003 &lt;/Utype&gt; &lt;/hello&gt; &lt;/CellMethods&gt;&lt;/root&gt; 属性作用域如果一个属性的作用域分为多个部分，那么在实体的对应部分都存在该属性。 存在于实体多个部分的属性只能从属性的源头进行修改，其他部分会得到同步。 请参考如下表：（S与SC或者C都代表属性包含这个部分，不同的是S代表属性的源头，C代表数据由源头同步，SC代表实体的real部分才是源头，ghosts部分也是被同步过去的） [类型] [ClientEntity] [BaseEntity] [CellEntity] BASE - S - BASE_AND_CLIENT C S - CELL_PRIVATE - - S CELL_PUBLIC - - SC CELL_PUBLIC_AND_OWN C - SC ALL_CLIENTS C(All Clients) - SC OWN_CLIENT C - S OTHER_CLIENTS C(Other Clients) - SC Python层的KBEngine.time()是调用的App的time()，例如BaseApp，而BaseApp的time()是放回g_kbetime KBEngine逻辑也可以组合。 继承的只是interface， demo没有很严格的实现为interface。 你可以将要暴露的网络接口定义在interface中，你可以实现组件挂在interface脚本下面，这样继承了interface实际上他包含了一个组件就是组合了 在BaseApp上的python entity都是继承Base类的，在CellApp上的python entity都是继承Entity类，如果BaseApp的entity的属性标志是设置成BaseAndClient，才会每次更改属性会实时同步属性给客户端，可以看Base类的onDefDataChange函数，CellApp上就看Entity类的onDefDataChange函数 kbengine.xml里的externalAddress和baseapp的externalAddress、loginapp的externalAddress是强制指定外部ip，kbengine是这么解释的： 强制指定外部IP地址或者域名，在某些网络环境下，可能会使用端口映射的方式来访问局域网内部的KBE服务器，那么KBE在当前 的机器上获得的外部地址是局域网地址，此时某些功能将会不正常。例如：账号激活邮件中给出的回调地址,登录baseapp。 注意：服务端并不会检查这个地址的可用性，因为无法检查。 如果这个被指定后，引擎只会发会这个地址 FixedDictType:这个类型在addStreamEx时会检查字典里的key是不是跟设计的也就是alias.xml里设计的key一样，如果不是一样，会报这类错误：&quot;FixedDictType::addToStreamEx: {} not found key[{}]. keyNames[{}]\n&quot; 图说明了在python层只要改变了position那么会调用pySetPosition，而pySetPosition是会调用onDefDataChanged，而在这之前会调用Network::FixedMessages::MSGInfo* msgInfo = Network::FixedMessages::getSingleton().isFixed(&quot;Property::position&quot;);因为要给这个属性定义ID，所以会发现messages_fixed.xml还可以给属性进行固定ID，不过这个只是给特别的属性进行固定ID而已，而且这个固定会固定所有的python类的指定属性的ID，就例如这个position属性，而且这里巧妙的调用了onDefDataChanged 12Network::FixedMessages::MSGInfo* msgInfo = Network::FixedMessages::getSingleton().isFixed("Property::spaceID") C++层会调用python层的onEntitiesEnabled时是这个实体已经成为client的代理 FixedDictType如果有ImplementedBy了的话，那么C++层的createFromStream即调用了python层的createFromDict方法，这个方法是数据库数据转换成内存数据调用的；C++层的addToStream即调用了python层的asDict方法，这个方法是内存数据要换成数据库数据或者网络流数据时调用的，也就是说客户端会得到的数据都是asDict之后的，注意是有ImplementedBy了后 FixedDictType是不传输里面的key的，详情可以看FixedDictType::addToStreamEx这个方法，只传里面的key相对应的值 在alias.xml里定义的类型别名，根本类型如果是FixedType，那么kbengine C++底层是实现了一个python类型（FixedType）,然后在实例化时调用了，也就是从数据到对象，调用了createFromDict(obj)，然后再调用isSameType，判断是否是同类型。 不能在createFromDict里调用deepcopy，因为deepcopy会把源对象的所有信息，包括源对象保存的类型信息拷贝到目标对象，因为createFromDict时的obj对象还是FixedType。 Alias.xml里面的类型是不能初始化的 FixedDictType:createFromDict时 是最底层级最先createFromDict，然后才一级级往上createFromDict，而addstream时是最高层级先asDict再往下一层级asDict，这里的层级是指包含的属性，可以参考datatype.cpp的FixedDictType的createFromStream和addToStream及FixedDict.cpp的initialize KBEngine的BaseApp的base和proxy是有区别的，proxy继承于base，但是proxy类有与客户端的mailbox绑定的功能，也就是某个proxy object与客户端建立绑定，proxy就可以利用self.client直接与客户端通信 如果要接平台接口的话，kbengine有提供Interfaces的一条进程/App在进行管理，bigworld是没有的，玩家进行账号创建或者登录时，DBMgr都会发送给Interfaces消息， 登陆时发送的登陆消息，详情请看dbmgr/interfaces_handler.cpp的279行 createAccount方法： 凡是将Entity化的都会使用这个宏 ENTITY_HEADER，例如Base ENTITY_HEADER这个宏声明及定义了很多entity的相关信息，有entityID之类的信息 凡是将python子类化都会使用这个宏 BASE_SCRIPT_HREADER，例如Base KBEngine有提供了断线重连，BaseApp提供了方法给客户端断线重连，需要懂的知识点 与client交互的Proxy类型，当这个实体与客户端建立实体联系后，Proxy会产生一个key，存在在Proxy的成员rnnUID，这个rnnUUID是方便断线重连时进行校验，重连后会再产生key 方法如下 会校验key（就是rnnUUID） 详细请看baseapp.cpp的reLoginBaseapp函数，在2700多行那 在启动服务器时就会就会读取alias.xml，然后一一创建类型，一一调用initialize，例如是dict类型的话， 然后读取数据结构类型里的属性类型，字段解析创建字段 最终调用addDataType加入到数据类型集 Witness类：aoiEntities：观察到的entity witnesses_:保存着所有观察到我的entity witnesses_count_：观察到的entitycount 客户端连接服务端，都会请求importClientMessage 和importClientEntityDef importClientMessage是连接loginapp和baseapp都会请求，importClientEntityDef只有在连接baseapp之后才会请求 kbe底层的Space是维护了一个coordinateSystem_，这是一个坐标系统，进入到这个space的entity都会加入到这个坐标系统里面，这个坐标系统会维护加入到这个坐标系统里面的entity，space-&gt;addEntityAndEnterWorld(pEntity); 当entity首次进入cell里，如果有客户端的话，那么会设置一个观察者 witness，会调用setWitness函数，而setWitness函数里会调用attach函数，attach函数会调用onAttach函数，onAttach函数会通知客户端onEntityEnterWorld，而当entity进行teleport的时候，那么传送到目的地时，entity不会调用setWitness，而是调用entity的createWitnessFromStream，然后这个方法会调用witness的createFromStream，而在reqTeleportToCellApp函数里，是在createFromStream后，再调用onEnterWorld，而函数里面会判断如果有witness时，会发送onEntityEnterSpace的方法，也就是首次登录进场景是发送onEntityEnterWorld的方法，会调用onGetWitness，teleport进入场景的是发送onEntityEnterSpace的方法，会调用Space类的addEntityAndEnterWorld函数，不会调用onGetWitness 总结：也就是当A玩家拥有witness时会收到服务端发送的onEntityEnterWotld，并且会收到服务端发送的onEntityEnterSpace,当B玩家登录或者传送时，在A玩家的视野里，A玩家都会收到服务端发送的onEntityEnterWorld，当A玩家服务端调用teleport时，A玩家客户端会收到服务端发送来的onEntityEnterSpace，当A玩家离线时也就是不拥有witness时收到服务端发送的onEntityLeaveWorld 客户端通过调用cellCall调用cell entity的接口，onRemoteCallCellMethodFromClient]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
        <tag>KBEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBEngine源码解读三]]></title>
    <url>%2F2019%2F06%2F28%2FKBEngine%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%89%2F</url>
    <content type="text"><![CDATA[BaseApp::createEntityAnywhere123456789101112// 把Entity的类型和参数全部序列化，然后发送给Baseappmgr，由Baseappmgr::reqCreateEntityAnywhere负责创建。BaseApp::createEntityAnywhere// 首先会找一个没有一个实体的baseapp或者是所有baseapp中负载最小的baseapp。// 然后再调用这个baseapp的onCreateEntityAnywhere函数--&gt; Baseappmgr::reqCreateEntityAnywhere// 把数据反序列化，然后调用createEntity函数创建。创建完成后，如果发起方和创建方是相同的，// 则直接调用_onCreateEntityAnywhereCallback返回创建结果。如果不相同，则发送消息给发起方，// 调用发起方的onCreateEntityAnywhereCallback函数返回结果。--&gt; BaseApp::onCreateEntityAnywhere--&gt; BaseApp::onCreateEntityAnywhereCallback //调用_onCreateEntityAnywhereCallback// 如果有设置python的callback，就调用python的callback。如果是其他baseapp创建的，则创建并保存成EntityCall。-&gt; BaseApp::_onCreateEntityAnywhereCallback Baseapp::createEntityAnywhereFromDBID12345678910111213141516171819Baseapp::createEntityAnywhereFromDBID// 首先会找一个没有一个实体的baseapp或者是所有baseapp中负载最小的baseapp。// 然后再调用这个baseapp的onGetCreateEntityAnywhereFromDBIDBestBaseappID函数--&gt; Baseappmgr::reqCreateEntityAnywhereFromDBIDQueryBestBaseappID--&gt; Baseapp::onGetCreateEntityAnywhereFromDBIDBestBaseappID// 常见DBTaskQueryEntity，添加到bufferedDBTasksMaps_中，然后有task完成任务--&gt; Dbmgr::queryEntity// 从数据库中查询相关Entity的信息-&gt; DBTaskQueryEntity::db_thread_process// 回到主线程，因为query mode是1，所以调用onCreateEntityAnywhereFromDBIDCallback-&gt; DBTaskQueryEntity::presentMainThread--&gt; Baseapp::onCreateEntityAnywhereFromDBIDCallback// 消息中包含了要在那个Baseapp上创建，就是前面决定的那个Baseapp。--&gt; Baseappmgr::reqCreateEntityAnywhereFromDBID// 创建Entity，并调用回调函数onCreateEntityAnywhereFromDBIDOtherBaseappCallback，// 如果是同一个Baseapp就直接回调，否则就是RPC回调--&gt; Baseapp::createEntityAnywhereFromDBIDOtherBaseapp// 如果有python的callback id，就调用python的callback函数。--&gt; Baseapp::onCreateEntityAnywhereFromDBIDOtherBaseappCallback Baseapp::createCellEntityInNewSpace123456789101112// 在cellapp上创建一个空间(space)并且将该实体的cell创建到这个新的空间中，它请求通过cellappmgr来完成。Baseapp::createCellEntityInNewSpace// 如果CellappIndex &gt; 0，那么用这个index模总的cellapp数量，得到目标cellapp。// 如果CellappIndex == 0，那么找到负载最小的cellapp作为目标cellapp。--&gt; Cellappmgr::reqCreateCellEntityInNewSpace// 通过SpaceMemorys::createNewSpace(spaceID, entityType)创建一个新的SpaceMemory（就是一个space）。// 并且创建Cellapp上的Entity，设置Baseapp的EntityCall，如果有Client，则还设置Client的EntityCall，并创建Witness。// 把新创建的Entity加入到SpaceMemory中。// 在SpaceMemory的构造函数/析构函数中，还会调用Cellappmgr::updateSpaceData。Cellappmgr好像也维护了// 每个Cellapp中的Space信息，所有需要更新一下。--&gt; Cellapp::onCreateCellEntityInNewSpaceFromBaseapp--&gt; Baseapp::onEntityGetCell Baseapp::createCellEntity12345678// 请求在一个cell里面创建一个关联的实体。Baseapp::createCellEntity--&gt; Cellapp::onCreateCellEntityFromBaseapp// 根据spaceID找到对应的SpaceMemory，然后在该SpaceMemory中创建Entity，// 设置Baseapp的EntityCall，如果有Client，则还设置Client的EntityCall，并创建Witness。// 把新创建的Entity加入到SpaceMemory中。-&gt; Cellapp::_onCreateCellEntityFromBaseapp--&gt; Baseapp::onEntityGetCell Cell目前Cellapp中的Cell和Cells都只有很简单的骨架，没有任何实际功能，也基本没有其他地方用到。所以只是一个占位符，没有相应的功能。 SpaceBaseapp中也有个Sapce（是Entity的子类），它其实只是Cellapp中Space的一个句柄，用来关联操作Cellapp中的Space的。真正的Space是在Cellapp中的。Cellapp中的Space的c++类是SpaceMemory（也有个Space，但是里面没有东西，也没用到）。SpaceMemory中包含了所有在这个Space中的Entity（Cellapp中的Entity）。 CoordinateSystem有x，y，z三轴的CoordinateNode*的双向链表组成。每插入一个CoordinateNode，先放入三个双向链表的头，然后在update(Node)。每个双向链表中的Node，都是按照x，y或者z值从小到大排序的。如果xyz之发生了变化，那么update(Node)会跟新链表，让其有序。EntityCoordinateNode继承自CoordinateNode，关联了一个Entity。 SpaceViewer将Entity的position和rotation发送给Console，在GUIConsole的SpaceViewer里面可以查看Entity的信息 RangeTrigger需要一个中心点CoordinateNode，和xz范围，和y范围。然后会创建两个RangeTriggerNode来确定范围，如果进入范围了，会调用onEnter(CoordinateNode pNode)虚函数，如果离开范围了，会调用onLeave(CoordinateNode pNode)虚函数。 ViewTriggerRangeTrigger的子类，通过实现onEnter/onLeave，把onEnter/onLeave的消息传递给关联的Witness（这个是RangeTrigger的origin对应的Entity的Witness）。调用Witness::onEnterView/Witness::onLeaveView。 TrapTriggerRangeTrigger的子类，通过实现onEnter/onLeave，把onEnter/onLeave的消息传递给关联的ProximityController。调用ProximityController::onEnter/Witness::onLeave。 ProximityController包含一个TrapTrigger，通过onEnter/onLeave，调用关联的Entity的onEnterTrap/onLeaveTrap WitnessWitness就是客户端在Cellapp中的一个代理，cellapp将实体的View内的其他实体的信息不断的通过Witness同步给客户端。Entity A进入Entity B的viewRadius_范围内，才算进入了Entity B的View。而Entity A要离开Entity B的View，需要离开Entity B的viewRadius_ + viewHysteresisArea_才行。Witness会将进入本View的Entity记录在viewEntities_和viewEntities_map_中。在update()中，会把所有在view中的Entity的信息发送给本Entity的客户端。如果关掉了coordinate_system，那么View的功能就不起作用了，即本View中没有其他的Entity，也不会同步了。Entity A进入Entity B的View后，在Entity A的witnesses_中会记录Entity B的ID，表示Entity A被Entity B目击到了。 有三种状态，一（将要进入视野范围内）：其他玩家进入拥有者玩家的视野范围，那么将其他玩家在拥有者玩家的Witness里的引用的状态更改成普通状态，并同步其他玩家的客户端信息与位置信息给拥有者玩家，通知拥有者玩家有其他玩家进入视野范围；二（将要离开视1野范围内）：其他玩家离开拥有者玩家的视野范围，那么通知拥有者玩家有其他玩家要离开，并删除其他玩家在拥有者玩家的Witness里的引用；三（普通状态，还在视野范围内），同步还在视野范围的其他玩家的volatile信息给拥有者玩家。一直在update Updatable用来描述一个总是会被更新的对象。Updatables用来管理所有的Updatable。Cellapp每个tick都会调用Updatables::update, 这个函数会调用所有的Updatable来更新状态。需要实现不同的Updatable来完成不同的更新特性。 Controllers每个Entity都包含一个Controllers的实例，来管理所有的Controller。Entity有对应的函数来创建对应的Controller，然后添加到Controllers实例中来管理。目前有的Controller有MoveController，ProximityController和TurnController。如果Controller需要每帧更新，那么是通过继承Updatable来实现的。 globalDataglobalData是通过GlobalDataClient和GlobalDataServer来实现的。Dbmgr在初始化的时候会创建三个GlobalDataServer的实例，分别对应globalData，baseAppData和cellAppData。EntityApp在初始化的时候，会创建GlobalDataClient对应globalData的GlobalDataServer。Baseapp在初始化时，会创建GlobalDataClient对应baseAppData的GlobalDataServer。Cellapp在初始化时，会创建GlobalDataClient对应cellAppData的GlobalDataServer。然后只要client的map发生变化，会自动同步到所有的baseApp（对应baseAppData），或者所有的cellApp（对应cellAppData），或者所有的baseApp和cellApp（对应globalData）。 EntityCallEntityCall和EntityComponentCall都继承自EntityCallAbstract，两者的行为很类似。只是EntityCall是可以远程调用Entity的方法，而EntityComponentCall是远程调用EntityComponent的方法。EntityCallAbstract中存储了远端机器的COMPONENT_ID和Address，在调用远程方法时（EntityCallAbstract::sendCall），会通过这两个属性找到对应的Channel，然后通过这个Channel把调用消息发送给对应的远程主机。当然其中还包括ENTITY_ID和对应方法的信息。在构建远程方法时（EntityCallAbstract::newCall_），如果远程主机是Baseapp，则会将目标函数BaseApp::onEntityCall编码到消息中，如果是Cellapp，则目标函数是CellApp::onEntityCall。如果是客户端远程调用服务器的方法，则如果是BaseApp的方法，那么目标方法是BaseApp::onRemoteMethodCall，如果是CellApp的方法，那么目标方法是BaseApp::onRemoteCallCellMethodFromClient。 下面以BaseApp::onEntityCall为例讲解后面的步骤。在这个方法中，会先根据ENTITY_ID从本进程的所有Entity中找到对应的Entity。然后根据ENTITYCALL_TYPE来做不同的处理。如果是ENTITYCALL_TYPE_BASE，则直接调用这个Entity的onRemoteMethodCall方法（这个方法会调用对应的python方法）。如果是ENTITYCALL_TYPE_CELL_VIA_BASE，则会通过pEntity-&gt;cellEntityCall()得到cell部分的EntityCall，然后在转发这个EntityCall给Cell部分。如果是ENTITYCALL_TYPE_CLIENT_VIA_BASE，则会通过pEntity-&gt;clientEntityCall()得到对应Client的EntityCall，然后转发这个EntityCall给Client。 注册流程123456789101112131415161718Unity3d:CreateAccount--&gt; Loginapp::reqCreateAccount // 看看注册是否开放，传入的注册信息是否符合要求，是否已经有相同的账号还在注册中的。// 还会调用python层的回调函数onRequestCreateAccount来处理。// 如果中途失败了，则直接调用ClientInterface::onCreateAccountResult返回错误。-&gt; Loginapp::_createAccount // 通过findBestInterfacesHandler()找到一个Interface来处理，如果没有第三方Interface，// 那么默认是InterfacesHandler_Dbmgr来处理，如果有第三方Interface。那么是InterfacesHandler_Interfaces// 来处理。InterfacesHandler_Interfaces::createAccount会把消息转发给第三方Interface来处理。// 下面的流程针对InterfacesHandler_Dbmgr来说的--&gt; Dbmgr::reqCreateAccount// 会根据是不是mail账号，分别创建DBTaskCreateMailAccount或者DBTaskCreateAccount，// 然后添加到ThreadPool中，有task来完成相应的任务。下面以DBTaskCreateAccount为例说明。-&gt; InterfacesHandler_Dbmgr::createAccount-&gt; DBTaskCreateAccount::writeAccount // 创建新账号，存入数据库，这个函数在其他线程中执行-&gt; DBTaskCreateAccount::presentMainThread // 回到主线程，把创建结果返回给Loginapp--&gt; Loginapp::onReqCreateAccountResult // 把结果交给python回调函数处理onCreateAccountCallbackFromDB--&gt; Client::onCreateAccountResult 登录流程登录 Step112345678910111213141516171819202122232425Unity3d:login// 检查数据合法性，调用python回调函数onRequestLogin，// 如果中途出错了，直接调用ClientInterface::onLoginFailed返回错误--&gt; Loginapp::login// 通过findBestInterfacesHandler()找到一个Interface来处理，如果没有第三方Interface，// 那么默认是InterfacesHandler_Dbmgr来处理，如果有第三方Interface。那么是InterfacesHandler_Interfaces// 来处理。InterfacesHandler_Interfaces::loginAccount会把消息转发给第三方Interface来处理（InterfacesInterface::onAccountLogin）。// 下面的流程针对InterfacesHandler_Dbmgr来说的--&gt; Dbmgr::onAccountLogin// 创建DBTaskAccountLogin加入到ThreadPool中，有task来完成任务-&gt; InterfacesHandler_Dbmgr::loginAccount-&gt; DBTaskAccountLogin::db_thread_process // 查询数据库，验证账号合法性，这个函数在其他线程中执行。-&gt; DBTaskAccountLogin::presentMainThread // 回到主线程，把账号信息发送回Loginapp// 调用python的回调函数onLoginCallbackFromDB，如果componentID&gt;0（说明当前账号仍然存活于某个baseapp上），// 则调用Baseappmgr::registerPendingAccountToBaseappAddr，否则调用Baseappmgr::registerPendingAccountToBaseapp，// 下面以Baseappmgr::registerPendingAccountToBaseapp来说明--&gt; Loginapp::onLoginAccountQueryResultFromDbmgr// 找到当前负载最低的Baseapp，把账号信息发送给该Baseapp。--&gt; Baseappmgr::registerPendingAccountToBaseapp// 得到当前Baseapp的IP和port（包括TCP和UDP），再发送回Baseappmgr--&gt; Baseapp::registerPendingLogin--&gt; Baseappmgr::onPendingAccountGetBaseappAddr-&gt; Baseappmgr::sendAllocatedBaseappAddr // 把IP和port等信息，发送给Loginapp--&gt; Loginapp::onLoginAccountQueryBaseappAddrFromBaseappmgr // 把IP和port等信息，发送给Client--&gt; ClientInterface::onLoginSuccessfully 登录 Step21234567891011121314Unity3d:loginBaseapp //尝试在指定Baseapp上登录// 检查登录 处理重复登录 向数据库查询账号详细信息--&gt; Baseapp::loginBaseapp // 新建DBTaskQueryAccount，加入到Buffered_DBTasks，然后由task来执行操作--&gt; Dbmgr::queryAccount-&gt; DBTaskQueryAccount::db_thread_process // 查询数据库，得到账号详细信息-&gt; DBTaskQueryAccount::presentMainThread // 回到主线程，把信息发送给Baseapp// 创建Proxy，并且绑定客户端的EntityCall--&gt; Baseapp::onQueryAccountCBFromDbmgr// 调用pEntity-&gt;initClientBasePropertys()，在这个函数调用中，把这个Proxy的一些属性，// 通过ClientInterface::onUpdatePropertys远程调用，发回给了客户端。// 最后调用ClientInterface::onCreatedProxies，把proxy的ID等传给了客户端。-&gt; Baseapp::createClientProxies--&gt; ClientInterface::onCreatedProxies Backuper &amp; ArchiverBackuper是将entity置脏，而Archiver是将数据写入db，如果没有entity没有置脏，那么不会写入db。每帧都会调用Backuper和Archiver的tick函数。他们都会生成一个需要backup/archive的Entity的列表。然后根据设置的备份时间间隔，把列表分散到每个帧中，就是每个帧处理列表中的一部分。知道列表为空了，再重新生成一份列表。至于什么会加入到列表中，是根据shouldAutoBackup_/shouldAutoArchive_这两个flag来的，如果flag&gt;0就是true，默认这两个Flag都是1。如果flag == KBEngine.NEXT_ONLY，那么只有一次是true的，然后就会自动变成false。 如果此Entity没有dbid，那么DBMgr认为是insert类型，反之，update类型；如果是insert类型 且 Entity有个属性是ARRAY类型，那么这个会在Content的optable循环插入需要insert或update的数据。只要Entity的一个需要持久化的属性改变了，那么它就脏了，整个Entity会存档。 1234567891011121314151617// 在BaseApp上backup某个EntityBackuper::backup-&gt; Entity::writeBackupData-&gt; Entity::onBackup// 把需要backup的Cell Entity的ID发给Cellapp-&gt; Entity::reqBackupCellData// 找到对应的Entity，调用backupCellData--&gt; Cellapp::reqBackupEntityCellData// 通过函数addCellDataToStream序列化自己，然后用sha1做hash，// 得到的hash值和persistentDigest_的hash值做比较，如果改变了，// 就说明数据脏了，需要backup，并且把新的hash存到persistentDigest_中。// 只有数据脏了，才会把序列化的数据发送会Baseapp。-&gt; Entity::backupCellData// 找到对应的Entity，调用onBackupCellData--&gt; Baseapp::onBackupEntityCellData// 如果Cell数据没脏，那么什么也不做，如果脏了，就把cell数据存下来，并且清空persistentDigest_。-&gt; Entity::onBackupCellData 1234567891011121314151617181920212223242526// 在Baseapp上archive某个EntityArchiver::archive// 什么也没做，直接调用Cellapp的reqWriteToDBFromBaseapp-&gt; Entity::writeToDB(NULL, NULL, NULL);// 找到对应的Entity，调用writeToDB--&gt; Cellapp::reqWriteToDBFromBaseapp// 其中callbackid是0，如果Baseapp中的Entity的DBID &lt;= 0, 那么shouldAutoLoad就是0，否则是-1，extra2是null。// 调用python的回调函数onWriteToDB，还调用Entity::backupCellData()，如果数据脏了，// 就序列化并发送给Baseapp中的Entity保存。没脏，那就什么也不做。-&gt; Entity::writeToDB// 找到对应的Entity，调用onCellWriteToDBCompleted。--&gt; Baseapp::onCellWriteToDBCompleted// 调用python回调onWriteToDB。调用addPersistentsDataToStream，把base和cell部分的数据都序列化。// 计算sha1，并和persistentDigest_比较，如果没脏，就直接退出了，脏了，就记录新的hash到persistentDigest_，// 并把数据发送给Dbmgr。如果cell部分的数据脏了，那么persistentDigest_已经被清空了，所以肯定是脏的。-&gt; Entity::onCellWriteToDBCompleted// 新建DBTaskWriteEntity，让这个task来处理--&gt; Dbmgr::writeEntity// 把Entity数据写入数据库-&gt; DBTaskWriteEntity::db_thread_process // 返回写entity的结果， 成功或者失败-&gt; DBTaskWriteEntity::presentMainThread// 调用对应Entity的onWriteToDBCallback--&gt; Baseapp::onWriteToDBCallback// 如果有python的callback，就调用callback。-&gt; Entity::onWriteToDBCallback AllClients所有的目击到自己的Entity。 Entity::teleport 任何形式的teleport都被认为是瞬间移动的（可突破空间限制进入到任何空间）， 哪怕是在当前位置只移动了0.1米, 这就造成如果当前entity刚好在某个trap中， teleport向前移动0.1米但是没有出trap， 因为这是瞬间移动的特性我们目前认为entity会先离开trap并且触发相关回调, 然后瞬时出现在了另一个点， 那么因为该点也是在当前trap中所以又会抛出进入trap回调. Entity::teleportLocal如果是当前space上跳转则立即进行移动操作 Entity::teleportRefEntity如果是跳转到其他space上, 但是那个space也在当前cellapp上的情况时， 立即执行跳转操作(因为不需要进行任何其他关系的维护， 直接切换就好了)。 从当前Space移除，加入到目标Space。 如果要跳转的目标space在另一个cellapp上Entity::teleportRefEntityCall： 4.1 当前entity没有base部分， 不考虑维护base部分的关系， 但是还是要考虑意外情况导致跳转失败， 那么此时应该返回跳转失败回调并且继续 正常存在于当前space上。 4.2 当前entity有base部分， 那么我们需要改变base所映射的cell部分(并且在未正式切换关系时baseapp上所有送达cell的消息都应该不被丢失)， 为了安全我们需要做一些工作 123456789101112131415161718192021// 如果这个entity有base部分， 假如是本进程级别的传送，那么相关操作按照正常的执行// 如果是跨cellapp的传送， 那么我们可以先设置entity为ghost并立即序列化entity发往目的cellapp// 如果期间有base的消息发送过来， entity的ghost机制能够转到real上去， 因此传送之前不需要对base// 做一些设置，传送成功后先设置base的关系base在被改变关系后仍然有0.1秒的时间收到包继续发往ghost，// 如果一直有包则一直刷新时间直到没有任何包需要广播并且超时0.1秒之后的包才会直接发往real）, // 这样做的好处是传送并不需要非常谨慎的与base耦合// 传送过程中有任何错误也不会影响到base部分，base部分的包也能够按照秩序送往real。//// 有base会调用Baseapp上的Entity::onMigrationCellappStartEntity::teleportRefEntityCall// 我们需要将entity打包发往目的cellapp// 暂时不销毁这个entity, 把这个entity设置成ghost，等那边成功创建之后再回来销毁// 此期间的消息可以通过ghost转发给real// 如果未能正确传输过去则可以从当前cell继续恢复entity.-&gt; Entity::onTeleportRefEntityCall// 创建一个新的Entity，并从数据中反序列化，通知Baseapp上的Entity::onMigrationCellappEnd// 通知客户端离开了老的space，ClientInterface::onEntityLeaveSpace。// 进入新的Space。把结果发送回原来的Cellapp--&gt; Cellapp::reqTeleportToCellApp// 如果传送成功了，就销毁原来的Entity。如果失败了，就恢复Entity，并恢复成Real实体。--&gt; Cellapp::reqTeleportToCellAppCB Entity和Proxy等python类不能有虚函数原文链接 最近在做一些底层实体的扩展工作，发现了 如果 entity或者proxy中有虚函数的声明，那么程序在创建实体后 调用init方法时会抛出异常。于是我打开调试，看看为什么会这样，发现了很奇怪的事情： obj和entity应该是一个对象，但是调试信息里面obj的python结构是正常的，而entity的python结构是异常的。这时我才恍然大悟，原来是C++编译器给对象内存添加了虚函数表的指针导致了内存错位了。 可以看到，python的GC头部后面应该紧接的是PyObject，但是因为有虚函数，所以C++编译器在给的对象指针顶部添加了一个虚函数表指针数据，导致entity的值比(PyObject*)entity的值小个指针大小。所以 entity 以及派送类是不允许有虚函数以及虚继承的。 总结就是能够被python继承的类，C++对象模型顶部一定是PyObject，所以该类不能有虚函数以及虚继承等 破坏这个内存模型的操作。]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
        <tag>KBEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBEngine源码解读二组件互联]]></title>
    <url>%2F2019%2F06%2F22%2FKBEngine%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%BA%8C%E7%BB%84%E4%BB%B6%E4%BA%92%E8%81%94%2F</url>
    <content type="text"><![CDATA[server/Components组件在互相发现的时候用的是UDP广播，找到对应组件知道其IP和Port后，用TCP来建立并保持连接。 machine组件会监听20086端口，然后其他组件都会往这个端口广播自己的信息（RPC调用MachineInterface::onBroadcastInterface函数）。然后machine收到信息后会判断有效性（有没有和其他组件用了相同的设置），如果无效会发回一个无效的通知给该组件，然后该组件就会退出。如果有效的则，machine会记录和自己同一台物理机上的组件到自己的组件列表中（不同物理机的组件会被忽略掉）。 然后组件会有一个需要找的组件的类型的列表，对于每种类型，它都会广播MachineInterface::onFindInterfaceAddr这个消息到machine。machine收到这个消息后，会把已经注册过的本地同类型组件发回给该组件。如果没有找到对应的类型，会定时到下一个循环继续找，直到找到自己感兴趣的所有类型的组件为止。 external port和telnet的port都是通过配置文件指定好的，internal port传的零，就是让系统决定一个可用的随机端口，bind成功后在用getsockname()得到具体的端口。 下面是每个组件感兴趣的列表: 组件类型 感兴趣的类型 Cell app logger, dbmgr, cellAppMgr, baseAppMgr Base app logger, dbmgr, baseAppMgr, cellAppMgr Base app mgr logger, dbmgr, cellAppMgr Cell app mgr logger, dbmgr, baseAppMgr db mgr logger 在连接其他组件时，会调用被连接组件的XXX::onRegisterNewApp函数。当baseApp或者cellApp连接Dbmgr时，同样会调用Dbmgr::onRegisterNewApp。在这个函数中，如果连接者是baseApp或者cellApp，那么就会将自己注册到所有其他baseapp和cellapp中。主要是通过遍历已经注册在Dbmgr中的其他baseapp和cellapp，然后RPC调用相应的onGetEntityAppFromDbmgr。在被调用的onGetEntityAppFromDbmgr中，被调用者会去连接当前组件。这样就能让所有的baseApp和cellApp互相连接了。 运行时每个组件的互联情况：在Windows命令行中运行命令1netstat -ano | findstr &lt;PID&gt; 可以得到相应程序的网络活动数据。 db mgr： Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:32000 0.0.0.0:0 LISTENING 684 telnet TCP 0.0.0.0:50277 0.0.0.0:0 LISTENING 684 internal TCP TCP 127.0.0.1:50279 127.0.0.1:30099 ESTABLISHED 684 interfaces TCP 169.254.43.79:50277 169.254.43.79:50297 ESTABLISHED 684 cell app mgr TCP 169.254.43.79:50277 169.254.43.79:50299 ESTABLISHED 684 base app mgr TCP 169.254.43.79:50277 169.254.43.79:50301 ESTABLISHED 684 login app TCP 169.254.43.79:50277 169.254.43.79:50303 ESTABLISHED 684 cell app 1 TCP 169.254.43.79:50277 169.254.43.79:50305 ESTABLISHED 684 base app 2 TCP 169.254.43.79:50277 169.254.43.79:50310 ESTABLISHED 684 base app 3 TCP 169.254.43.79:50277 169.254.43.79:50315 ESTABLISHED 684 base app 1 TCP 169.254.43.79:50277 169.254.43.79:50321 ESTABLISHED 684 cell app 2 TCP 169.254.43.79:50277 169.254.43.79:50328 ESTABLISHED 684 cell app 3 TCP 169.254.43.79:50295 169.254.43.79:50267 ESTABLISHED 684 logger TCP [::1]:50281 [::1]:3306 ESTABLISHED 684 mysqld TCP [::1]:50282 [::1]:3306 ESTABLISHED 684 mysqld TCP [::1]:50283 [::1]:3306 ESTABLISHED 684 mysqld TCP [::1]:50284 [::1]:3306 ESTABLISHED 684 mysqld TCP [::1]:50285 [::1]:3306 ESTABLISHED 684 mysqld base app 1 Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:20015 0.0.0.0:0 LISTENING 7624 external TCP TCP 0.0.0.0:40001 0.0.0.0:0 LISTENING 7624 telnet TCP 0.0.0.0:50269 0.0.0.0:0 LISTENING 7624 internal TCP TCP 169.254.43.79:50269 169.254.43.79:50316 ESTABLISHED 7624 base app 2 TCP 169.254.43.79:50269 169.254.43.79:50317 ESTABLISHED 7624 base app 3 TCP 169.254.43.79:50269 169.254.43.79:50318 ESTABLISHED 7624 cell app 1 TCP 169.254.43.79:50293 169.254.43.79:50267 ESTABLISHED 7624 logger TCP 169.254.43.79:50315 169.254.43.79:50277 ESTABLISHED 7624 db mgr TCP 169.254.43.79:50319 169.254.43.79:50275 ESTABLISHED 7624 base app mgr TCP 169.254.43.79:50320 169.254.43.79:50272 ESTABLISHED 7624 cell app mgr TCP 169.254.43.79:50324 169.254.43.79:50274 ESTABLISHED 7624 cell app 2 TCP 169.254.43.79:50331 169.254.43.79:50276 ESTABLISHED 7624 cell app 3 UDP 0.0.0.0:20005 *:* 7624 external UDP base app 2 Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:20016 0.0.0.0:0 LISTENING 13068 external TCP TCP 0.0.0.0:40000 0.0.0.0:0 LISTENING 13068 telnet TCP 0.0.0.0:50270 0.0.0.0:0 LISTENING 13068 internal TCP TCP 169.254.43.79:50270 169.254.43.79:50306 ESTABLISHED 13068 cell app 1 TCP 169.254.43.79:50291 169.254.43.79:50267 ESTABLISHED 13068 logger TCP 169.254.43.79:50305 169.254.43.79:50277 ESTABLISHED 13068 db mgr TCP 169.254.43.79:50307 169.254.43.79:50275 ESTABLISHED 13068 base app mgr TCP 169.254.43.79:50309 169.254.43.79:50272 ESTABLISHED 13068 cell app mgr TCP 169.254.43.79:50311 169.254.43.79:50271 ESTABLISHED 13068 base app 3 TCP 169.254.43.79:50316 169.254.43.79:50269 ESTABLISHED 13068 base app 1 TCP 169.254.43.79:50322 169.254.43.79:50274 ESTABLISHED 13068 cell app 2 TCP 169.254.43.79:50329 169.254.43.79:50276 ESTABLISHED 13068 cell app 3 UDP 0.0.0.0:20006 *:* 13068 external UDP base app 3 Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:20017 0.0.0.0:0 LISTENING 3412 external TCP TCP 0.0.0.0:40002 0.0.0.0:0 LISTENING 3412 telnet TCP 0.0.0.0:50271 0.0.0.0:0 LISTENING 3412 internal TCP TCP 169.254.43.79:50271 169.254.43.79:50311 ESTABLISHED 3412 base app 2 TCP 169.254.43.79:50271 169.254.43.79:50312 ESTABLISHED 3412 cell app 1 TCP 169.254.43.79:50294 169.254.43.79:50267 ESTABLISHED 3412 logger TCP 169.254.43.79:50310 169.254.43.79:50277 ESTABLISHED 3412 db mgr TCP 169.254.43.79:50313 169.254.43.79:50275 ESTABLISHED 3412 base app mgr TCP 169.254.43.79:50314 169.254.43.79:50272 ESTABLISHED 3412 cell app mgr TCP 169.254.43.79:50317 169.254.43.79:50269 ESTABLISHED 3412 base app 1 TCP 169.254.43.79:50323 169.254.43.79:50274 ESTABLISHED 3412 cell app 2 TCP 169.254.43.79:50330 169.254.43.79:50276 ESTABLISHED 3412 cell app 3 UDP 0.0.0.0:20007 *:* 3412 external UDP cell app 1 Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:50001 0.0.0.0:0 LISTENING 2816 telnet TCP 0.0.0.0:50273 0.0.0.0:0 LISTENING 2816 internal TCP TCP 169.254.43.79:50289 169.254.43.79:50267 ESTABLISHED 2816 logger TCP 169.254.43.79:50303 169.254.43.79:50277 ESTABLISHED 2816 db mgr TCP 169.254.43.79:50304 169.254.43.79:50272 ESTABLISHED 2816 cell app mgr TCP 169.254.43.79:50306 169.254.43.79:50270 ESTABLISHED 2816 base app 2 TCP 169.254.43.79:50308 169.254.43.79:50275 ESTABLISHED 2816 base app mgr TCP 169.254.43.79:50312 169.254.43.79:50271 ESTABLISHED 2816 base app 3 TCP 169.254.43.79:50318 169.254.43.79:50269 ESTABLISHED 2816 base app 1 TCP 169.254.43.79:50325 169.254.43.79:50274 ESTABLISHED 2816 cell app 2 TCP 169.254.43.79:50332 169.254.43.79:50276 ESTABLISHED 2816 cell app 3 cell app 2 Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:50000 0.0.0.0:0 LISTENING 8280 telnet TCP 0.0.0.0:50274 0.0.0.0:0 LISTENING 8280 internal TCP TCP 169.254.43.79:50274 169.254.43.79:50322 ESTABLISHED 8280 base app 2 TCP 169.254.43.79:50274 169.254.43.79:50323 ESTABLISHED 8280 base app 3 TCP 169.254.43.79:50274 169.254.43.79:50324 ESTABLISHED 8280 base app 1 TCP 169.254.43.79:50274 169.254.43.79:50325 ESTABLISHED 8280 cell app 1 TCP 169.254.43.79:50288 169.254.43.79:50267 ESTABLISHED 8280 logger TCP 169.254.43.79:50321 169.254.43.79:50277 ESTABLISHED 8280 db mgr TCP 169.254.43.79:50326 169.254.43.79:50272 ESTABLISHED 8280 cell app mgr TCP 169.254.43.79:50327 169.254.43.79:50275 ESTABLISHED 8280 base app mgr TCP 169.254.43.79:50333 169.254.43.79:50276 ESTABLISHED 8280 cell app 3 cell app 3 Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:50002 0.0.0.0:0 LISTENING 8600 telnet TCP 0.0.0.0:50276 0.0.0.0:0 LISTENING 8600 internal TCP TCP 169.254.43.79:50276 169.254.43.79:50329 ESTABLISHED 8600 base app 2 TCP 169.254.43.79:50276 169.254.43.79:50330 ESTABLISHED 8600 base app 3 TCP 169.254.43.79:50276 169.254.43.79:50331 ESTABLISHED 8600 base app 1 TCP 169.254.43.79:50276 169.254.43.79:50332 ESTABLISHED 8600 cell app 1 TCP 169.254.43.79:50276 169.254.43.79:50333 ESTABLISHED 8600 cell app 2 TCP 169.254.43.79:50290 169.254.43.79:50267 ESTABLISHED 8600 logger TCP 169.254.43.79:50328 169.254.43.79:50277 ESTABLISHED 8600 db mgr TCP 169.254.43.79:50334 169.254.43.79:50272 ESTABLISHED 8600 cell app mgr TCP 169.254.43.79:50335 169.254.43.79:50275 ESTABLISHED 8600 base app mgr login app Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:20013 0.0.0.0:0 LISTENING 10896 external TCP TCP 0.0.0.0:31000 0.0.0.0:0 LISTENING 10896 telnet TCP 0.0.0.0:50278 0.0.0.0:0 LISTENING 10896 internal TCP TCP 169.254.43.79:21103 0.0.0.0:0 LISTENING 10896 http call back TCP 169.254.43.79:50296 169.254.43.79:50267 ESTABLISHED 10896 logger TCP 169.254.43.79:50301 169.254.43.79:50277 ESTABLISHED 10896 db mgr TCP 169.254.43.79:50302 169.254.43.79:50275 ESTABLISHED 10896 base app mgr cell app mgr Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:50272 0.0.0.0:0 LISTENING 10860 internal TCP TCP 169.254.43.79:50272 169.254.43.79:50300 ESTABLISHED 10860 base app mgr TCP 169.254.43.79:50272 169.254.43.79:50304 ESTABLISHED 10860 cell app 1 TCP 169.254.43.79:50272 169.254.43.79:50309 ESTABLISHED 10860 base app 2 TCP 169.254.43.79:50272 169.254.43.79:50314 ESTABLISHED 10860 base app 3 TCP 169.254.43.79:50272 169.254.43.79:50320 ESTABLISHED 10860 base app 1 TCP 169.254.43.79:50272 169.254.43.79:50326 ESTABLISHED 10860 cell app 2 TCP 169.254.43.79:50272 169.254.43.79:50334 ESTABLISHED 10860 cell app 3 TCP 169.254.43.79:50287 169.254.43.79:50267 ESTABLISHED 10860 logger TCP 169.254.43.79:50297 169.254.43.79:50277 ESTABLISHED 10860 db mgr base app mgr Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:50275 0.0.0.0:0 LISTENING 11652 internal TCP TCP 169.254.43.79:50275 169.254.43.79:50302 ESTABLISHED 11652 login app TCP 169.254.43.79:50275 169.254.43.79:50307 ESTABLISHED 11652 base app 2 TCP 169.254.43.79:50275 169.254.43.79:50308 ESTABLISHED 11652 cell app 1 TCP 169.254.43.79:50275 169.254.43.79:50313 ESTABLISHED 11652 base app 3 TCP 169.254.43.79:50275 169.254.43.79:50319 ESTABLISHED 11652 base app 1 TCP 169.254.43.79:50275 169.254.43.79:50327 ESTABLISHED 11652 cell app 2 TCP 169.254.43.79:50275 169.254.43.79:50335 ESTABLISHED 11652 cell app 3 TCP 169.254.43.79:50292 169.254.43.79:50267 ESTABLISHED 11652 logger TCP 169.254.43.79:50299 169.254.43.79:50277 ESTABLISHED 11652 db mgr TCP 169.254.43.79:50300 169.254.43.79:50272 ESTABLISHED 11652 cell app mgr machine Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:20099 0.0.0.0:0 LISTENING 12256 external TCP TCP 0.0.0.0:50268 0.0.0.0:0 LISTENING 12256 internal TCP UDP 0.0.0.0:20086 *:* 12256 UDP广播接收 UDP 127.0.0.1:20086 *:* 12256 UDP广播接收 UDP 169.254.43.79:20086 *:* 12256 UDP广播接收 logger Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:34000 0.0.0.0:0 LISTENING 11204 telnet TCP 0.0.0.0:50266 0.0.0.0:0 LISTENING 11204 external TCP TCP 0.0.0.0:50267 0.0.0.0:0 LISTENING 11204 internal TCP TCP 169.254.43.79:50267 169.254.43.79:50287 ESTABLISHED 11204 cell app mgr TCP 169.254.43.79:50267 169.254.43.79:50288 ESTABLISHED 11204 cell app 2 TCP 169.254.43.79:50267 169.254.43.79:50289 ESTABLISHED 11204 cell app 1 TCP 169.254.43.79:50267 169.254.43.79:50290 ESTABLISHED 11204 cell app 3 TCP 169.254.43.79:50267 169.254.43.79:50291 ESTABLISHED 11204 base app 2 TCP 169.254.43.79:50267 169.254.43.79:50292 ESTABLISHED 11204 base app mgr TCP 169.254.43.79:50267 169.254.43.79:50293 ESTABLISHED 11204 base app 1 TCP 169.254.43.79:50267 169.254.43.79:50294 ESTABLISHED 11204 base app 3 TCP 169.254.43.79:50267 169.254.43.79:50295 ESTABLISHED 11204 db mgr TCP 169.254.43.79:50267 169.254.43.79:50296 ESTABLISHED 11204 login app interfaces Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:30099 0.0.0.0:0 LISTENING 5324 TCP 0.0.0.0:33000 0.0.0.0:0 LISTENING 5324 telnet TCP 127.0.0.1:30040 0.0.0.0:0 LISTENING 5324 TCP 127.0.0.1:30099 127.0.0.1:50279 ESTABLISHED 5324 db mgr mysqld Socket 源地址 目标地址 状态 PID 目标组件 TCP 0.0.0.0:3306 0.0.0.0:0 LISTENING 4808 TCP 0.0.0.0:33060 0.0.0.0:0 LISTENING 4808 TCP 127.0.0.1:49670 127.0.0.1:49671 ESTABLISHED 4808 TCP 127.0.0.1:49671 127.0.0.1:49670 ESTABLISHED 4808 TCP [::]:3306 [::]:0 LISTENING 4808 TCP [::]:33060 [::]:0 LISTENING 4808 TCP [::1]:3306 [::1]:50281 ESTABLISHED 4808 db mgr TCP [::1]:3306 [::1]:50282 ESTABLISHED 4808 db mgr TCP [::1]:3306 [::1]:50283 ESTABLISHED 4808 db mgr TCP [::1]:3306 [::1]:50284 ESTABLISHED 4808 db mgr TCP [::1]:3306 [::1]:50285 ESTABLISHED 4808 db mgr]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
        <tag>KBEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBEngine源码解读一]]></title>
    <url>%2F2019%2F06%2F14%2FKBEngine%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%80%2F</url>
    <content type="text"><![CDATA[一. libs/common部分MemoryStream： 将常用数据类型二进制序列化与反序列化，内部封装了一个std::vector&lt;uint8&gt;。使用方法:1234567891011121314MemoryStream stream; stream &lt;&lt; (int64)100000000;stream &lt;&lt; (uint8)1;stream &lt;&lt; (uint8)32;stream &lt;&lt; "kbe";stream.print_storage();uint8 n, n1;int64 x;std::string a;stream &gt;&gt; x;stream &gt;&gt; n;stream &gt;&gt; n1;stream &gt;&gt; a;printf("还原: %lld, %d, %d, %s", x, n, n1, a.c_str()); Tasks: 任务Task的管理类，内部封装了std::vector&lt;Task *&gt;，通过调用process()来遍历所有Tash的process()虚函数。 TimersT&lt;T&gt;: 定时器管理类，内部用小顶堆管理所有的定时器。通过12TimerHandle add(TimeStamp startTime, TimeStamp interval, TimerHandler* pHandler, void * pUser); 来新增一个定时器，返回的TimerHandle是新增定时器的句柄，可以控制相应的定时器。用户需要继承TimerHandler类，并重载1virtual void handleTimeout(TimerHandle handle, void * pUser) 虚函数来实现定时器的回调。 内部预定义了两个TimersT&lt;T&gt;类12typedef TimersT&lt;uint32&gt; Timers;typedef TimersT&lt;uint64&gt; Timers64; 二. libs/network部分Address: 封装了ip和port Packet: 发送和接收的包的最小单位，继承自MemoryStream。在发送的时候Packet都是嵌套在Bundle的内部来使用的。 TcpPacket: 代表一个收到的TCP包，继承自Packet，这个包只是recv收到的字节流，并不是上层协议中的消息(Message)。 UDPPacket: 代表一个收到的UDP包，继承自Packet，这个包只是recv收到的字节流，并不是上层协议中的消息(Message)。 Bundle: 代表要发送的包的集合，内部有std::vector&lt;Packet*&gt;数组。内部的Packet根据TCP和UDP不同，有不同的最大字节数限制。TCP的一个Packet最多包含1460个字节，UDP是1472字节。也就是说序列化到Bundle中的数据会自动分包的。 BundleBroadcast: 继承自Bundle，可以方便的处理如:用UDP向局域网内广播某些信息，并处理收集相关信息。 EndPoint: 抽象一个Socket及其相关操作，隔离平台相关性。 FixedMessages: 从“server/messages_fixed_defaults.xml”中读入消息的定义，维护了一个消息名字到消息id的映射。 MessageHandlers: 每个MessageHandler类对应一个消息的处理，通过重载下面的虚函数来处理：1virtual void handle(Channel* pChannel, MemoryStream&amp; s) MessageHandlers维护MessageID -&gt; MessageHandler的映射。 在把MessageHandler加入到MessageHandlers时，如果是固定消息，那么MessageID是从FixedMessages中的设置中来的，否则就一个递增得到的（会避开固定消息的id）。 以Baseapp为例，所有的消息都是用宏定义在baseapp_interface.h/cpp中的，每个消息需要实现一个MessageHandler的子类和一个MessageArgs的子类（用来处理消息的参数）。并且会创建相应的实例，添加到messageHandlers(是Network::MessageHandlers的实例)中去。然后在这些子类的handle中又会去调用Baseapp中的相同名字的方法，最后这个消息其实是在Baseapp的同名方法中处理的。 PacketReceiver: 用来收Packet，收到后会转给PacketReader来处理。 PacketReader: 会利用MessageHandlers把包转成对应的Message，即相应的函数调用。 PacketSender: 用来发送Packet的。 Channel: 抽象一个Socket连接，每个EndPoint都有其对应的Channel，它代表和维护一个Socket连接，如缓冲Packet，统计连接状态等。提供一个ProcessPackets(MsgHanders* handers)接口处理该Channel上所有待处理数据。 EventPoller: 用于注册和回调网络事件，具体的网络事件由其子类实现processPendingEvents产生，目前EventPoller有两个子类: EpollPoller和SelectorPoller，分别针对于Linux和Windows。通过bool registerForRead(int fd, InputNotificationHandler * handler);注册套接字的可读事件，回调类需实现InputNotificationHandler接口。 EventDispatcher: 核心类，管理和分发所有事件，包括网络事件，定时器事件，任务队列，统计信息等等。它包含 EventPoller Tasks Timers64 三个组件，在每次处理时，依次在这三个组件中取出事件或任务进行处理。Epoll的最小等待时间是到下一个timer的触发时间，最大等待时间目前是0.1秒。这样可以保证Timer能被精确的触发。 ListenerReceiver/PacketReceiver: 继承自InputNotificationHandler，分别用于处理监听套接字和客户端套接字的可读事件，通过bool registerReadFileDescriptor(int fd, InputNotificationHandler * handler); 注册可读事件。 NetworkInterface: 维护管理监听套接字，创建监听套接字对应的ListenerReceiver，并且通过一个EndPoint -&gt; Channel的Map管理所有已连接套接字，提供一个processChannels(MsgHandlers* handers)接口处理所有Channel上的待处理数据。这一点上，有点像NGServer:ServiceManager。 网络部分的核心类结构图 各个组件之间的关系图]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
        <tag>KBEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你从头写游戏服务器框架三]]></title>
    <url>%2F2019%2F06%2F04%2F%E6%95%99%E4%BD%A0%E4%BB%8E%E5%A4%B4%E5%86%99%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A1%86%E6%9E%B6%E4%B8%89%2F</url>
    <content type="text"><![CDATA[原文出处 协程使用异步非阻塞编程，确实能获得很好的性能。但是在代码上，确非常不直观。因为任何一个可能阻塞的操作，都必须要要通过“回调”函数来链接。比如一个玩家登录，你需要先读数据库，然后读一个远程缓冲服务器（如 redis），然后返回登录结果：用户名、等级……在这个过程里，有两个可能阻塞的操作，你就必须把这个登录的程序，分成三个函数来编写：一个是收到客户端数据包的回调，第二个是读取数据库后的回调，第三个是读取缓冲服务器后的回调。 这种情况下，代码被放在三个函数里，对于阅读代码的人来说，是一种负担。因为我们阅读代码，比如通过日志、coredump 去查问题，往往会直接切入到某一个函数里。这个被切入阅读的函数，很可能就是一个回调函数，对于这个函数为什么会被调用，属于什么流程，单从这个函数的代码是很难理解的。 另外一个负担，是关于开发过程的。我们知道回调函数的代码，是需要“上下文”的，也就是发起回调时的数据状态的。为了让回调函数能获得发起函数的一个变量内容，我们就必须把这个变量内容放到某个“上下文”的变量中，然后传给回调函数。由于业务逻辑的变化，这种需要传递的上下文变量会不停的变化，反复的编写“放入”“取出”上下文的代码，也是一种重复的编码劳动。而且上下文本身的设置可能也不够安全，因为你无法预计，哪个回调函数会怎么样的修改这个上下文对象，这也是很多难以调试的 BUG 的来源。 为了解决这个问题，出现了所谓的协程技术。我们可以认为，协程技术提供给我们一种特殊的 return 语句：yield。这个语句会类似 return 一样从函数中返回，但你可以用另外一个特殊的语句 resume(id) 来从新从 yield 语句下方开始运行代码。更重要的是，在 resume 之后，之前整个函数中的所有临时变量，都是可以继续访问的。 当然，做 resume(id) 的时候，肯定是在进程的所谓“主循环”中，而这个 id 参数，则代表了被中断了的函数。这种可以被中断的函数调用过程，就叫协程。而这个 id ，则是代表了协程的一个数字。异步调用的上下文变量，就被自动的以这个协程函数的“栈”所取代，也就是说，协程函数中的所有局部变量，都自动的成为了上下文的内容。这样就再也不用反复的编写“放入”“取出”上下文内容的代码了。 我使用了 https://github.com/Tencent/Pebble/tree/master/src/common 项目下的 coroutine.cpp/.h 作为协程的实现者。 游戏开发中，协程确实能大大的提高开发效率。因此我认为协程也应该是 Game Server 所应该具备的能力。特别是在处理业务逻辑的 Handler 的 Process() 函数，本身就应该是一个协程函数。所以我设计了一个 CoroutineProcessor 的类，为普通的 Processor 添加上协程的能力。——基于装饰器模式。这样任何的 Processor::Process() 函数，就自然的在一个协程之中。 因为有了协程的支持，那些可能产生阻塞而要求编写回调的功能，就可以统一的变成以协程使用的 API 了： DataStore -&gt; CoroutineDataStore Cache -&gt; CoroutineCache Client -&gt; CoroutineClient 使用协程的 API，就完全不需要各种 Callback 类型的参数了，完全提供一个返回结果用的输出参数即可。 12345678910111213141516171819202122232425262728293031323334353637/*** @brief DataStore 的具备协程能力的装饰器类型。* @attention 除了定义变量语句和 Update() 以外，其他的操作都需要在协程中调用。*/class CoroutineDataStore : public Updateable&#123;public: CoroutineDataStore(DataStore* data_store, CoroutineSchedule* schedule); virtual ~CoroutineDataStore(); int Init(Config* cfg, std::string* err_msg); /** * 读取一个数据对象，通过 key ，把数据放入到输出参数 value。 * 此函数会在调用过程中使用协程的 yield 出去。 */ int Get(const std::string&amp;key, Serializable* value); /** * 写入一个数据对象，写入 key ，value * 写入结果从返回值获得，返回 0 表示成功，其他值表示失败。 * 此函数会在调用过程中使用协程的 yield 出去。 */ int Put(const std::string&amp;key, const Serializable&amp; value); /** * 删除一个数据对象，通过 key * 写入结果从返回值获得，返回 0 表示成功，其他值表示失败。 * 此函数会在调用过程中使用协程的 yield 出去 */ int Remove(const std::string&amp; key); int Update();private: DataStore* data_store_; CoroutineSchedule* schedule_;&#125;; 服务器对象管理组件模型一般来说服务器上，主要是运行各种各样处理请求的代码为主（通常叫 Handler）。然而，我们也会有一些需要持续运行的逻辑代码，比如处理匹配玩家战斗的逻辑，检查玩家是否超时发呆的逻辑，循环处理支付订单等等。这些代码的很多功能，同时还需要被各种 Handler 所调用。所以我们必须要有一种能让所有的这些自定义代码，以一种标准的方式在进程中互相引用，以及管理生命周期的方法。 借鉴于 Unity, 我觉得使用所谓的组件模型是很好的。它的特点包括： 组件之间通过 Application::GetComponet(name) 的方式互相调用。以一个字符串作为索引，就可以方便的获得对于的对象。组件自己通过 Application::Register(com_obj) 注册到系统中去，注册的名字自己实现 string GetName() 的接口去提供。 每个组件有预定的几个回调函数，提供进程生命周期的调用机会。包括： 初始化：Init() 主循环更新：Update() 关闭：Close() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*** 代表一个应用程序组件，所有的应用程序组件应该继承此对象*/class Component : public Updateable &#123;public: Component(); virtual ~Component(); /** * 返回此组件的名字 * @return 名字 */ virtual std::string GetName() = 0; /** * 初始化过程会调用此方法 * @param app 进程对象 * @param cfg 配置 * @return 返回 0 表示成功，其他表示失败 */ virtual int Init(Application* app, Config* cfg); /** * 更新过程会调用此方法 * @return 返回更新处理的负载，0 表示没有负载，负数表示出错或需要停止对此组件进行更新 */ virtual int Update(); /** * 应用停止时会调用此方法 * @return 返回 0 表示可以退出，返回 &gt;0 表示需要等待返回值的秒数，返回 &lt; 0 表示出错退出 */ virtual int Stop() &#123; return 0; &#125;; /** * 设置组件被加入的应用程序，用于让组件继承者能简单的获取 Application 对象 * @note 如果一个组件被加入多个不同的 Application，必须使用 @see Init() 方法来具体保存 Application 对象， * 因为此处修改的成员对象 app_ 将是最后一次被添加进的 Application 对象。 * @param app 要设置的 Application 对象。 */ void set_app(Application* app)&#123; app_ = app; &#125;protected: Application* app_;&#125;; Server 对象由于一个游戏服务器，所集成的功能实在是太多了，比如配置不同的协议、不同的处理器、提供数据库功能等等。要让这样一个服务器对象启动起来，需要大量的“组装代码”。为了节省这种代码，我设计了一个 LocalServer 的类型，作为一个 Server 模板，简化网络层的组装。使用者可以继承这个类，用来实现各种不同的 Server。 12345678910111213141516171819202122class LocalServerApp : public Application &#123;public: LocalServerApp(); virtual ~LocalServerApp(); virtual int Init(Config* cfg = NULL); virtual int Exit(); void set_transport(Transport* transport); void set_protocol(Protocol* protocol); void set_processor(Processor* processor);private: Transport* transport_; Protocol* protocol_; Processor* processor_; Server* server_;&#125;; 这个简单的类，可以通过 setter 方法来自定义网络层的组件，否则就是最常用的 TCP， TLV， Echo 这种服务器。而且这个类还是继承于 Application 的，这样可以让数据库或者其他的组件，也很方便的利用组件系统安装到服务器上。 集群功能需求分析游戏常常是一个带状态的服务。所以集群功能非常困难。 有一些框架，试图把状态从逻辑进程中搬迁出来，放在缓冲服务器中，但是往往满足不了性能需求。另外一些框架，则把集群定义成一个固定的层次架构，通过复杂的消息转发规则，来试图“把请求发到装载状态的进程上”，但这导致了运维部署的巨大复杂性。 为了解决这些问题，我觉得有几个设计决策是必须要订立的： 使用 SOA 的模式：集群中心的地址作为集群的地址，通过服务名来分割逻辑 提供给用户自定义路由的接口：由于集群中的进程都带有状态，要把请求发给哪个进程，并不能完全自动选择，所以必须要用户提供代码来选择 作为 SOA 模式下的集群，必须定义每个服务的“合同”格式。由于一个游戏服务器，可能存在各种不同的通信协议和编码协议，所以这个合同必须要能包含所有这些内容。在传统的 RPC 设计中，比如 WebService ，就采用了 WSDL 的格式，但是现在这种风格更多的被 RESTful 所取代。因此我决定使用类似 URL 类型的字符串来表述合同： 1tcp://1.1.1.1:8888/tlv 这样的合同描述，可以包含通信协议，IP地址和端口，编码协议三个部分，如果需要，还可以在 PATH 部分继续添加，如增加 QueryString 等。 集群中心根据之前的设计，集群中心地址，即事集群的地址。而集群中心，为了避免单点故障，自己也必须是一个集群。能符合这个要求的可用开源软件，非 ZooKeeper 莫属。 所以我直接把集群中心的功能，使用 ZooKeeper 来实现。虽然 ZooKeeper 的 API 设计也足够优秀了，但是作为异步非阻塞的框架，还是必须要做一层封装和抽象。在编译 C 的 ZK 客户端 API 时，也碰到了一个讨厌的问题，就是这个 API 使用了一个旧版本的测试框架库 cppunit-devel ，在新版本的 Linux 发行版 CentOS 和 Ubuntu 中，直接从源安装的版本都和这个版本不兼容，没办法只好去官网上下载 cppunit-1.13 的源代码来编译安装。 为了方便使用 ZooKeeper ，我先实现了一个 ZooKeeperMap 的类，属于 cache 模块的 DataMap 的子类，用以完成标准的 Key-Value 存取。实际上在这里是为了完成链接 ZooKeeper 和初始化的功能。 如前文的合同所设计，当获得一个“合同”字符串的时候，是需要“构造”出一个使用对应合同的客户端对象的。不同的协议对应着不同类型的对象，在这里就需要一种类似“反射”生成对象的技术。对于没有这种反射能力的 C++ 来说，我添加了一个“注册”模板方法，这个模板方法会把注册的类的构造工厂方法，记录到一个 map 里面。当然，这对于注册的类的构造器是有要求，需要有无参数构造器，或者是带“字符串，数字”构造器。当然，如果写错了也不要紧，只是不能编译成功而已。这也是静态绑定的好处之一了。 整个集群中心，最核心的接口其实就三个： 注册一个合同，包括提供的“服务名”和“合同”，这个合同内容必须是能让客户端访问到自己的通信地址。 查询合同，通过输入“服务名”，获得所有提供这个服务的合同列表 通过合同构建客户端，得到的客户端对象就是可以发送请求给对应合同的服务提供进程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123/*** @brief 集群中心客户端* 每个 DenOS 进程启动时，都会向 ZooKeeper 注册自己的服务。* 此类型的对象，就是作为每个进程中，代表集群中心的存在。* ZooKeeper 默认根据 2 个 tick （心跳），大概为 3 秒，是否收到，来决定客户端是否死掉。* 在 DenOS 中，可以使用配置项目 ZK_RECV_TIMEOUT 参数（单位为毫秒）来决定这个超时时间。*/class Center : public ZooKeeperMap &#123; friend void ContractsWatcher(zhandle_t *zh, int type, int state, const char *path, void *watcherCtx);public: int last_error_code_; // 用来测试最后一个操作是否成功的变量 std::map&lt;std::string, std::string&gt; service_process_; // key: 服务名字，value 服务节点名字 /** * @brief 构造一个集群中心客户端 * @param urls ZooKeeper 的连接参数，形如："127.0.0.1:2181,10.1.2.3:2182,192.168.3.23:2183" */ Center(const std::string&amp; urls = "127.0.0.1:2181"); virtual ~Center(); /// 驱动整个异步流程 virtual int Update(); /** * 往集群中声明注册服务 * @param name 服务的名字 * @param contract 服务的通信方式 * @return 返回 0 表示已经发起注册流程，其他值表示失败 */ int RegisterService(const std::string&amp; name, const Contract&amp; contract); /** * @brief 注册一个类作为对应协议字符串名字 * 如果这个类是 Connector 的子类，必须要有一个形如 XXConnector(const string&amp; p1, int p2) 这样的构造器。 * 或者这个类是 Protocol 的子类，必须要有一个无参数构造器。 * @param reg_name 协议字符串名字，如 tcp/udp/kcp/tconnd 或者 tlv/line/tdr */ template&lt;typename T&gt; void RegProto(const std::string&amp; reg_name) &#123; if (reg_name.empty()) return; constructors_[reg_name] = new DefaultConstructor&lt;T&gt;(); &#125; template&lt;typename T&gt; void RegConn(const std::string&amp; reg_name) &#123; if (reg_name.empty()) return; constructors_[reg_name] = new StrIntConstructor&lt;T&gt;(); &#125; /** * @brief 查询一个服务去发起请求 * 注意这是一个异步的接口，有可能会返回 -1 表示服务合同还未拿到。需要重复的去获取。 * @param name 服务的名字 * @param callback 当获得对应的服务的客户端的回调 * @param client_cb 预期每个新的 Client 所注册的默认回调，用来接收连接、中断、收听通知。 * @param route_param 用来传给路由器的自定义路由相关数据 * @return 如果返回 0 表示成功，失败则会是其他数值 */ int QueryService(const std::string&amp; name, GetServiceClientCallback* callback, ClientCallback* client_cb = NULL, Router* router = NULL, void* route_param = NULL); /** * 根据合同缓存获得客户端对象 * @param cache 合同缓存对象 * @param client_cb 预期每个新的 Client 所注册的默认回调，用来接收连接、中断、收听通知。 * @param router 路由器对象 * @param route_param 路由参数 * @return 客户端对象指针，无需主动 delete，因为会缓存起来 */ Client* GetClientByContracts(ContractCache* cache, ClientCallback* client_cb, Router* router, void* route_param); /** * 获得存放集群的 ZK 基础路径 * @return ZNode 基础路径 */ inline const std::string&amp; cluster_prefix() const &#123; return CLUSTER_PREFIX; &#125; /** * 获得建立进程用的 ZK 标记 (Create flags) * @return zk 的 create flags */ inline int process_flags() const &#123; return process_flags_; &#125; /// 在 ZK 写入本进程对此服务的合约 void AddProcessContract(const std::string&amp; service_name, const std::string&amp; contract_data); /// 清理相关对象 void CloseClient(Client* client); void SetContractsCache(const std::string&amp; service_name, ContractCache* cache); /*---------------------------------- 继承自 ZooKeeper 为了实现功能用 -----------------------------------*/ /// 建立存储节点父目录时，增加一个监听器，监听这些节点增加和删除变化 virtual void CreatePrefixNode(); /// 初始化 zookeeper 客户端连接，会修改 ZKMAP_KEY_PREFIX 为集群专用路径 virtual int Init(Config* config = NULL); /** * 清理掉生成的客户端对象 * @param client 客户端对象 * @param content 相关的服务名字 */ void ClearClientMember(Client* client, const std::string&amp; content);&#125;; 服务器间通信在上面所说的集群中心功能中，最后一项“获得客户端”的方法，是需要用户输入一个 Router 类型的对象的。其原因就是，游戏服务器往往都是带状态，所以必须要让调用者有办法选择具体的服务提供者。比如游戏中的聊天功能，一般都支持“组队聊天”的功能，这个功能，需要把消息转发到不同的服务器进程上，因为队伍中的玩家可能登录在不同的服务器上。那么，如果玩家本身登录的规则，就是根据自己的 ID 做某种哈希去选择服务器进程的，那么，这个聊天功能，只要让 Router 对象也按同样的哈希方法去选择服务器进程，就能正确的发送消息了。当然了，根据某种类似“服务器进程ID”去选择服务器，也是一种路由方式，可以写入 Router 中去。 1234567891011121314151617181920/*** @brief 路由器基类*/class Router &#123;public: Router(); virtual ~Router(); /** * @brief 决定服务路由的接口 * 此接口默认实现是取 cache 中的第一个非空结果 * @param cache 需要选择的所有合同的缓存集合 * @param content 输出参数，具体选择的合同的内容 * @param route_param 用来提供给路由算法运行的额外参数 */ virtual void RouteToSevice(const ContractCache&amp; cache, std::string* content, void* route_param = NULL);&#125;; 路由器的写法非常简单，也附带了一个 route_param 用来帮助传递一些路由选择所需的数据。当然你也可以构造多个不同的 Router 子类对象，用对象成员属性来携带更复杂的路由参数。 当我们选择出了合同，就可以利用 Center 的功能去发起服务请求了。下面是单元测试的部分代码，展示了如何在服务器之间调用服务： 12345678910111213141516171819202122232425262728// 获取客户端DEBUG_LOG("========== Getting Client ==========");Center* center = obj_pro_reg.center();TestGetClientCallback callback;TestClientCallback cli_cb;center-&gt;QueryService(handler.service_name_, &amp;callback, &amp;cli_cb);for (int i = 0; i &lt; 100; i++) &#123; usleep(100); svr-&gt;Update();&#125;EXPECT_EQ(0, callback.err_code_);EXPECT_TRUE(cli_cb.is_connected_);ASSERT_TRUE(callback.client_ != NULL);// 访问服务器DEBUG_LOG("========== Requesting Service ==========");Client* client = callback.client_;Request req;string data("I love JMX!");req.service = handler.service_name_;req.SetData(data.c_str(), data.length());client-&gt;SendRequest(&amp;req, &amp;cli_cb);for (int i = 0; i &lt; 100; i++) &#123; usleep(100); svr-&gt;Update();&#125;EXPECT_EQ(data, cli_cb.resp_data_); 在一般的异步编程中，访问集群中的服务，需要两个回调（组赛）过程，一个是通过集群中心查询合同，一个是请求服务。这样显然会让代码分散在不同的函数中，阅读起来非常不方便。所以我又使用了协程功能，封装了集群和客户端的能力，让整个过程可以用同步代码的写法来完成。 展望写到这里，基本上关于一个游戏服务器框架的主体功能设计，都基本完成了。但是，一个游戏中还包含了很多不同的能力需要考虑。比如说排行榜、拍卖行、战斗记录日志等等，这个功能往往不能靠上文所述的 key-value 数据能力简单解决。而需要额外直接的对一些特殊的设施，比如 redis/MySQL 直接编程，这些部分，也只能放在框架之外处理了。也许以后，我会总结出更好的抽象层，能把带排序、模糊搜索、大容量记录的功能，一起放入框架的想法。另外，对于 cache 模块（缓冲），使用一致的 API 风格，去操作真正的分布式缓冲，还是一个未能很好解决的课题。虽然 Orcal Conherence 提供了很好的参考方案，但是限于时间和精力，也只能用简单的二级缓存来部分模拟其能力，这一方面也是值得去深入研究的部分。 总结一下，游戏服务器框架，其实基本能力也非常简单： 网络功能：提供请求响应、通知两种能力即可组合大部分功能 缓存功能：提供二级缓存的远程缓冲功能，也可以满足很多需求 持久化功能：以 key-value 方式的存储足以满足很多用户存档的需求 对于现代服务器系统，需要增加的能力还有： 集群功能：可以用 SOA 但自定义路由的方式，提供集群服务 协程功能：避免大量异步回调的代码阅读问题 组件功能：给框架一个结合不同体系代码的接口 全文完。]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你从头写游戏服务器框架二]]></title>
    <url>%2F2019%2F06%2F04%2F%E6%95%99%E4%BD%A0%E4%BB%8E%E5%A4%B4%E5%86%99%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A1%86%E6%9E%B6%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[原文出处 对象序列化 现代编程技术中，面向对象是一个最常见的思想。因此不管是 C++ Java C#，还是 Python JS，都有对象的概念。虽然说面向对象并不是软件开发的“银弹”，但也不失为一种解决复杂逻辑的优秀工具。回到游戏服务器端程序来说，自然我会希望能有一定面向对象方面的支持。所以，从游戏服务器端的整个处理过程来看，我认为，有以下几个地方，是可以用对象来抽象业务数据的： 数据传输：我们可以把通过网络传输的数据，看成是一个对象。这样我们可以简单的构造一个对象，然后直接通过网络收、发它。 数据缓存：一批在内存中的，可以用对象进行“抽象”。而 key-value 的模型也是最常见的数据容器，因此我们可以起码把 key-value 中的 value 作为对象处理。 数据持久化：长久以来，我们使用 SQL 的二维表结构来持久化数据。但是 ORM （对象关系映射）的库一直非常流行，就是想在二维表和对象之间搭起桥梁。现在 NoSQL 的使用越来越常见，其实也是一种 key-value 模型。所以也是可以视为一个存放“对象”的工具。 对象序列化的标准模式也很常见，因此我定义成： 1234567891011121314151617181920class Serializable &#123;public: /** * 序列化到一个数组中 * @param buffer 目地缓冲区数组 * @param buffer_length 缓冲区长度 * @return 返回写入了 buffer 的数据长度。如果返回 -1 表示出错，比如 buffer_length 不够。 */ virtual ssize_t SerializeTo(char* buffer, int buffer_length) const = 0; /** * @brief 从一个 buffer 中读取 length 个字节，反序列化到本对象。 *@return 返回 0 表示成功，其他值表示出错。 */ virtual int SerializeFrom(const char* buffer, int length) = 0; virtual ~Serializable()&#123;&#125;&#125;; 网络传输有了对象序列化的定义，就可以从网络传输处使用了。因此专门在 Processor 层设计一个收发对象的处理器 ObjectProcessor，它可以接纳一种 ObjectHandler 的对象注册。这个对象根据注册的 Service 名字，负责把收发的接口从 Request, Response 里面的字节数组转换成对象，然后处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293// ObjectProcessor 定义class ObjectProcessor : public ProcessorHelper &#123;public: ObjectProcessor(); virtual ~ObjectProcessor(); // 继承自 Processor 处理器函数 virtual int Init(Server* server, Config* config = NULL); // 继承自 Processor 的处理函数 virtual int Process(const Request&amp; request, const Peer&amp; peer); virtual int Process(const Request&amp; request, const Peer&amp; peer, Server* server); ///@brief 设置默认处理器，所有没有注册具体服务名字的消息都会用这个消息处理 inline void set_default_handler(ObjectHandler* default_handler) &#123; default_handler_ = default_handler; &#125; /** * @brief 针对 service_name，注册对应处理的 handler ，注意 handler 本身是带对象类型信息的。 * @param service_name 服务名字，通过 Request.service 传输 * @param handler 请求的处理对象 */ void Register(const std::string&amp; service_name, ObjectHandler* handler); /** * @brief 使用 handler 自己的 GetName() 返回值，注册服务。 * 如果 handler-&gt;GetName() 返回 "" 字符串，则会替换默认处理器对象 * @param handler 服务处理对象。 */ void Register(ObjectHandler* handler); ///@brief 关闭此服务 virtual int Close();private: std::map&lt;std::string, ObjectHandler*&gt; handler_table_; Config* config_; ObjectHandler* default_handler_; int ProcessBy(const Request&amp; request, const Peer&amp; peer, ObjectHandler* handler, Server* server = NULL); int DefaultProcess(const Request&amp; request, const Peer&amp; peer, Server* server = NULL); bool InitHandler(ObjectHandler* handler);&#125;;// ObjectHandler 定义class ObjectHandler : public Serializable, public Updateable &#123;public: ObjectHandler(); virtual ~ObjectHandler() ; virtual void ProcessRequest(const Peer&amp; peer); virtual void ProcessRequest(const Peer&amp; peer, Server* server); /** * DenOS 用此方法确定进行服务注册，你应该覆盖此方法。 * 默认名字为空，会注册为“默认服务”，就是所有找不到对应名字服务的请求，都会转发给此对象处理。 * @return 注册此服务的名字。在 Request.service 字段中传输。 */ virtual std::string GetName() ; int Reply(const char* buffer, int length, const Peer&amp; peer, const std::string&amp; service_name = "", Server* server = NULL) ; int Inform(char* buffer, int length, const std::string&amp; session_id, const std::string&amp; service_name, Server* server = NULL); int Inform(char* buffer, int length, const Peer&amp; peer, const std::string&amp; service_name = "", Server* server = NULL); virtual int Init(Server* server, Config* config); /** * 如果需要在主循环中进行操作，可以实现此方法。 * 返回值小于 0 的话，此任务会被移除循环 */ virtual int Update() &#123; return 0; &#125;protected: Server* server_; std::map&lt;int, MessageHeader&gt; header_map_; Response response_; Notice notice_;&#125;; 由于我们对于可序列化的对象，要求一定要实现 Serializable 这个接口，所以所有需要收发的数据，都要定义一个类来实现这个接口。但是，这种强迫用户一定要实现某个接口的方式，可能会不够友好，因为针对业务逻辑设计的类，加上一个这种接口，会比较繁琐。为了解决这种问题，我利用 C++ 的模板功能，对于那些不想去实现 Serializable 的类型，使用一个额外的 Pack()/Upack() 模板方法，来插入具体的序列化和反序列化方法（定义 ObjectHandlerCast 模板）。这样除了可以减少实现类型的代码，还可以让接受消息处理的接口方法 ProcessObject() 直接获得对应的类型指针，而不是通过 Serializable 来强行转换。在这里，其实也有另外一个思路，就是把 Serializable 设计成一个模板类，也是可以减少强制类型转换。但是我考虑到，序列化和反序列化，以及处理业务对象，都是使用同样一个（或两个，一个输入一个输出）模板类型参数，不如直接统一到一个类型里面好了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192// ObjectHandlerCast 模板定义/*** 用户继承这个模板的实例化类型，可以节省关于对象序列化的编写代码。* 直接编写开发业务逻辑的函数。*/template&lt;typename REQ, typename RES = REQ&gt;class ObjectHandlerCast : public ObjectHandler &#123;public: ObjectHandlerCast() : req_obj_(NULL), res_obj_(NULL), buffer_(NULL) &#123; buffer_ = new char[Message::MAX_MAESSAGE_LENGTH]; bzero(buffer_, Message::MAX_MAESSAGE_LENGTH); &#125; virtual ~ObjectHandlerCast() &#123; delete[] buffer_; &#125; /** * 对于不想使用 obj_ 成员来实现 Serializable 接口的，可以实现此接口 */ virtual int Pack(char* buffer, int length, const RES&amp; object) const &#123; return -1; &#125; virtual int Unpack(const char* buffer, int length, REQ* object) &#123; return -1; &#125; int ReplyObject(const RES&amp; object, const Peer&amp; peer, const std::string&amp; service_name = "") &#123; res_obj_ = &amp;object; int len = SerializeTo(buffer_, Message::MAX_MAESSAGE_LENGTH); if (len &lt; 0) return -1; return Reply(buffer_, len, peer, service_name); &#125; int InformObject(const std::string&amp; session_id, const RES&amp; object, const std::string&amp; service_name) &#123; res_obj_ = &amp;object; int len = SerializeTo(buffer_, Message::MAX_MAESSAGE_LENGTH); if (len &lt; 0) return -1; return Inform(buffer_, len, session_id, service_name); &#125; virtual void ProcessRequest(const Peer&amp; peer, Server* server) &#123; REQ* obj = req_obj_; ProcessObject(*obj, peer, server); delete obj; req_obj_ = NULL; &#125; virtual void ProcessObject(const REQ&amp; object, const Peer&amp; peer) &#123; ERROR_LOG("This object have no process handler."); &#125; virtual void ProcessObject(const REQ&amp; object, const Peer&amp; peer, Server* server) &#123; ProcessObject(object, peer); &#125;protected: REQ* req_obj_; const RES* res_obj_;private: char* buffer_; virtual ssize_t SerializeTo(char* buffer, int buffer_length) const &#123; ssize_t ret = 0; ret = Pack(buffer, buffer_length, *res_obj_); return ret; &#125; virtual int SerializeFrom(const char* buffer, int length) &#123; req_obj_ = new REQ(); // 新建一个对象，为了协程中不被别的协程篡改 int ret = Unpack(buffer, length, req_obj_); if (ret) &#123; delete req_obj_; req_obj_ = NULL; &#125; return ret; &#125;&#125;; 任何类型的对象，如果想要在这个框架中以网络收发，只要为他写一个模板，完成 Pack() 和 UnPack() 这两个方法，就完成了。看起来确实方便。 （如果想节省注册的时候编写其“类名”，还需要完成一个简单的 GetName() 方法） 当我完成上面的设计，不禁赞叹 C++ 对于模板支持的好处。由于模板可以在编译时绑定，只要是具备“预设”的方法的任何类型，都可以自动生成一个符合既有继承结构的类。这对于框架设计来说，是一个巨大的便利。而且编译时绑定也把可能出现的类型错误，暴露在编译期。————对比那些可以通过反射实现同样功能的技术，其实是更容易修正的问题。 缓冲和持久化数据传输的对象序列化问题解决后，下来就是缓存和持久化。由于缓存和持久化，我的设计都是基于 Map 接口的，也就是一种 Key-Value 的方式，所以就没有设计模板，而是希望用户自己去实现 Serializable 接口。但是我也实现了最常用的几种可序列化对象的实现代码： 固定长度的类型，比如 int 。序列化其实就是一个 memcpy() 而已。 std::string 字符串。这个需要使用 c_str() 变成一个字节数组。 JSON 格式串。使用了某个开源的 json 解析器。推荐 GITHUB 上的 Tencent/RapidJson。 数据缓冲 数据缓冲这个需求，虽然在互联网领域非常常见，但是游戏的缓冲和其他一些领域的缓冲，实际需求是有非常大的差别。这里的差别主要有： 游戏的缓冲要求延迟极低，而且需要对服务器性能占用极少，因为游戏运行过程中，会有非常非常频繁的缓冲读写操作。举个例子来说，一个“群体伤害”的技能，可能会涉及对几十上百个数据对象的修改。而且这个操作可能会以每秒几百上千次的频率请求服务器。如果我们以传统的 memcache 方式来建立缓冲，这么高频率的网络 IO 往往不能满足延迟的要求，而且非常容易导致服务器过载。 游戏的缓冲数据之间的关联性非常强。和一张张互不关联的订单，或者一条条浏览结果不一样。游戏缓冲中往往存放着一个完整的虚拟世界的描述。比如一个地区中有几个房间，每个房间里面有不通的角色，角色身上又有各种状态和道具。而角色会在不同的房间里切换，道具也经常在不同角色身上转移。这种复杂的关系会导致一个游戏操作，带来的是多个数据的同时修改。如果我们把数据分割放在多个不同的进程上，这种关联性的修改可能会让进程间通信发生严重的过载。 游戏的缓冲数据的安全性具有一个明显的特点：更新时间越短，变换频率越大的数据，安全性要求越低。这对于简化数据缓冲安全性涉及，非常具有价值。我们不需要过于追求缓冲的“一致性”和“时效”，对于一些异常情况下的“脏”数据丢失，游戏领域的忍耐程度往往比较高。只要我们能保证最终一致性，甚至丢失一定程度以内的数据，都是可以接受的。这给了我们不挑战 CAP 定律的情况下，设计分布式缓冲系统的机会。 基本模型基于上面的分析，我首先希望是建立一个足够简单的缓冲使用模型，那就是 Map 模型。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class DataMap : public Updateable &#123;public: DataMap(); virtual ~DataMap(); /** * @brief 对于可能阻塞的异步操作，需要调用这个接口来驱动回调。 */ virtual int Update()&#123; return 0; &#125; /** * @brief 获取 key 对应的数据。 * @param key 数据的 Key * @param value_buf 是输出缓冲区指针 * @param value_buf_len 是缓冲区最大长度 * @return 返回 -1 表示找不到这个 key，返回 -2 表示 value_buf_len 太小，不足以读出数据。其他负数表示错误，返回 &gt;= 0 的值表示 value 的长度 */ virtual int Get(const std::string&amp; key, char* value_buf, int value_buf_len) = 0; virtual int Get(const std::string&amp; key, Serializable* value); /** * @brief 异步 Get 的接口 * @param key 获取数据的 Key * @param callback 获取数据的回调对象，如果 key 不存在， FetchData() 参数 value_buf 会为 NULL * @return 返回 0 表示发起查询成功，其他值表示错误。 */ virtual int Get(const std::string&amp; key, DataMapCallback* callback) = 0; /** * @brief 覆盖、写入key对应的缓冲数据。 * @return 成功返回0。 返回 -1 表示value数据太大，其他负数表示其他错误 */ virtual int Put(const std::string&amp; key, const char* value_buf, int value_buf_len) = 0; virtual int Put(const std::string&amp; key, const Serializable&amp; value); /** * 写入数据的异步接口，使用 callback 来通知写入结果 * @param key 数据 key * @param value 数据 Value * @param callback 写入结果会调用此对象的 PutResult() 方法 * @return 返回 0 表示准备操作成功 */ virtual int Put(const std::string&amp; key, const Serializable&amp; value, DataMapCallback* callback); virtual int Put(const std::string&amp; key, const char* value_buf, int value_buf_len, DataMapCallback* callback); /** * 删除 key 对应的数据 * @return 返回 0 表示成功删除，返回 -1 表示这个 key 本身就不存在，其他负数返回值表示其他错误。 */ virtual int Remove(const std::string&amp; key) = 0; virtual int Remove(const std::string&amp;key, DataMapCallback* callback); /** * 是否有 Key 对应的数据 *@return 返回 key 值表示找到，0 表示找不到 */ virtual int ContainsKey(const std::string&amp; key) = 0; /** * 异步 ContainsKey 接口，如果 key 不存在， FetchData() 参数 value_buf 会为 NULL。 * 如果 key 存在，value_buf 则不为NULL，但也不保证指向任何可用数据。可能是目标数值， * 也可能内部的某个空缓冲区。如果是在本地的数据，就会是目标数据，如果是远程的数据， * 为了减少性能就不会传入具体的 value 数值。 */ virtual int ContainsKey(const std::string&amp; key, DataMapCallback* callback) = 0; /** * 遍历整个缓存。 * @param callback 每条记录都会调用 callback 对象的 FetchData() 方法 * @return 返回 0 表示成功，其他表示错误。 */ virtual int GetAll(DataMapCallback* callback) = 0; /** * 获取整个缓存的数据 * @param result 结果会放在这个 map 里面，记得每条记录中的 Bytes 的 buffer_ptr 需要 delete[] * @return 返回 0 表示成功，其他表示错误 */ virtual int GetAll(std::map&lt;std::string, Bytes&gt;* result);private: char* tmp_buffer_;&#125;; 这个接口其实只是一个 std::map 的简单模仿，把 key 固定成 string ，而把 value 固定成一个 buffer 或者是一个可序列化对象。另外为了实现分布式的缓冲，所有的接口都增加了回调接口。 可以用来充当数据缓存的业界方案其实非常多，他们包括： 堆内存，这个是最简单的缓存容器 Redis Memcached ZooKeeper 这个自带了和进程关联的数据管理 由于我希望这个框架，可以让程序自由的选用不同的缓冲存储“设备”，比如在测试的时候，可以不按照任何其他软件，直接用自己的内存做缓冲，而在运营或者其他情况下，可以使用 Redis 或其他的设备。所以我们可以编写代码来实现上面的 DataMap 接口，以实现不同的缓冲存储方案。当然必须要把最简单的，使用堆内存的实现完成： RamMap 分布式设计如果作为一个仅仅在“本地”服务器使用的缓冲，上面的 DataMap 已经足够了，但是我希望缓存是可以分布式的。不过，并不是任何的数据，都需要分布式存储，因为这会带来更多延迟和服务器负载。因此我希望设计一个接口，可以在使用时指定是否使用分布式存储，并且指定分布式存储的模式。 根据经验，在游戏领域中，分布式存储一般有以下几种模式： 本地模式 如果是分区分服的游戏，数据缓存全部放在一个地方即可。或者我们可以用一个 Redis 作为缓存存储点，然后多个游戏服务进程共同访问它。总之对于数据全部都缓存在一个地方的，都可以叫做本地模式。这也是最简单的缓冲模式。 按数据类型分布 这种模式和“本地模式”的差别，仅仅在于数据内容不同，就放在不同的地方。比如我们可以所有的场景数据放在一个 Redis 里面，然后把角色数据放在另外一个 Redis 里面。这种分布节点的选择是固定，仅仅根据数据类型来决定。这是为了减缓某一个缓冲节点的压力而设计。或者你对不同数据有缓冲隔离的需求：比如我不希望对用户的缓冲请求负载，影响对支付服务的缓冲请求负载。 按数据的 Key 分布 这是最复杂也最有价值的一种分布式缓存。因为缓冲模式是按照 Key-Vaule 的方式来存放的，所以我们可以把不同的 Key 的数据分布到不同节点上。如果刚好对应的 Key 数据，是分布在“本地”的，那么我们将获得本地操作的性能！ 这种缓冲 按复制分布 就是多个节点间的数据一摸一样，在修改数据的时候，会广播到所有节点，这种是典型的读多写少性能好的模型。 在游戏开发中，我们往往习惯于把进程，按游戏所需要的数据来分布。比如我们会按照用户 ID ，把用户的状态数据，分布到不同的机器上，在登录的时候，就按照用户 ID 去引导客户端，直接连接到对应的服务器进程处。或者我们会把每个战斗副本或者游戏房间，放在不同的服务器上，所有的战斗操作请求，都会转发到对应的存放其副本、房间数据的服务器上。在这种开发中，我们会需要把大量的数据包路由、转发代码耦合到业务代码中。 如果我们按照上面的第 3 种模型，就可以把按“用户ID”或者“房间ID”分布的事情，交给底层缓冲模块去处理。当然如果仅仅这样做，也许会有大量的跨进程通信，导致性能下降。但是我们还可以增加一个“本地二级缓存”的设计，来提高性能。具体的流程大概为： 取 key 在本地二级缓存（一般是 RamMap）中读写数据。如果没有则从远端读取数据建立缓存，并在远端增加一条“二级缓存记录”。此记录包含了二级所在的服务器地址。 按 key 计算写入远端数据。根据“二级缓存记录”广播“清理二级缓存”的消息。此广播会忽略掉刚写入远端数据的那个服务节点。（此行为是异步的） 只要不是频繁的在不同的节点上写入同一个 Key 的记录，那么二级缓存的生存周期会足够长，从而提供足够好的性能。当然这种模式在同时多个写入记录时，有可能出现脏数据丢失或者覆盖，但可以再添加上乐观锁设计来防止。不过对于一般游戏业务，我们在 Key 的设计上，就应该尽量避免这种情况：如果涉及非常重要的，多个用户都可能修改的数据，应该避免使用“二级缓存”功能的模型 3 缓存，而是尽量利用服务器间通信，把请求集中转发到数据所在节点，以模式 1 （本地模式）使用缓冲。 以下为分布式设计的缓冲接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/*** 定义几种网络缓冲模型*/enum CacheType &#123; TypeLocal, ///&lt; 本地 TypeName, ///&lt; 按 Cache 名字分布 TypeKey, ///&lt; 先按 Cache 名字分布，再按数据的 key 分布 TypeCopy ///&lt; 复制型分布&#125;;/*** @brief 定义了分布式缓存对象的基本结构*/class Cache : public DataMap &#123;public: /** * @brief 连接集群。 * @return 返回是否连接成功。 */ static bool EnsureCluster(Config* config = NULL); /** * 获得一个 Cache 对象。 * @attention 如果第一次调用此函数时，输入的 type, local_data_map 会成为这个 Cache 的固定属性，以后只要是 name 对的上，都会是同一个 Cache 对象。 * @param name 为名字 * @param type 此缓存希望是哪种类型 * @param local_data_map 为本地存放的数据容器。 * @return 如果是 NULL 表示已经达到 Cache 数量的上限。 */ static Cache* GetCache(const std::string&amp; name, CacheType type, DataMap* local_data_map); /** * 获得一个 Cache 对象。 * @param name 为名字 * @param local_data_map 为本地存放的数据容器。 * @note 如果第一次调用此函数时，输入的 type, class M 会成为这个 Cache 的固定属性，以后只要是 name 对的上，都会是同一个 Cache 对象。 * @return 如果是 NULL 表示已经达到 Cache 数量的上限。 */ template&lt;class M&gt; static Cache* GetCache(const std::string&amp; name, CacheType type, int* data_map_arg1 = NULL) &#123; DataMap* local_data_map = NULL; std::map&lt;std::string, DataMap*&gt;::iterator it = cache_store_.find(name); if (it != cache_store_.end()) &#123; local_data_map = it-&gt;second; &#125; else &#123; if (data_map_arg1 != NULL) &#123; local_data_map = new M(*data_map_arg1); &#125; else &#123; local_data_map = new M(); &#125; cache_store_[name] = local_data_map; &#125; return GetCache(name, type, local_data_map); &#125; /** * 删除掉本进程内存放的 Cache */ static void RemoveCache(const std::string&amp; name); explicit Cache(const std::string&amp; name); virtual ~Cache(); virtual std::string GetName() const;protected: static std::map&lt;std::string, Cache*&gt; cache_map_; std::string name_;private: static int MAX_CACHE_NUM; static std::map&lt;std::string, DataMap*&gt; cache_store_;&#125;; 持久化 长久以来，互联网的应用会使用类似 MySQL 这一类 SQL 数据库来存储数据。当然也有很多游戏是使用 SQL 数据库的，后来业界也出现“数据连接层”（DAL）的设计，其目的就是当需要更换不同的数据库时，可以不需要修改大量的代码。但是这种设计，依然是基于 SQL 这种抽象。然而不久之后，互联网业务都转向 NoSQL 的存储模型。实际上，游戏中对于玩家存档的数据，是完全可以不需要 SQL 这种关系型数据库的了。早期的游戏都是把玩家存档存放到文件里，就连游戏机如 PlayStation ，都是用存储卡就可以了。 一般来说，游戏中需要存储的数据会有两类： 玩家的存档数据 游戏中的各种设定数据 对于第一种数据，用 Key-Value 的方式基本上能满足。而第二种数据的模型可能会有很多种类，所以不需要特别的去规定什么模型。因此我设计了一个 key-value 模型的持久化结构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142/*** @brief 由于持久化操作一般都是耗时等待的操作，所以需要回调接口来通知各种操作的结果。*/class DataStoreCallback &#123;public: static int MAX_HANG_UP_CALLBACK_NUM; // 最大回调挂起数 std::string key; Serializable* value; DataStoreCallback(); virtual ~DataStoreCallback(); /** * 当 Init() 初始化结束时会被调用。 * @param result 是初始化的结果。0 表示初始化成功。 * @param msg 是初始化可能存在的错误信息。可能是空字符串 ""。 */ virtual void OnInit(int result, const std::string&amp; msg); /** * 当调用 Get() 获得结果会被调用 * @param key * @param value * @param result 是 0 表示能获得对象，否则 value 中的数据可能是没有被修改的。 */ virtual void OnGot(const std::string&amp; key, Serializable* value, int result); /** * 当调用 Put() 获得结果会被调用。 * @param key * @param result 如果 result 是 0 表示写入成功，其他值表示失败。 */ virtual void OnPut(const std::string&amp;key, int result); /** * 当调用 Remove() 获得结果会被调用 * @param key * @param result 返回 0 表示删除成功，其他值表示失败，如这个 key 代表的对象并不存在。 */ virtual void OnRemove(const std::string&amp;key, int result); /** * 准备在发起用户设置的回调，如果使用者没有单独为一个回调事务设置单独的回调对象。 * 在 PrepareRegisterCallback() 中会生成一个临时对象，在此处会被串点参数并清理。 * @param privdata 由底层回调机制所携带的回调标识参数，如果为 NULL 则返回 NULL。 * @param init_cb 初始化时传入的共用回调对象 * @return 可以发起回调的对象，如果为 NULL 表示参数 privdata 是 NULL */ static DataStoreCallback* PrepareUseCallback(void* privdata, DataStoreCallback* init_cb); /** * 检查和登记回调对象，以防内存泄漏。 * @param callback 可以是NULL，会新建一个仅仅用于存放key数据的临时callback对象。 * @return 返回一个callback对象，如果是 NULL 表示有太多的回调过程未被释放。 */ static DataStoreCallback* PrepareRegisterCallback( DataStoreCallback* callback);protected: static int kHandupCallbacks; /// 有多少个回调指针被挂起，达到上限后会停止工作&#125;;/***@brief 用来定义可以持久化对象的数据存储工具接口*/class DataStore : public Component &#123;public: DataStore(DataStoreCallback* callback = NULL) : callback_(callback) &#123; // Do nothing &#125; virtual ~DataStore() &#123; // Do nothing &#125; /** * 初始化数据存取设备的方法，譬如去连接数据库、打开文件之类。 * @param config 配置对象 * @param callback 参数 callback 为基本回调对象，初始化的结果会回调其 OnInit() 函数通知用户。 * @return 基本的配置是否正确，返回 0 表示正常。 */ virtual int Init(Config* config, DataStoreCallback* callback); virtual int Init(Application* app, Config* cfg)&#123; app_ = app; return Init(cfg, callback_); &#125; virtual std::string GetName() &#123; return "den::DataStore"; &#125; virtual int Stop() &#123; Close(); return 0; &#125; /** * 驱动存储接口程序运行，触发回调函数。 * @return 返回 0 表示此次没有进行任何操作，通知上层本次调用后可以 sleep 一下。 */ virtual int Update() = 0; /** * 关闭程序，关闭动作虽然是异步的，但不再返回结果，直接关闭就好。 */ virtual void Close() = 0; /** * 读取一个数据对象，通过 key ，把数据放入到 value，结果会回调通知 callback。 * 发起调用前，必须把 callback 的 value 字段设置为输出参数。 * @param key * @param callback 如果NULL，则会回调从 Init() 中传入的 callback 对象。 * @return 0 表示发起请求成功，其他值为失败 */ virtual int Get(const std::string&amp; key, DataStoreCallback* callback = NULL) = 0; /** * 写入一个数据对象，写入 key ，value，写入结果会回调通知 callback。 * @param key * @param value * @param callback 如果是 NULL，则会回调从 Init() 中传入的 callback 对象。 * @return 表示发起请求成功，其他值为失败 */ virtual int Put(const std::string&amp;key, const Serializable&amp; value, DataStoreCallback* callback = NULL) = 0; /** * 删除一个数据对象，通过 key ，结果会回调通知 callback。 * @param key * @param callback 如果是 NULL，则会回调从 Init() 中传入的 callback 对象。 * @return 表示发起请求成功，其他值为失败 */ virtual int Remove(const std::string&amp; key, DataStoreCallback* callback = NULL) = 0;protected: /// 存放初始化传入的回调指针 DataStoreCallback* callback_;&#125;; 针对上面的 DataStore 模型，可以实现出多个具体的实现： 文件存储 Redis 其他的各种数据库 基本上文件存储，是每个操作系统都会具备，所以在测试和一般场景下，是最方便的用法，所以这个是一定需要的。 在游戏的持久化数据里面，还有两类功能是比较常用的，一种是排行榜的使用；另外一种是拍卖行。这两个功能是基本的 Key-Value 无法完成的。使用 SQL 或者 Redis 一类 NOSQL 都有排序功能，所以实现排行榜问题不大。而拍卖行功能，则需要多个索引，所以只有一个索引的 Key-Value NoSQL 是无法满足的。不过 NOSQL 也可以“手工”的去建立多个 Key 的记录。不过这类需求，还真的很难统一到某一个框架里面，所以设计也是有限度，包含太多的东西可能还会有反效果。因此我并不打算在持久化这里包含太多的模型。]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你从头写游戏服务器框架一]]></title>
    <url>%2F2019%2F06%2F04%2F%E6%95%99%E4%BD%A0%E4%BB%8E%E5%A4%B4%E5%86%99%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A1%86%E6%9E%B6%E4%B8%80%2F</url>
    <content type="text"><![CDATA[原文出处 前言大概已经有差不多一年没写技术文章了，原因是今年投入了一些具体游戏项目的开发。这些新的游戏项目，比较接近独立游戏的开发方式。我觉得公司的“祖传”服务器框架技术不太适合，所以从头写了一个游戏服务器端的框架，以便获得更好的开发效率和灵活性。现在项目将近上线，有时间就想总结一下，这样一个游戏服务器框架的设计和实现过程。 这个框架的基本运行环境是 Linux ，采用 C++ 编写。为了能在各种环境上运行和使用，所以采用了 gcc 4.8 这个“古老”的编译器，以 C99 规范开发。 需求由于“越通用的代码，就是越没用的代码”，所以在设计之初，我就认为应该使用分层的模式来构建整个系统。按照游戏服务器的一般需求划分，最基本的可以分为两层： 底层基础功能：包括通信、持久化等非常通用的部分，关注的是性能、易用性、扩展性等指标。 高层逻辑功能：包括具体的游戏逻辑，针对不同的游戏会有不同的设计。 我希望能有一个基本完整的“底层基础功能”的框架，可以被复用于多个不同的游戏。由于目标是开发一个 适合独立游戏开发 的游戏服务器框架。所以最基本的需求分析为： 功能性需求 并发：所有的服务器程序，都会碰到这个基本的问题：如何处理并发任务。一般来说，会有多线程、异步两种技术。多线程编程在编码上比较符合人类的思维习惯，但带来了“锁”这个问题。而异步非阻塞的模型，其程序执行的情况是比较简单的，而且也能比较充分的利用硬件性能，但是问题是很多代码需要以“回调”的形式编写，对于复杂的业务逻辑来说，显得非常繁琐，可读性非常差。虽然这两种方案各有利弊，也有人结合这两种技术希望能各取所长，但是我更倾向于基础是使用异步、单线程、非阻塞的调度方式，因为这个方案是最清晰简单的。为了解决“回调”的问题，我们可以在其上再添加其他的抽象层，比如协程或者添加线程池之类的技术予以改善。 通信：支持 请求响应 模式以及 通知 模式的通信（广播视为一种多目标的通知）。游戏有很多登录、买卖、打开背包之类的功能，都是明确的有请求和响应的。而大量的联机游戏中，多个客户端的位置、HP 等东西都需要经过网络同步，其实就是一种“主动通知”的通信方式。 持久化：可以存取 对象 。游戏存档的格式非常复杂，但其索引的需求往往都是根据玩家 ID 来读写就可以。在很多游戏主机如 PlayStation 上，以前的存档都是可以以类似“文件”的方式存放在记忆卡里的。所以游戏持久化最基本的需求，就是一个 key-value 存取模型。当然，游戏中还会有更复杂的持久化需求，比如排行榜、拍卖行等，这些需求应该额外对待，不适合包含在一个最基本的通用底层中。 缓存：支持远程、分布式的对象缓存。游戏服务基本上都是“带状态”的服务，因为游戏要求响应延迟非常苛刻，基本上都需要利用服务器进程的内存来存放过程数据。但是游戏的数据，往往是变化越快的，价值越低，比如经验值、金币、HP，而等级、装备等变化比较慢的，价值则越高，这种特征，非常适合用一个缓存模型来处理。 协程：可以用 C++ 来编写协程代码，避免大量回调函数分割代码。这个是对于异步代码非常有用的特性，能大大提高代码的可读性和开发效率。特别是把很多底层涉及IO的功能，都提供了协程化 API，使用起来就会像同步的 API 一样轻松惬意。 脚本：初步设想是支持可以用 Lua 来编写业务逻辑。游戏需求变化是出了名快的，用脚本语言编写业务逻辑正好能提供这方面的支持。实际上脚本在游戏行业里的使用非常广泛。所以支持脚本，也是一个游戏服务器框架很重要的能力。 其他功能：包括定时器、服务器端的对象管理等等。这些功能很常用，所以也需要包含在框架中，但已经有很多成熟方案，所以只要选取常见易懂的模型即可。比如对象管理，我会采用类似 Unity 的组件模型来实现。 非功能性需求 灵活性：支持可替换的通信协议；可替换的持久化设备（如数据库）；可替换的缓存设备（如 memcached/redis）；以静态库和头文件的方式发布，不对使用者代码做过多的要求。游戏的运营环境比较复杂，特别是在不同的项目之间，可能会使用不同的数据库、不同的通信协议。但是游戏本身业务逻辑很多都是基于对象模型去设计的，所以应该有一层能够基于“对象”来抽象所有这些底层功能的模型。这样才能让多个不同的游戏，都基于一套底层进行开发。 部署便利性：支持灵活的配置文件、命令行参数、环境变量的引用；支持单独进程启动，而无须依赖数据库、消息队列中间件等设施。一般游戏都会有至少三套运行环境，包括一个开发环境、一个内测环境、一个外测或运营环境。一个游戏的版本更新，往往需要更新多个环境。所以如何能尽量简化部署就成为一个很重要的问题。我认为一个好的服务器端框架，应该能让这个服务器端程序，在无配置、无依赖的情况下独立启动，以符合在开发、测试、演示环境下快速部署。并且能很简单的通过配置文件、或者命令行参数的不同，在集群化下的外部测试或者运营环境下启动。 性能：很多游戏服务器，都会使用异步非阻塞的方式来编程。因为异步非阻塞可以很好的提高服务器的吞吐量，而且可以很明确的控制多个用户任务并发下的代码执行顺序，从而避免多线程锁之类的复杂问题。所以这个框架我也希望是以异步非阻塞作为基本的并发模型。这样做还有另外一个好处，就是可以手工的控制具体的进程，充分利用多核 CPU 服务器的性能。当然异步代码可读性因为大量的回调函数，会变得很难阅读，幸好我们还可以用“协程”来改善这个问题。 扩展性：支持服务器之间的通信，进程状态管理，类似 SOA 的集群管理。自动容灾和自动扩容，其实关键点是服务进程的状态同步和管理。我希望一个通用的底层，可以把所有的服务器间调用，都通过一个统一的集权管理模型管理起来，这样就可以不再每个项目去关心集群间通信、寻址等问题。 一旦需求明确下来，基本的层级结构也可以设计了： 层次 功能 约束 逻辑层 实现更具体的业务逻辑 能调用所有下层代码，但应主要依赖接口层 实现层 对各种具体的通信协议、存储设备等功能的实现 满足下层的接口层来做实现，禁止同层间互相调用 接口层 定义了各模块的基本使用方式，用以隔离具体的实现和设计，从而提供互相替换的能力 本层之间代码可以互相调用，但禁止调用上层代码 工具层 提供通用的 C++ 工具库功能，如 log/json/ini/日期时间/字符串处理 等等 不应该调用其他层代码，也不应该调用同层其他模块 第三方库 提供诸如 redis/tcaplus 或者其他现成功能，其地位和“工具层”一样 不应该调用其他层代码，甚至不应该修改其源码 最后，整体的架构模块类似： 说明通信处理器缓存持久化功能实现Tcp Udp Kcp Tlv LineJsonHandler ObjectProcessorSession LocalCache RedisMap RamMap ZooKeeperMapFileDataStore RedisDataStroe接口定义Transfer ProtocolServer Client ProcessorDataMap SerializableDataStore工具类库Config LOG JSON Coroutine 通信模块对于通信模块来说，需要有灵活的可替换协议的能力，就必须按一定的层次进行进一步的划分。对于游戏来说，最底层的通信协议，一般会使用 TCP 和 UDP 这两种，在服务器之间，也会使用消息队列中间件一类通信软件。框架必须要有能同事支持这几通信协议的能力。故此设计了一个层次为: Transport 在协议层面，最基本的需求有“分包”“分发”“对象序列化”等几种需求。如果要支持“请求-响应”模式，还需要在协议中带上“序列号”的数据，以便对应“请求”和“响应”。另外，游戏通常都是一种“会话”式的应用，也就是一系列的请求，会被视为一次“会话”，这就需要协众需要有类似 Session ID 这种数据。为了满足这些需求，设计一个层次为： Protocol 拥有了以上两个层次，是可以完成最基本的协议层能力了。但是，我们往往希望业务数据的协议包，能自动化的成为编程中的 对象，所以在处理消息体这里，需要一个可选的额外层次，用来把字节数组，转换成对象。所以我设计了一个特别的处理器：ObjectProcessor ，去规范通信模块中对象序列化、反序列化的接口。 输入 层次 功能 输出 data Transport 通信 buffer buffer Protocol 分包 Message Message Processor 分发 object object 处理模块 处理 业务逻辑 Transport此层次是为了统一各种不同的底层传输协议而设置的，最基本应该支持 TCP 和 UDP 这两种协议。对于通信协议的抽象，其实在很多底层库也做的非常好了，比如 Linux 的 socket 库，其读写 API 甚至可以和文件的读写通用。C# 的 Socket 库在 TCP 和 UDP 之间，其 api 也几乎是完全一样的。但是由于作用游戏服务器，很多时候还会接入一些特别的“接入层”，比如一些代理服务器，或者一些消息中间件，这些 API 可是五花八门的。另外，在 html5 游戏（比如微信小游戏）和一些页游领域，还有用 HTTP 服务器作为游戏服务器的传统（如使用 WebSocket 协议），这样就需要一个完全不同的传输层了。 服务器传输层在异步模型下的基本使用序列，就是： 在主循环中，不断尝试读取有什么数据可读 如果上一步返回有数据到达了，则读取数据 读取数据处理后，需要发送数据，则向网络写入数据 根据上面三个特点，可以归纳出一个基本的接口： 1234567891011121314151617181920212223242526272829303132333435363738class Transport &#123;public: /** * 初始化Transport对象，输入Config对象配置最大连接数等参数，可以是一个新建的Config对象。 */ virtual int Init(Config* config) = 0; /** * 检查是否有数据可以读取，返回可读的事件数。后续代码应该根据此返回值循环调用Read()提取数据。 * 参数fds用于返回出现事件的所有fd列表，len表示这个列表的最大长度。如果可用事件大于这个数字，并不影响后续可以Read()的次数。 * fds的内容，如果出现负数，表示有一个新的终端等待接入。 */ virtual int Peek(int* fds, int len) = 0; /** * 读取网络管道中的数据。数据放在输出参数 peer 的缓冲区中。 * @param peer 参数是产生事件的通信对端对象。 * @return 返回值为可读数据的长度，如果是 0 表示没有数据可以读，返回 -1 表示连接需要被关闭。 */ virtual int Read( Peer* peer) = 0; /** * 写入数据，output_buf, buf_len为想要写入的数据缓冲区，output_peer为目标队端， * 返回值表示成功写入了的数据长度。-1表示写入出错。 */ virtual int Write(const char* output_buf, int buf_len, const Peer&amp; output_peer) = 0; /** * 关闭一个对端的连接 */ virtual void ClosePeer(const Peer&amp; peer) = 0; /** * 关闭Transport对象。 */ virtual void Close() = 0;&#125; 在上面的定义中，可以看到需要有一个 Peer 类型。这个类型是为了代表通信的客户端（对端）对象。在一般的 Linux 系统中，一般我们用 fd （File Description）来代表。但是因为在框架中，我们还需要为每个客户端建立接收数据的缓存区，以及记录通信地址等功能，所以在 fd 的基础上封装了一个这样的类型。这样也有利于把 UDP 通信以不同客户端的模型，进行封装。 123456789101112131415161718192021///@brief 此类型负责存放连接过来的客户端信息和数据缓冲区class Peer &#123;public: int buf_size_; ///&lt; 缓冲区长度 char* const buffer_;///&lt; 缓冲区起始地址 int produced_pos_; ///&lt; 填入了数据的长度 int consumed_pos_; ///&lt; 消耗了数据的长度 int GetFd() const; void SetFd(int fd); /// 获得本地地址 const struct sockaddr_in&amp; GetLocalAddr() const; void SetLocalAddr(const struct sockaddr_in&amp; localAddr); /// 获得远程地址 const struct sockaddr_in&amp; GetRemoteAddr() const; void SetRemoteAddr(const struct sockaddr_in&amp; remoteAddr);private: int fd_; ///&lt; 收发数据用的fd struct sockaddr_in remote_addr_; ///&lt; 对端地址 struct sockaddr_in local_addr_; ///&lt; 本端地址&#125;; 游戏使用 UDP 协议的特点：一般来说 UDP 是无连接的，但是对于游戏来说，是肯定需要有明确的客户端的，所以就不能简单用一个 UDP socket 的fd 来代表客户端，这就造成了上层的代码无法简单在 UDP 和 TCP 之间保持一致。因此这里使用 Peer 这个抽象层，正好可以解决这个问题。这也可以用于那些使用某种消息队列中间件的情况，因为可能这些中间件，也是多路复用一个 fd 的，甚至可能就不是通过使用 fd 的 API 来开发的。 对于上面的 Transport 定义，对于 TCP 的实现者来说，是非常容易能完成的。但是对于 UDP 的实现者来说，则需要考虑如何充分利用 Peer ，特别是 Peer.fd_ 这个数据。我在实现的时候，使用了一套虚拟的 fd 机制，通过一个客户端的 IPv4 地址到 int 的对应 Map ，来对上层提供区分客户端的功能。在 Linux 上，这些 IO 都可以使用 epoll 库来实现，在 Peek() 函数中读取 IO 事件，在 Read()/Write() 填上 socket 的调用就可以了。 另外，为了实现服务器之间的通信，还需要设计和 Tansport 对应的一个类型：Connector 。这个抽象基类，用于以客户端模型对服务器发起请求。其设计和 Transport 大同小异。除了 Linux 环境下的 Connecotr ，我还实现了在 C# 下的代码，以便用 Unity 开发的客户端可以方便的使用。由于 .NET 本身就支持异步模型，所以其实现也不费太多功夫。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @brief 客户端使用的连接器类，代表传输协议，如 TCP 或 UDP */class Connector &#123;public: virtual ~Connector() &#123;&#125; /** * @brief 初始化建立连接等 * @param config 需要的配置 * @return 0 为成功 */ virtual int Init(Config* config) = 0; /** * @brief 关闭 */ virtual void Close() = 0; /** * @brief 读取是否有网络数据到来 * 读取有无数据到来，返回值为可读事件的数量，通常为1 * 如果为0表示没有数据可以读取。 * 如果返回 -1 表示出现网络错误，需要关闭此连接。 * 如果返回 -2 表示此连接成功连上对端。 * @return 网络数据的情况 */ virtual int Peek() = 0; /** * @brief 读取网络数 * 读取连接里面的数据，返回读取到的字节数，如果返回0表示没有数据， * 如果buffer_length是0, 也会返回0， * @return 返回-1表示连接需要关闭（各种出错也返回0） */ virtual int Read(char* ouput_buffer, int buffer_length) = 0; /** * @brief 把input_buffer里的数据写入网络连接，返回写入的字节数。 * @return 如果返回-1表示写入出错，需要关闭此连接。 */ virtual int Write(const char* input_buffer, int buffer_length) = 0;protected: Connector()&#123;&#125;&#125;; Protocol对于通信“协议”来说，其实包含了许许多多的含义。在众多的需求中，我所定义的这个协议层，只希望完成四个最基本的能力： 分包：从流式传输层切分出一个个单独的数据单元，或者把多个“碎片”数据拼合成一个完整的数据单元的能力。一般解决这个问题，需要在协议头部添加一个“长度”字段。 请求响应对应：这对于异步非阻塞的通信模式下，是非常重要的功能。因为可能在一瞬间发出了很多个请求，而回应则会不分先后的到达。协议头部如果有一个不重复的“序列号”字段，就可以对应起哪个回应是属于哪个请求的。 会话保持：由于游戏的底层网络，可能会使用 UDP 或者 HTTP 这种非长连接的传输方式，所以要在逻辑上保持一个会话，就不能单纯的依靠传输层。加上我们都希望程序有抗网络抖动、断线重连的能力，所以保持会话成为一个常见的需求。我参考在 Web 服务领域的会话功能，设计了一个 Session 功能，在协议中加上 Session ID 这样的数据，就能比较简单的保持会话。 分发：游戏服务器必定会包含多个不同的业务逻辑，因此需要多种不同数据格式的协议包，为了把对应格式的数据转发。 除了以上三个功能，实际上希望在协议层处理的能力，还有很多，最典型的就是对象序列化的功能，还有压缩、加密功能等等。我之所以没有把对象序列化的能力放在 Protocol 中，原因是对象序列化中的“对象”本身是一个业务逻辑关联性非常强的概念。在 C++ 中，并没有完整的“对象”模型，也缺乏原生的反射支持，所以无法很简单的把代码层次通过“对象”这个抽象概念划分开来。但是我也设计了一个 ObjectProcessor ，把对象序列化的支持，以更上层的形式结合到框架中。这个 Processor 是可以自定义对象序列化的方法，这样开发者就可以自己选择任何“编码、解码”的能力，而不需要依靠底层的支持。 至于压缩和加密这一类功能，确实是可以放在 Protocol 层中实现，甚至可以作为一个抽象层次加入 Protocol ，可能只有一个 Protocol 层不足以支持这么丰富的功能，需要好像 Apache Mina 这样，设计一个“调用链”的模型。但是为了简单起见，我觉得在具体需要用到的地方，再额外添加 Protocol 的实现类就好，比如添加一个“带压缩功能的 TLV Protocol 类型”之类的。 消息本身被抽象成一个叫 Message 的类型，它拥有“服务名字”“会话ID”两个消息头字段，用以完成“分发”和“会话保持”功能。而消息体则被放在一个字节数组中，并记录下字节数组的长度。 123456789101112131415161718192021222324252627282930313233343536373839404142434445enum MessageType &#123; TypeError, ///&lt; 错误的协议 TypeRequest, ///&lt; 请求类型，从客户端发往服务器 TypeResponse, ///&lt; 响应类型，服务器收到请求后返回 TypeNotice ///&lt; 通知类型，服务器主动通知客户端&#125;;///@brief 通信消息体的基类///基本上是一个 char[] 缓冲区struct Message &#123;public: static int MAX_MAESSAGE_LENGTH; static int MAX_HEADER_LENGTH; MessageType type; ///&lt; 此消息体的类型(MessageType)信息 virtual ~Message(); virtual Message&amp; operator=(const Message&amp; right); /** * @brief 把数据拷贝进此包体缓冲区 */ void SetData(const char* input_ptr, int input_length); ///@brief 获得数据指针 inline char* GetData() const&#123; return data_; &#125; ///@brief 获得数据长度 inline int GetDataLen() const&#123; return data_len_; &#125; char* GetHeader() const; int GetHeaderLen() const;protected: Message(); Message(const Message&amp; message); private: char* data_; // 包体内容缓冲区 int data_len_; // 包体长度&#125;; 根据之前设计的“请求响应”和“通知”两种通信模式，需要设计出三种消息类型继承于 Message，他们是： Request 请求包 Response 响应包 Notice 通知包 Request 和 Response 两个类，都有记录序列号的 seq_id 字段，但 Notice 没有。Protocol 类就是负责把一段 buffer 字节数组，转换成 Message 的子类对象。所以需要针对三种 Message 的子类型都实现对应的 Encode() / Decode() 方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class Protocol &#123;public: virtual ~Protocol() &#123; &#125; /** * @brief 把请求消息编码成二进制数据 * 编码，把msg编码到buf里面，返回写入了多长的数据，如果超过了 len，则返回-1表示错误。 * 如果返回 0 ，表示不需要编码，框架会直接从 msg 的缓冲区读取数据发送。 * @param buf 目标数据缓冲区 * @param offset 目标偏移量 * @param len 目标数据长度 * @param msg 输入消息对象 * @return 编码完成所用的字节数，如果 &lt; 0 表示出错 */ virtual int Encode(char* buf, int offset, int len, const Request&amp; msg) = 0; /** * 编码，把msg编码到buf里面，返回写入了多长的数据，如果超过了 len，则返回-1表示错误。 * 如果返回 0 ，表示不需要编码，框架会直接从 msg 的缓冲区读取数据发送。 * @param buf 目标数据缓冲区 * @param offset 目标偏移量 * @param len 目标数据长度 * @param msg 输入消息对象 * @return 编码完成所用的字节数，如果 &lt; 0 表示出错 */ virtual int Encode(char* buf, int offset, int len, const Response&amp; msg) = 0; /** * 编码，把msg编码到buf里面，返回写入了多长的数据，如果超过了 len，则返回-1表示错误。 * 如果返回 0 ，表示不需要编码，框架会直接从 msg 的缓冲区读取数据发送。 * @param buf 目标数据缓冲区 * @param offset 目标偏移量 * @param len 目标数据长度 * @param msg 输入消息对象 * @return 编码完成所用的字节数，如果 &lt; 0 表示出错 */ virtual int Encode(char* buf, int offset, int len, const Notice&amp; msg) = 0; /** * 开始编码，会返回即将解码出来的消息类型，以便使用者构造合适的对象。 * 实际操作是在进行“分包”操作。 * @param buf 输入缓冲区 * @param offset 输入偏移量 * @param len 缓冲区长度 * @param msg_type 输出参数，表示下一个消息的类型，只在返回值 &gt; 0 的情况下有效，否则都是 TypeError * @return 如果返回0表示分包未完成，需要继续分包。如果返回-1表示协议包头解析出错。其他返回值表示这个消息包占用的长度。 */ virtual int DecodeBegin(const char* buf, int offset, int len, MessageType* msg_type) = 0; /** * 解码，把之前DecodeBegin()的buf数据解码成具体消息对象。 * @param request 输出参数，解码对象会写入此指针 * @return 返回0表示成功，-1表示失败。 */ virtual int Decode(Request* request) = 0; /** * 解码，把之前DecodeBegin()的buf数据解码成具体消息对象。 * @param request 输出参数，解码对象会写入此指针 * @return 返回0表示成功，-1表示失败。 */ virtual int Decode(Response* response) = 0; /** * 解码，把之前DecodeBegin()的buf数据解码成具体消息对象。 * @param request 输出参数，解码对象会写入此指针 * @return 返回0表示成功，-1表示失败。 */ virtual int Decode(Notice* notice) = 0;protected: Protocol() &#123; &#125;&#125;; 这里有一点需要注意，由于 C++ 没有内存垃圾搜集和反射的能力，在解释数据的时候，并不能一步就把一个 char[] 转换成某个子类对象，而必须分成两步处理。 先通过 DecodeBegin() 来返回，将要解码的数据是属于哪个子类型的。同时完成分包的工作，通过返回值来告知调用者，是否已经完整的收到一个包。 调用对应类型为参数的 Decode() 来具体把数据写入对应的输出变量。 对于 Protocol 的具体实现子类，我首先实现了一个 LineProtocol ，是一个非常不严谨的，基于文本ASCII编码的，用空格分隔字段，用回车分包的协议。用来测试这个框架是否可行。因为这样可以直接通过 telnet 工具，来测试协议的编解码。然后我按照 TLV （Type Length Value）的方法设计了一个二进制的协议。大概的定义如下： 协议分包： [消息类型:int:2] [消息长度:int:4] [消息内容:bytes:消息长度] 消息类型取值: 0x00 Error 0x01 Request 0x02 Response 0x03 Notice 包类型字段编码细节Request服务名[字段:int:2][长度:int:2][字符串内容:chars:消息长度]序列号[字段:int:2][整数内容:int:4]会话ID[字段:int:2][整数内容:int:4]消息体[字段:int:2][长度:int:2][字符串内容:chars:消息长度]Response服务名[字段:int:2][长度:int:2][字符串内容:chars:消息长度]序列号[字段:int:2][整数内容:int:4]会话ID[字段:int:2][整数内容:int:4]消息体[字段:int:2][长度:int:2][字符串内容:chars:消息长度]Notice服务名[字段:int:2][长度:int:2][字符串内容:chars:消息长度]消息体[字段:int:2][长度:int:2][字符串内容:chars:消息长度] 一个名为 TlvProtocol 的类型完成对这个协议的实现。 Processor处理器层是我设计用来对接具体业务逻辑的抽象层，它主要通过输入参数 Request 和 Peer 来获得客户端的输入数据，然后通过 Server 类的 Reply()/Inform() 来返回 Response 和 Notice 消息。实际上 Transport 和 Protocol 的子类们，都属于 net 模块，而各种 Processor 和 Server/Client 这些功能类型，属于另外一个 processor 模块。这样设计的原因，是希望所有 processor 模块的代码单向的依赖 net 模块的代码，但反过来不成立。 Processor 基类非常简单，就是一个处理函数回调函数入口 Process()： 1234567891011121314151617181920212223242526///@brief 处理器基类，提供业务逻辑回调接口class Processor &#123;public: Processor(); virtual ~Processor(); /** * 初始化一个处理器，参数server为业务逻辑提供了基本的能力接口。 */ virtual int Init(Server* server, Config* config = NULL); /** * 处理请求-响应类型包实现此方法，返回值是0表示成功，否则会被记录在错误日志中。 * 参数peer表示发来请求的对端情况。其中 Server 对象的指针，可以用来调用 Reply(), * Inform() 等方法。如果是监听多个服务器，server 参数则会是不同的对象。 */ virtual int Process(const Request&amp; request, const Peer&amp; peer, Server* server); /** * 关闭清理处理器所占用的资源 */ virtual int Close();&#125;; 设计完 Transport/Protocol/Processor 三个通信处理层次后，就需要一个组合这三个层次的代码，那就是 Server 类。这个类在 Init() 的时候，需要上面三个类型的子类作为参数，以组合成不同功能的服务器，如： 12345TlvProtocol tlv_protocol; // Type Length Value 格式分包协议，需要和客户端一致TcpTransport tcp_transport; // 使用 TCP 的通信协议，默认监听 0.0.0.0:6666EchoProcessor echo_processor; // 业务逻辑处理器Server server; // DenOS 的网络服务器主对象server.Init(&amp;tcp_transport, &amp;tlv_protocol, &amp;echo_processor); // 组装一个游戏服务器对象：TLV 编码、TCP 通信和回音服务 Server 类型还需要一个 Update() 函数，让用户进程的“主循环”不停的调用，用来驱动整个程序的运行。这个 Update() 函数的内容非常明确： 检查网络是否有数据需要处理（通过 Transport 对象） 有数据的话就进行解码处理（通过 Protocol 对象） 解码成功后进行业务逻辑的分发调用（通过 Processor 对象） 另外，Server 还需要处理一些额外的功能，比如维护一个会话缓存池（Session），提供发送 Response 和 Notice 消息的接口。当这些工作都完成后，整套系统已经可以用来作为一个比较“通用”的网络消息服务器框架存在了。剩下的就是添加各种 Transport/Protocol/Processor 子类的工作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class Server &#123;public: Server(); virtual ~Server(); /** * 初始化服务器，需要选择组装你的通信协议链 */ int Init(Transport* transport, Protocol* protocol, Processor* processor, Config* config = NULL); /** * 阻塞方法，进入主循环。 */ void Start(); /** * 需要循环调用驱动的方法。如果返回值是0表示空闲。其他返回值表示处理过的任务数。 */ virtual int Update(); void ClosePeer(Peer* peer, bool is_clear = false); //关闭当个连接，is_clear 表示是否最终整体清理 /** * 关闭服务器 */ void Close(); /** * 对某个客户端发送通知消息， * 参数peer代表要通知的对端。 */ int Inform(const Notice&amp; notice, const Peer&amp; peer); /** * 对某个 Session ID 对应的客户端发送通知消息，返回 0 表示可以发送，其他值为发送失败。 * 此接口能支持断线重连，只要客户端已经成功连接，并使用旧的 Session ID，同样有效。 */ int Inform(const Notice&amp; notice, const std::string&amp; session_id); /** * 对某个客户端发来的Request发回回应消息。 * 参数response的成员seqid必须正确填写，才能正确回应。 * 返回0成功，其它值（-1）表示失败。 */ int Reply(Response* response, const Peer&amp; peer); /** * 对某个 Session ID 对应的客户端发送回应消息。 * 参数 response 的 seqid 成员系统会自动填写会话中记录的数值。 * 此接口能支持断线重连，只要客户端已经成功连接，并使用旧的 Session ID，同样有效。 * 返回0成功，其它值（-1）表示失败。 */ int Reply(Response* response, const std::string&amp; session_id); /** * 会话功能 */ Session* GetSession(const std::string&amp; session_id = "", bool use_this_id = false); Session* GetSessionByNumId(int session_id = 0); bool IsExist(const std::string&amp; session_id); &#125;; 有了 Server 类型，肯定也需要有 Client 类型。而 Client 类型的设计和 Server 类似，但就不是使用 Transport 接口作为传输层，而是 Connector 接口。不过 Protocol 的抽象层是完全重用的。Client 并不需要 Processor 这种形式的回调，而是直接传入接受数据消息就发起回调的接口对象 ClientCallback。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788class ClientCallback &#123;public: ClientCallback() &#123; &#125; virtual ~ClientCallback() &#123; // Do nothing &#125; /** * 当连接建立成功时回调此方法。 * @return 返回 -1 表示不接受这个连接，需要关闭掉此连接。 */ virtual int OnConnected() &#123; return 0; &#125; /** * 当网络连接被关闭的时候，调用此方法 */ virtual void OnDisconnected() &#123; // Do nothing &#125; /** * 收到响应，或者请求超时，此方法会被调用。 * @param response 从服务器发来的回应 * @return 如果返回非0值，服务器会打印一行错误日志。 */ virtual int Callback(const Response&amp; response) &#123; return 0; &#125; /** * 当请求发生错误，比如超时的时候，返回这个错误 * @param err_code 错误码 */ virtual void OnError(int err_code)&#123; WARN_LOG("The request is timeout, err_code: %d", err_code); &#125; /** * 收到通知消息时，此方法会被调用 */ virtual int Callback(const Notice&amp; notice) &#123; return 0; &#125; /** * 返回此对象是否应该被删除。此方法会被在 Callback() 调用前调用。 * @return 如果返回 true，则会调用 delete 此对象的指针。 */ virtual bool ShouldBeRemoved() &#123; return false; &#125;&#125;;class Client : public Updateable &#123; public: Client(); virtual ~Client(); /** * 连接服务器 * @param connector 传输协议，如 TCP， UDP ... * @param protocol 分包协议，如 TLV, Line, TDR ... * @param notice_callback 收到通知后触发的回调对象，如果传输协议有“连接概念”（如TCP/TCONND），建立、关闭连接时也会调用。 * @param config 配置文件对象，将读取以下配置项目：MAX_TRANSACTIONS_OF_CLIENT 客户端最大并发连接数; BUFFER_LENGTH_OF_CLIENT客户端收包缓存；CLIENT_RESPONSE_TIMEOUT 客户端响应等待超时时间。 * @return 返回 0 表示成功，其他表示失败 */ int Init(Connector* connector, Protocol* protocol, ClientCallback* notice_callback = NULL, Config* config = NULL); /** * callback 参数可以为 NULL，表示不需要回应，只是单纯的发包即可。 */ virtual int SendRequest(Request* request, ClientCallback* callback = NULL); /** * 返回值表示有多少数据需要处理，返回-1为出错，需要关闭连接。返回0表示没有数据需要处理。 */ virtual int Update(); virtual void OnExit(); void Close(); Connector* connector() ; ClientCallback* notice_callback() ; Protocol* protocol() ;&#125;; 至此，客户端和服务器端基本设计完成，可以直接通过编写测试代码，来检查是否运行正常。 下期将推送这个系列后一篇《数据持久化模块和缓存模块》,感谢关注。]]></content>
      <categories>
        <category>GameServer</category>
      </categories>
      <tags>
        <tag>ServerFramework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unity的AnimationCurve的实现方法]]></title>
    <url>%2F2019%2F05%2F24%2FUnity%E7%9A%84AnimationCurve%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原文原文出处 方法一：没有把参数t从系数中分离开来，直接混在一起计算系数a，b，c，d。这样看算法比较直观，但是不是最优化的。 1234567891011121314151617float Evaluate(float t, Keyframe keyframe0, Keyframe keyframe1)&#123; float dt = keyframe1.time - keyframe0.time; float m0 = keyframe0.outTangent * dt; float m1 = keyframe1.inTangent * dt; float t2 = t * t; float t3 = t2 * t; float a = 2 * t3 - 3 * t2 + 1; float b = t3 - 2 * t2 + t; float c = t3 - t2; float d = -2 * t3 + 3 * t2; return a * keyframe0.value + b * m0 + c * m1 + d * keyframe1.value;&#125; 方法二：通过解四元三次方程组，得到系数a，b，c，d，这样得出的系数计算公式看着很复杂，但是因为系数可以提前计算好，所以效率要比方法一好。 Unity define a curve with 2 keyframes, each composed of a point and a tangent. I guess the simplest curve matching that is a third degree polynomial (a cubic function). Given the 2 points and tangents, it is possible to compute the polynomial coefficients simply by solving the following equation system: 1234(1) a*p1x^3 + b*p1x^2 + c*p1x + d = p1y(2) a*p2x^3 + b*p2x^2 + c*p2x + d = p2y(3) 3*a*p1x^2 + 2*b*p1x + c = tp1(4) 3*a*p2x^2 + 2*b*p2x + c = tp2 You can solve this manually or using a computer algebra system. This gives you: 1234float a = (p1x * tp1 + p1x * tp2 - p2x * tp1 - p2x * tp2 - 2 * p1y + 2 * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x);float b = ((-p1x * p1x * tp1 - 2 * p1x * p1x * tp2 + 2 * p2x * p2x * tp1 + p2x * p2x * tp2 - p1x * p2x * tp1 + p1x * p2x * tp2 + 3 * p1x * p1y - 3 * p1x * p2y + 3 * p1y * p2x - 3 * p2x * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x));float c = ((p1x * p1x * p1x * tp2 - p2x * p2x * p2x * tp1 - p1x * p2x * p2x * tp1 - 2 * p1x * p2x * p2x * tp2 + 2 * p1x * p1x * p2x * tp1 + p1x * p1x * p2x * tp2 - 6 * p1x * p1y * p2x + 6 * p1x * p2x * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x));float d = ((p1x * p2x * p2x * p2x * tp1 - p1x * p1x * p2x * p2x * tp1 + p1x * p1x * p2x * p2x * tp2 - p1x * p1x * p1x * p2x * tp2 - p1y * p2x * p2x * p2x + p1x * p1x * p1x * p2y + 3 * p1x * p1y * p2x * p2x - 3 * p1x * p1x * p2x * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x)); Then, to evaluate the value: 1234float Evaluate(float t)&#123; return a*t*t*t + b*t*t + c*t + d;&#125; I checked with Unity with the following quick and dirty code: 1234567891011121314151617181920212223242526272829using UnityEngine;[ExecuteInEditMode]public class TestAnimCurve : MonoBehaviour &#123; public AnimationCurve anim = AnimationCurve.EaseInOut(0, 0, 1, 1); float a; float b; float c; float d; void Update () &#123; float p1x= anim.keys[0].time; float p1y= anim.keys[0].value; float tp1=anim.keys[0].outTangent; float p2x=anim.keys[1].time; float p2y= anim.keys[1].value; float tp2= anim.keys[1].inTangent; Debug.Log(p1x+ ", " + p1y+ ", " + tp1 + ", " + p2x + ", " + p2y + ", " + tp2); Debug.Log("Evaluate Unity: " + anim.Evaluate(0.1f) + ", " + anim.Evaluate(0.2f) + ", " + anim.Evaluate(0.3f) + ", " + anim.Evaluate(0.4f) + ", " + anim.Evaluate(0.5f) + ", " + anim.Evaluate(0.6f) + ", " + anim.Evaluate(0.76f) + ", " + anim.Evaluate(0.88f) + ", " + anim.Evaluate(0.98f)); a = (p1x * tp1 + p1x * tp2 - p2x * tp1 - p2x * tp2 - 2 * p1y + 2 * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x); b = ((-p1x * p1x * tp1 - 2 * p1x * p1x * tp2 + 2 * p2x * p2x * tp1 + p2x * p2x * tp2 - p1x * p2x * tp1 + p1x * p2x * tp2 + 3 * p1x * p1y - 3 * p1x * p2y + 3 * p1y * p2x - 3 * p2x * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x)); c = ((p1x * p1x * p1x * tp2 - p2x * p2x * p2x * tp1 - p1x * p2x * p2x * tp1 - 2 * p1x * p2x * p2x * tp2 + 2 * p1x * p1x * p2x * tp1 + p1x * p1x * p2x * tp2 - 6 * p1x * p1y * p2x + 6 * p1x * p2x * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x)); d = ((p1x * p2x * p2x * p2x * tp1 - p1x * p1x * p2x * p2x * tp1 + p1x * p1x * p2x * p2x * tp2 - p1x * p1x * p1x * p2x * tp2 - p1y * p2x * p2x * p2x + p1x * p1x * p1x * p2y + 3 * p1x * p1y * p2x * p2x - 3 * p1x * p1x * p2x * p2y) / (p1x * p1x * p1x - p2x * p2x * p2x + 3 * p1x * p2x * p2x - 3 * p1x * p1x * p2x)); Debug.Log("Evaluate Cubic: " + Evaluate(0.1f) + ", " + Evaluate(0.2f) + ", " + Evaluate(0.3f) + ", " + Evaluate(0.4f) + ", " + Evaluate(0.5f) + ", " + Evaluate(0.6f) + ", " + Evaluate(0.76f) + ", " + Evaluate(0.88f) + ", " + anim.Evaluate(0.98f)); &#125; float Evaluate(float t) &#123; return a * t * t * t + b * t * t + c * t + d; &#125; &#125; After modifing tangents of the animation curve, the debug messages produced by this code are: 1230, 0, -4.484611, 1, 1, -10.23884Evaluate Unity: -0.2431039, -0.1423873, 0.2018093, 0.6891449, 1.219279, 1.691871, 2.077879, 1.854902, 1.193726Evaluate Cubic: -0.2431039, -0.1423873, 0.2018092, 0.6891448, 1.219279, 1.691871, 2.077879, 1.854902, 1.193726 So it really seams that this approach is the math behind AnimationCurve.Evaluate ;) 方法三：系数的计算公式看着比较简单，并且也可以提前计算好，效率应该和方法二差不多。 AnimationCurve is a simple spline of cubic equations, where for each segment you specify the (time, value) coordinates and slope for each of two end points (the first point’s “out” “tangent”, second one’s “in”). A cubic equation is the simplest polynomial which can meet these criteria. This is also known as a cubic Hermite spline, where the only difference is that AnimationCurve allows you to assign different slopes for each direction from a keyframe (a point), which creates discontinuities in the first derivative (sharp angles). However, this does not change the formula. https://en.wikipedia.org/wiki/Cubic_Hermite_spline It took me some time (much more than I’d like to admit…) but I simplified the system of cubic equations for the endpoints into this form. 12345pd = 1/(p2x-p1x)td = t-p1xta = td*pdout(t) = (((p2s+p1s)/2 - (p2y-p1y)*pd)*ta*(t*2 + p1x - p2x*3) + (p2s-p1s)/2*(t + p1x - p2x*2))*ta + p2s*td + p1y Isolated t to calculate coefficients in advance, providing a fast t evaluation: 123456789pd = 1/(p2x-p1x)td = pd*(p2s-p1s)a = pd^2*(p2s + p1s - 2*pd*(p2y-p1y))b = (-a*3*(p2x + p1x) + td)/2c = p2x*(a*3*p1x + -td) + p2sd = p1x*(a/2*p1x*(-p2x*3 + p1x) + td*(p2x + -p1x/2) + -p2s) + p1yout(t) = t*(t^2*a + t*b + c) + d I had the same question, was looking for a straightforward answer. Because my search query took me here and this is the closest I could find, I assume such solutions to this exact problem are not abundant, so I thought I’d share mine. I derived it because I need to emulate the same function outside of unity. Looking at the other three answers here, they seem to be correct but not simplified (trust me this is not an easy task) (Varaughe’s leaves out the evaluation of the Bezier curve he made from the AnimationCurve, as it is a well-known thing). I tested these against AnimationCurve for correctness with a Python script which can randomize all six parameters of the cubic function: 12345678910111213141516171819202122232425262728293031323334353637383940414243# isolated tdef curve1(t,p1x,p2x,p1y,p2y,p1s,p2s): e = 1/(p2x-p1x) f = e*(p1s-p2s) a = (p1s + p2s + 2*e*(p1y-p2y))*e**2 b = (a*3*(p2x+p1x) + f)/-2 c = p2x*(a*3*p1x + f) + p2s d = p1x/2*(a*p1x*(p1x - p2x*3) + f*(p1x - p2x*2) - p2s*2) + p1y return t*(t**2*a + t*b + c) + d# short formdef curve2(t,p1x,p2x,p1y,p2y,p1s,p2s): pd = 1/(p2x-p1x) td = t-p1x ta = td*pd return ((t*2 + p1x - p2x*3)*((p2s+p1s)/2 - (p2y-p1y)*pd)*ta + (t + p1x - p2x*2)*(p2s-p1s)/2)*ta + p2s*td + p1y# Pauliusdef curve_Paulius(t,p1x,p2x,p1y,p2y,p1s,p2s): xd = p2x-p1x t = (t-p1x)/xd return (2*t**3 - 3*t**2 + 1)*p1y + (t**3 - 2*t**2 + t)*p1s*xd + (t**3 - t**2)*p2s*xd + (-2*t**3 + 3*t**2)*p2ydef test(curve, times, curve_parameters): out = [] for i in times: out.append(curve(i, *curve_parameters)) return outfrom random import randomrand_args = []for i in range(4): rand_args.append(random()*100)for i in range(2): rand_args.append(random()*2)rand_times = []e,f = rand_args[0:2]points = 21# interval count is points - 1for i in range(points - 1): td = (f - e)/(points - 1) rand_times.append(random()*td + e + i*td)]]></content>
      <categories>
        <category>GameEngine</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paragon Feature Examples: Animation Techniques | Feature Highlight | Unreal Engine]]></title>
    <url>%2F2019%2F05%2F22%2FParagon-Feature-Examples-Animation-Techniques-Feature-Highlight-Unreal-Engine%2F</url>
    <content type="text"><![CDATA[主要内容：该视频由Paragon游戏制作者Laurent Delayen(Senior Programmer, Gameplay)和RayArnett(Senior Artist, Animation)讲述制作过程中使用到的动画技术。 包括：Transitions（动画过渡），Synchronize marker（同步标记），Turn（转向动作），Speed Warping（使用IK根据速度调整步幅），Slope Warping（使用IK根据斜坡斜率调整脚步位置），Jump动作的制作，以及AnimGraph。 声明：以下英文部分为个人听力记录，视频无字幕，不保证准确性，其中带问号的为听不清的单词。中文翻译部分更不保证准确性，建议结合视频看英文部分。 24’50’’ Transitions.Our idea is to do motion prediction. Predict where the character is going to stop. Gives a few frame to do the anticipation for the stop. When we do the starts, we drop the marker and look backwards. Drop the marker location in the world, and we are using them to synchronizing our animations. We translate the physical movement to a curve, a distance curve. In the case of start transition, look backing time and where the marker was, that curve describes the distance of the actor to that marker. 我们的想法是做动作预测。预测角色将要停止的位置。用一些动作帧来做停止动作。当做起步动作时，在起点放标记然后往回看。在世界中放下标记位置，用他们来同步我们的动作。我们把物理移动转换成一个距离曲线。 在起步的过渡时，往回看标记所在的位置，曲线描述actor到标记的距离。 26’46’’ 使用DistanceCurve保存Actor to Marker的位置。 28’21’’ Backward动作的Transition. 28’50’’ Pivoting。两个动作的结合。Reach the pivot and leave that pivot. 29’39’’ when we call the actor in the studio, remap the recorded motion to the map. Using the distance, we get really precise foot placement, no foot sliding at all. 30’15’’ For people who are familiar with root motion, like the animation tells the capsule whereto go. If I cross the room in the animation, the capsule will follow the animation along. With this kind of foot slide ahead(?) basically the capsule is gonna cross the room, and looks that curve it say, it walks? across the room,am I in this animation, and plays that for aim. so uses the motion of the capsule to pick which frame to play in the animation. the animation stay is locked. to the capsule is doing movement. If the capsule is sitting still and starts moving forwards , the animation says OK, 2 units from where I started,I’ll find that on this curve and play that for aim. 对熟悉RootMotion的人来说，动画告诉胶囊该移动到哪里。如果人物在动画中穿过房间，胶囊会跟随动画。所以用胶囊的运动来选取哪一帧来在动画中播放。在胶囊运动的过程中，动画所在位置是锁定的。如果胶囊还在原地准备向前移动，动画说距离出发的位置有2个单位距离，会在曲线上找到那帧来播放。 33’ synchronize marker? 从start到loop动作。当toe和root交叉时，有标记。 33’47’’ The reason for doing that way is, they sort of tells us, spash? you where the foot is and we can synchronize animation that way. We thought about this, other people seem to describe the movement when the foot touch the ground and leave the ground,sort of like the anpitute? of the step. and we decided to go instead with the spash? where it was around the player to minimize foot sliding. So when you transition between animations, we sort of try to get the closest position. to where the capsule is basically to minimize sliding. So we sacrifice bits the where the foot is, we try to keep the position. 35’10’’ 以脚的动作为标准，可以保证不会滑步。 37’50’’ 转向。 38’50’’ 角度Curve。 41’02’’ 动画蓝图。 42’40’’Backward-Forward转换。DistanceCurve标记Turn角度。 43’40’’ .速度低会导致动作慢，用Speed Warping可以小步移动。 45’55’’ 显示Speed Warping和无Speed Warping的区别。 51’ 原理：根据SpeedScale移动IKBone。 53’24’’ 比如2X速率移动，IKBone在2X距离位置 54’50’’ 后退动作的有无SpeedWarping比较 58’ BlendSpace。JogForwardSlopeLean。slope斜率。左右倾斜角度。速度 60’ Jogging状态机节点。Blend-&gt;AimOffset-&gt;RotateRoot 60’50’’ 移动效果。 62’40’’ Slope移动效果。 64’43’’ SlopeWarping开关的区别。 66’41’’ 斜坡上开启网格模式的效果。能看到地板位置，Normal，IKBone的位置 68’14’’ TwoBoneIK for legs蓝图节点 68’47’’ 改变角度观察BlendSpace 70’50’’ 脚悬空不可避免。 73’30’’ Jump动作。Jumping分解为3个部分。InAir，Apex，Landing。用DistanceCurve标记到地面的距离。We use it here to get a few extra frames of compression. Because animation runs after physics. So when we note that you are jumping, it’s already too late you are already in jumping. So this allows it to compensate and have a few extra frames that feet on the ground. Because the capsule is moving. 74’53’’ Jump动作演示。 75’43’’DistanceCurveForLanding. Put a marker on where you are gonna lands. Use that to synchronize the feet getting close to the ground. The arc is synchronizing with the apex? 76’06’’ Apex。DistanceCurve. How far before it and go past it. 76’30’’ Recovery Additive.Play on top of anything in the game. 为了nice landing compression.如果不用additive。会Blending Jogforward, backward。Maybe the result is not what would be blended exactly, you still get a nice feel without making a time to time content. 77’38’’ when we do blend between animations, it have difference. Blend feet quickly, Upperbody lowly. 78’50’’ AnimDynamics. Not show. 可以在另一个视频中看到专门讲这个节点。AnimDynamic features used in Paragon.https://www.youtube.com/watch?v=5h5CvZEBBWo 79’36’’ AnimGraph. 总结：一、曲线的使用。根据曲线同步动作。如起步过渡动作中，进入过渡状态时在起点放置标记marker，actor移动时根据当前actor和marker的距离distance，在DistanceCurve上查找该Distance应该对应的帧进行播放。 如原地转身动作（Rotate）中，根据当前actor的旋转角度angle，在Curve上查找该Angle对应的帧进行播放。 二、Synchronize Marker 左右脚同步标记在动画中，当toe和root重合时，添加Notify记录当前是左脚还是右脚。动画过渡时，以脚为基础，避免了滑步的出现。 三、IK的使用SpeedWarping和SlopeWarping，使用IK使脚在动画中处于正确的位置。 四、Jump动作由于胶囊运动，动画制作原地起跳时需要考虑胶囊的移动。Curve标记当前和地面的距离。通过Curve同步动画。]]></content>
      <categories>
        <category>GameEngine</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Animation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏五之演示Demo]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%BA%94%E4%B9%8B%E6%BC%94%E7%A4%BADemo%2F</url>
    <content type="text"><![CDATA[在浏览器中玩 移动蓝球 ：受 Player1 控制, 用左右箭头键 移动红球 ：受 Player2 控制, 用A和D键 canvas { border: dotted 1px; padding: 0; background: lightgray;}This is a sample implementation of a client-server architecture demonstrating the main concepts explained in my Fast-Paced Multiplayer(原文出处) series of articles. It won’t make much sense unless you’ve read the articles first.The code is pure JavaScript and it’s fully contained in this page. It’s less than 500 lines of code, including a lot of comments, showing that once you really understand the concepts, implementing them is relatively straightforward.Although it’s not production-quality code, you may use this code in your own applications. Credit is appreciated although not required.Player 1 view - move with LEFT and RIGHT arrow keys Lag = ms · Prediction · Reconciliation · InterpolationWaiting for connection…Server view · Update times per secondPlayer 2 view - move with A and D keys Lag = ms · Prediction · Reconciliation · InterpolationWaiting for connection… Guided Tour Move the blue ball. There’s considerable delay between pressing the arrow keys and the blue ball actually moving. Without client-side prediction, the client only renders the new position of the ball only after a round-trip to the server. Because of the 250ms lag, this takes a while.Set the player 1 Lag to 0ms, and try again. Now the client and the server move in sync because there’s no delay between them, but the movement isn’t smooth, because the server only updates its internal state 3 times per second. If you increase the update rate of the server to 60, we get smooth movement.But this is not a very realistic scenario. Set the player 1 lag back to 250ms, and the server update rate back to 3. This is closer to the awful conditions where a real game still needs to work.Client-side prediction and server reconciliation to the rescue! Enable both of them for Player 1 and move the blue ball. Now the movement is very smooth, and there’s no perceptible delay between pressing the arrow keys and moving the ball.This still works if you make the conditions even worse - try setting the player 1 lag to 500ms and the server update rate to 1.Now things look fantastic for player 1’s own entity, the blue ball. However, player 2’s view of this same entity looks terrible. Because the low update rate of the server, player 2 only gets a new position for player 1’s entity once per second, so the movement is very jumpy.Enabling client-side prediction and server reconciliation for player 2 do nothing to smooth the movement of the blue ball, because these techniques only affect how a player renders its own entity. It does make a difference if you move the red ball, but now we have the same jumpiness in player 1’s view.To solve this, we use entity interpolation. Enable entity interpolation for player 2 and move the blue ball. Now it moves smoothly, but is always rendered “in the past” compared to player 1 and to the server.You may notice the speed of the interpolated entities may vary. This is an artifact of the interpolation, caused by setting the server update rate too low in relationship with the speeds. This effect should disappear almost entirely if you set the server update rate to 10, which is still pretty low.SummaryClient-Side Prediction and Server Reconciliation are very powerful techniques to make multiplayer games feel responsive even under extremely bad network conditions. Therefore, they are a fundamental part of almost any client/server multiplayer network architecture. // ============================================================================= // An Entity in the world. // ============================================================================= var Entity = function() { this.x = 0; this.speed = 2; // units/s this.position_buffer = []; } // Apply user's input to this entity. Entity.prototype.applyInput = function(input) { this.x += input.press_time*this.speed; } // ============================================================================= // A message queue with simulated network lag. // ============================================================================= var LagNetwork = function() { this.messages = []; } // "Send" a message. Store each message with the timestamp when it should be // received, to simulate lag. LagNetwork.prototype.send = function(lag_ms, message) { this.messages.push({recv_ts: +new Date() + lag_ms, payload: message}); } // Returns a "received" message, or undefined if there are no messages available // yet. LagNetwork.prototype.receive = function() { var now = +new Date(); for (var i = 0; i < this.messages.length; i++) { var message = this.messages[i]; if (message.recv_ts]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏四之延迟补偿实现爆头]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E5%9B%9B%E4%B9%8B%E5%BB%B6%E8%BF%9F%E8%A1%A5%E5%81%BF%E5%AE%9E%E7%8E%B0%E7%88%86%E5%A4%B4%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part IV): Lag Compensation IntroductionThe previous three articles explained a client-server game architecture which can be summarized as follows: Server gets inputs from all the clients, with timestamps Server processes inputs and updates world status Server sends regular world snapshots to all clients Client sends input and simulates their effects locally Client get world updates and Syncs predicted state to authoritative state Interpolates known past states for other entities From a player’s point of view, this has two important consequences: Player sees himself in the present Player sees other entities in the past This situation is generally fine, but it’s quite problematic for very time- and space-sensitive events; for example, shooting your enemy in the head! Lag CompensationSo you’re aiming perfectly at the target’s head with your sniper rifle. You shoot - it’s a shot you can’t miss. But you miss. Why does this happen? Because of the client-server architecture explained before, you were aiming at where the enemy’s head was 100ms before you shot - not when you shot! In a way, it’s like playing in an universe where the speed of light is really, really slow; you’re aiming at the past position of your enemy, but he’s long gone by the time you squeeze the trigger. Fortunately, there’s a relatively simple solution for this, which is also pleasant for most players most of the time (with the one exception discussed below). Here’s how it works: When you shoot, client sends this event to the server with full information: the exact timestamp of your shot, and the exact aim of the weapon. Here’s the crucial step. Since the server gets all the input with timestamps, it can authoritatively reconstruct the world at any instant in the past. In particular, it can reconstruct the world exactly as it looked like to any client at any point in time. This means the server can know exactly what was on your weapon’s sights the instant you shot. It was the past position of your enemy’s head, but the server knows it was the position of his head in your present. The server processes the shot at that point in time, and updates the clients. And everyone is happy! The server is happy because he’s the server. He’s always happy. You’re happy because you were aiming at your enemy’s head, shot, and got a rewarding headshot! The enemy may be the only one not entirely happy. If he was standing still when he got shot, it’s his fault, right? If he was moving… wow, you’re a really awesome sniper. But what if he was in an open position, got behind a wall, and then got shot, a fraction of a second later, when he thought he was safe? Well, that can happen. That’s the tradeoff you make. Because you shoot at him in the past, he may still be shot up to a few milliseconds after he took cover. It is somewhat unfair, but it’s the most agreeable solution for everyone involved. It would be much worse to miss an unmissable shot! ConclusionThis ends my series on Fast-paced Multiplayer. This kind of thing is clearly tricky to get right, but with a clear conceptual understanding about what’s going on, it’s not exceedingly difficult. Although the audience of these articles were game developers, it found another group of interested readers: gamers! From a gamer point of view it’s also interesting to understand why some things happen the way they happen. Further ReadingAs clever as these techniques are, I can’t claim any credit for them; these articles are just an easy to understand guide to some concepts I’ve learned from other sources, including articles and source code, and some experimentation.]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏三之实体插值]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%B8%89%E4%B9%8B%E5%AE%9E%E4%BD%93%E6%8F%92%E5%80%BC%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part III): Entity Interpolation IntroductionIn the first article of the series, we introduced the concept of an authoritative server and its usefulness to prevent client cheats. However, using this technique naively can lead to potentially showstopper issues regarding playability and responsiveness. In the second article, we proposed client-side prediction as a way to overcome these limitations. The net result of these two articles is a set of concepts and techniques that allow a player to control an in-game character in a way that feels exactly like a single-player game, even when connected to an authoritative server through an internet connection with transmission delays. In this article, we’ll explore the consequences of having other player-controled characters connected to the same server. Server time stepIn the previous article, the behavior of the server we described was pretty simple – it read client inputs, updated the game state, and sent it back to the client. When more than one client is connected, though, the main server loop is somewhat different. In this scenario, several clients may be sending inputs simultaneously, and at a fast pace (as fast as the player can issue commands, be it pressing arrow keys, moving the mouse or clicking the screen). Updating the game world every time inputs are received from each client and then broadcasting the game state would consume too much CPU and bandwidth. A better approach is to queue the client inputs as they are received, without any processing. Instead, the game world is updated periodically at low frequency, for example 10 times per second. The delay between every update, 100ms in this case, is called the time step. In every update loop iteration, all the unprocessed client input is applied (possibly in smaller time increments than the time step, to make physics more predictable), and the new game state is broadcast to the clients. In summary, the game world updates independent of the presence and amount of client input, at a predictable rate. Dealing with low-frequency updatesFrom the point of view of a client, this approach works as smoothly as before – client-side prediction works independently of the update delay, so it clearly also works under predictable, if relatively infrequent, state updates. However, since the game state is broadcast at a low frequency (continuing with the example, every 100ms), the client has very sparse information about the other entities that may be moving throughout the world. A first implementation would update the position of other characters when it receives a state update; this immediately leads to very choppy movement, that is, discrete jumps every 100ms instead of smooth movement. Client 1 as seen by Client 2. Depending on the type of game you’re developing there are many ways to deal with this; in general, the more predictable your game entities are, the easier it is to get it right. Dead reckoningSuppose you’re making a car racing game. A car that goes really fast is pretty predictable – for example, if it’s running at 100 meters per second, a second later it will be roughly 100 meters ahead of where it started. Why “roughly”? During that second the car could have accelerated or decelerated a bit, or turned to the right or to the left a bit – the key word here is “a bit”. The maneuverability of a car is such that at high speeds its position at any point in time is highly dependent on its previous position, speed and direction, regardless of what the player actually does. In other words, a racing car can’t do a 180º turn instantly. How does this work with a server that sends updates every 100 ms? The client receives authoritative speed and heading for every competing car; for the next 100 ms it won’t receive any new information, but it still needs to show them running. The simplest thing to do is to assume the car’s heading and acceleration will remain constant during that 100 ms, and run the car physics locally with that parameters. Then, 100 ms later, when the server update arrives, the car’s position is corrected. The correction can be big or relatively small depending on a lot of factors. If the player does keep the car on a straight line and doesn’t change the car speed, the predicted position will be exactly like the corrected position. On the other hand, if the player crashes against something, the predicted position will be extremely wrong. Note that dead reckoning can be applied to low-speed situations – battleships, for example. In fact, the term “dead reckoning” has its origins in marine navigation. Entity interpolationThere are some situations where dead reckoning can’t be applied at all – in particular, all scenarios where the player’s direction and speed can change instantly. For example, in a 3D shooter, players usually run, stop, and turn corners at very high speeds, making dead reckoning essentially useless, as positions and speeds can no longer be predicted from previous data. You can’t just update player positions when the server sends authoritative data; you’d get players who teleport short distances every 100 ms, making the game unplayable. What you do have is authoritative position data every 100 ms; the trick is how to show the player what happens inbetween. The key to the solution is to show the other players in the past relative to the user’s player. Say you receive position data at t = 1000. You already had received data at t = 900, so you know where the player was at t = 900 and t = 1000. So, from t = 1000 and t = 1100, you show what the other player did from t = 900 to t = 1000. This way you’re always showing the user actual movement data, except you’re showing it 100 ms “late”. Client 2 renders Client 1 “in the past”, interpolating last known positions. The position data you use to interpolate from t = 900 to t = 1000 depends on the game. Interpolation usually works well enough. If it doesn’t, you can have the server send more detailed movement data with each update – for example, a sequence of straight segments followed by the player, or positions sampled every 10 ms which look better when interpolated (you don’t need to send 10 times more data – since you’re sending deltas for small movements, the format on the wire can be heavily optimized for this particular case). Note that using this technique, every player sees a slightly different rendering of the game world, because each player sees itself in the present but sees the other entities in the past. Even for a fast paced game, however, seeing other entities with a 100 ms isn’t generally noticeable. There are exceptions – when you need a lot of spatial and temporal accuracy, such as when the player shoots at another player. Since the other players are seen in the past, you’re aiming with a 100 ms delay – that is, you’re shooting where your target was 100 ms ago! We’ll deal with this in the next article. SummaryIn a client-server environment with an authoritative server, infrequent updates and network delay, you must still give players the illusion of continuity and smooth movement. In part 2 of the series we explored a way to show the user controlled player’s movement in real time using client-side prediction and server reconciliation; this ensures user input has an immediate effect on the local player, removing a delay that would render the game unplayable. Other entities are still a problem, however. In this article we explored two ways of dealing with them. The first one, dead reckoning, applies to certain kinds of simulations where entity position can be acceptably estimated from previous entity data such as position, speed and acceleration. This approach fails when these conditions aren’t met. The second one, entity interpolation, doesn’t predict future positions at all – it uses only real entity data provided by the server, thus showing the other entities slightly delayed in time. The net effect is that the user’s player is seen in the present and the other entities are seen in the past. This usually creates an incredibly seamless experience. However, if nothing else is done, the illusion breaks down when an event needs high spatial and temporal accuracy, such as shooting at a moving target: the position where Client 2 renders Client 1 doesn’t match the server’s nor Client 1′s position, so headshots become impossible! Since no game is complete without headshots, we’ll deal with this issue in the next article.]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏二之客户端预测与服务器修正]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%BA%8C%E4%B9%8B%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%AE%E6%AD%A3%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part II): Client-Side Prediction and Server Reconciliation IntroductionIn the first article of this series, we explored a client-server model with an authoritative server and dumb clients that just send inputs to the server and then render the updated game state when the server sends it. A naive implementation of this scheme leads to a delay between user commands and changes on the screen; for example, the player presses the right arrow key, and the character takes half a second before it starts moving. This is because the client input must first travel to the server, the server must process the input and calculate a new game state, and the updated game state must reach the client again. Effect of network delays. In a networked environment such as the internet, where delays can be in the orders of tenths of a second, a game may feel unresponsive at best, or in the worst case, be rendered unplayable. In this article, we’ll find ways to minimize or even eliminate that problem. Client-side predictionEven though there are some cheating players, most of the time the game server is processing valid requests (from non-cheating clients and from cheating clients who aren’t cheating at that particular time). This means most of the input received will be valid and will update the game state as expected; that is, if your character is at (10, 10) and the right arrow key is pressed, it will end up at (11, 10). We can use this to our advantage. If the game world is deterministic enough (that is, given a game state and a set of inputs, the result is completely predictable), Let’s suppose we have a 100 ms lag, and the animation of the character moving from one square to the next takes 100 ms. Using the naive implementation, the whole action would take 200 ms: Network delay + animation. Since the world is deterministic, we can assume the inputs we send to the server will be executed successfully. Under this assumption, the client can predict the state of the game world after the inputs are processed, and most of the time this will be correct. Instead of sending the inputs and waiting for the new game state to start rendering it, we can send the input and start rendering the outcome of that inputs as if they had succeded, while we wait for the server to send the “true” game state – which more often than not, will match the state calculated locally : Animation plays while the server confirms the action. Now there’s absolutely no delay between the player’s actions and the results on the screen, while the server is still authoritative (if a hacked client would send invalid inputs, it could render whatever it wanted on the screen, but it wouldn’t affect the state of the server, which is what the other players see). Synchronization issuesIn the example above, I chose the numbers carefully to make everything work fine. However, consider a slightly modified scenario: let’s say we have a 250 ms lag to the server, and moving from a square to the next takes 100 ms. Let’s also say the player presses the right key 2 times in a row, trying to move 2 squares to the right. Using the techniques so far, this is what would happen: Predicted state and authoritative state mismatch. We run into an interesting problem at t = 250 ms, when the new game state arrives. The predicted state at the client is x = 12, but the server says the new game state is x = 11. Because the server is authoritative, the client must move the character back to x = 11. But then, a new server state arrives at t = 350, which says x = 12, so the character jumps again, forward this time. From the point of view of the player, he pressed the right arrow key twice; the character moved two squares to the right, stood there for 50 ms, jumped one square to the left, stood there for 100 ms, and jumped one square to the right. This, of course, is unacceptable. Server reconciliationThe key to fix this problem is to realize that the client sees the game world in present time, but because of lag, the updates it gets from the server are actually the state of the game in the past. By the time the server sent the updated game state, it hadn’t processed all the commands sent by the client. This isn’t terribly difficult to work around, though. First, the client adds a sequence number to each request; in our example, the first key press is request #1, and the second key press is request #2. Then, when the server replies, it includes the sequence number of the last input it processed: Client-side prediction + server reconciliation. 或许这幅图更加解释得清楚些 Now, at t = 250, the server says “based on what I’ve seen up to your request #1, your position is x = 11”. Because the server is authoritative, it sets the character position at x = 11. Now let’s assume the client keeps a copy of the requests it sends to the server. Based on the new game state, it knows the server has already processed request #1, so it can discard that copy. But it also knows the server still has to send back the result of processing request #2. So applying client-side prediction again, the client can calculate the “present” state of the game based on the last authoritative state sent by the server, plus the inputs the server hasn’t processed yet. So, at t = 250, the client gets “x = 11, last processed request = #1”. It discards its copies of sent input up to #1 – but it retains a copy of #2, which hasn’t been acknowledged by the server. It updates it internal game state with what the server sent, x = 11, and then applies all the input still not seen by the server – in this case, input #2, “move to the right”. The end result is x = 12, which is correct. Continuing with our example, at t = 350 a new game state arrives from the server; this time it says “x = 12, last processed request = #2”. At this point, the client discards all input up to #2, and updates the state with x = 12. There’s no unprocessed input to replay, so processing ends there, with the correct result. Odds and endsThe example discussed above implies movement, but the same principle can be applied to almost anything else. For example, in a turn-based combat game, when the player attacks another character, you can show blood and a number representing the damage done, but you shouldn’t actually update the health of the character until the server says so. Because of the complexities of game state, which isn’t always easily reversible, you may want to avoid killing a character until the server says so, even if its health dropped below zero in the client’s game state (what if the other character used a first-aid kit just before receiving your deadly attack, but the server hasn’t told you yet?) This brings us to an interesting point – even if the world is completely deterministic and no clients cheat at all, it’s still possible that the state predicted by the client and the state sent by the server don’t match after a reconciliation. The scenario is impossible as described above with a single player, but it’s easy to run into when several players are connected to the server at once. This will be the topic of the next article. SummaryWhen using an authoritative server, you need to give the player the illusion of responsiveness, while you wait for the server to actually process your inputs. To do this, the client simulates the results of the inputs. When the updated server state arrives, the predicted client state is recomputed from the updated state and the inputs the client sent but the server hasn’t acknowledged yet.]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏一之C/S游戏架构]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%B8%80%E4%B9%8BC-S%E6%B8%B8%E6%88%8F%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part I): Client-Server Game Architecture IntroductionThis is the first in a series of articles exploring the techniques and algorithms that make fast-paced multiplayer games possible. If you’re familiar with the concepts behind multiplayer games, you can safely skip to the next article – what follows is an introductory discussion. Developing any kind of game is itself challenging; multiplayer games, however, add a completely new set of problems to be dealt with. Interestingly enough, the core problems are human nature and physics! The problem of cheatingIt all starts with cheating. As a game developer, you usually don’t care whether a player cheats in your single-player game – his actions don’t affect anyone but him. A cheating player may not experience the game exactly as you planned, but since it’s their game, they have the right to play it in any way they please. Multiplayer games are different, though. In any competitive game, a cheating player isn’t just making the experience better for himself, he’s also making the experience worse for the other players. As the developer, you probably want to avoid that, since it tends to drive players away from your game. There are many things that can be done to prevent cheating, but the most important one (and probably the only really meaningful one) is simple : don’t trust the player. Always assume the worst – that players will try to cheat. Authoritative servers and dumb clientsThis leads to a seemingly simple solution – you make everything in your game happen in a central server under your control, and make the clients just privileged spectators of the game. In other words, your game client sends inputs (key presses, commands) to the server, the server runs the game, and you send the results back to the clients. This is usually called using an authoritative server, because the one and only authority regarding everything that happens in the world is the server. Of course, your server can be exploited for vulnerabilities, but that’s out of the scope of this series of articles. Using an authoritative server does prevent a wide range of hacks, though. For example, you don’t trust the client with the health of the player; a hacked client can modify its local copy of that value and tell the player it has 10000% health, but the server knows it only has 10% – when the player is attacked it will die, regardless of what a hacked client may think. You also don’t trust the player with its position in the world. If you did, a hacked client would tell the server “I’m at (10,10)” and a second later “I’m at (20,10)”, possibly going through a wall or moving faster than the other players. Instead, the server knows the player is at (10,10), the client tells the server “I want to move one square to the right”, the server updates its internal state with the new player position at (11,10), and then replies to the player “You’re at (11, 10)”: Effect of network delays. In summary: the game state is managed by the server alone. Clients send their actions to the server. The server updates the game state periodically, and then sends the new game state back to clients, who just render it on the screen. Dealing with networksThe dumb client scheme works fine for slow turn based games, for example strategy games or poker. It would also work in a LAN setting, where communications are, for all practical purposes, instantaneous. But this breaks down when used for a fast-paced game over a network such as the internet. Let’s talk physics. Suppose you’re in San Francisco, connected to a server in the NY. That’s approximately 4,000 km, or 2,500 miles (that’s roughly the distance between Lisbon and Moscow). Nothing can travel faster than light, not even bytes on the Internet (which at the lower level are pulses of light, electrons in a cable, or electromagnetic waves). Light travels at approximately 300,000 km/s, so it takes 13 ms to travel 4,000 km. This may sound quite fast, but it’s actually a very optimistic setup – it assumes data travels at the speed of light in a straight path, with is most likely not the case. In real life, data goes through a series of jumps (called hops in networking terminology) from router to router, most of which aren’t done at lightspeed; routers themselve introduce a bit of delay, since packets must be copied, inspected, and rerouted. For the sake of the argument, let’s assume data takes 50 ms from client to server. This is close to a best-case scenario – what happens if you’re in NY connected to a server in Tokyo? What if there’s network congestion for some reason? Delays of 100, 200, even 500 ms are not unheard of. Back to our example, your client sends some input to the server (“I pressed the right arrow”). The server gets it 50 ms later. Let’s say the server processes the request and sends back the updated state immediately. Your client gets the new game state (“You’re now at (1, 0)”) 50 ms later. From your point of view, what happened is that you pressed the right arrow but nothing happened for a tenth of a second; then your character finally moved one square to the right. This perceived lag between your inputs and its consequences may not sound like much, but it’s noticeable – and of course, a lag of half a second isn’t just noticeable, it actually makes the game unplayable. SummaryNetworked multiplayer games are incredibly fun, but introduce a whole new class of challenges. The authoritative server architecture is pretty good at stopping most cheats, but a straightforward implementation may make games quite unresponsive to the player. In the following articles, we’ll explore how can we build a system based on an authoritative server, while minimizing the delay experienced by the players, to the point of making it almost indistinguishable from local or single player games.]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAT穿越基础]]></title>
    <url>%2F2019%2F05%2F21%2FNAT%E7%A9%BF%E8%B6%8A%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[NAT类型 锥NAT 对称NAT NAT作用 穿透锥NAT 网络拓扑结构 使用UDP穿透NAT 使用TCP穿透NAT 穿透对称NAT 同时开放TCP（ Simultaneous TCP open ）策略 UDP端口猜测策略 问题总结 参考 NAT类型注 : 我们本文主要讨论穿越锥NAT 锥NAT 全锥NAT ：全锥NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。任何一个外部主机均可通过该映射发送数据包到该内部主机。 限制性锥NAT ：限制性锥NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。但是, 和全锥NAT 不同的是：只有当内部主机先给外部主机发送数据包, 该外部主机才能向该内部主机发送数据包。 端口限制性锥NAT ：端口限制性锥NAT 与限制性锥NAT 类似, 只是多了端口号的限制, 即只有内部主机先向外部地址：端口号对发送数据包, 该外部主机才能使用特定的端口号向内部主机发送数据包。 对称NAT对称NAT 与上述3 种类型都不同, 不管是全锥NAT ，限制性锥NAT 还是端口限制性锥NAT ，它们都属于锥NAT （Cone NAT ）。当同一内部主机使用相同的端口与不同地址的外部主机进行通信时, 对称NAT 会重新建立一个Session ，为这个Session 分配不同的端口号，或许还会改变IP 地址。 NAT作用NAT 不仅实现地址转换，同时还起到防火墙的作用，隐藏内部网络的拓扑结构，保护内部主机。 NAT 不仅完美地解决了 lP 地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 这样对于外部主机来说，内部主机是不可见的。 但是，对于P2P 应用来说，却要求能够建立端到端的连接，所以如何穿透NAT 也是P2P 技术中的一个关键。 穿透锥NAT要让处于NAT 设备之后的拥有私有IP 地址的主机之间建立P2P 连接，就必须想办法穿透NAT ，现在常用的传输层协议主要有TCP 和UDP ，下面就是用这两种协议来介绍穿透NAT 的策略。 网络拓扑结构下面假设有如图1 所示网络拓扑结构图。 Server （129.208.12.38 ）是公网上的服务器，NAT-A 和NAT-B 是两个NAT 设备（可能是集成NAT 功能的路由器，防火墙等），它们具有若干个合法公网IP ，在NAT-A 阻隔的私有网络中有若干台主机【ClientA-1 ，ClientA-N 】，在NAT-B 阻隔的私有网络中也有若干台主机【ClientB-1 ，ClientB-N 】。 为了以后说明问题方便，只讨论主机ClientA-1 和ClientB-1 。 假设主机ClientA-1 和主机ClientB-1 都和服务器Server 建立了“连接”，如图2 所示。 由于NAT 的透明性，所以ClientA-1 和ClientB-1 不用关心和Server 通信的过程，它们只需要知道Server 开放服务的地址和端口号即可。 根据图1 ，假设在ClientA-1 中有进程使用socket （192.168.0.2 ：7000 ）和Server 通信，在ClientB-1 中有进程使用socket （192.168.1.12:8000 ）和Server 通信。 它们通过各自的NAT 转换后分别变成了socket （202.103.142.29 ：5000 ）和socket （221.10.145.84 ：6000 ）。 使用UDP穿透NAT通常情况下，当进程使用UDP 和外部主机通信时，NAT 会建立一个Session ，这个Session 能够保留多久并没有标准，或许几秒，几分钟，几个小时。 假设ClientA-1 在应用程序中看到了ClientB-1 在线，并且想和ClientB-1 通信，一种办法是Server 作为中间人，负责转发ClientA-1 和ClientB-1 之间的消息，但是这样服务器太累，会吃不消。 另一种方法就是让ClientA-1 何ClientB-1 建立端到端的连接，然后他们自己通信。 这也就是P2P 连接。 根据不同类型的NAT ，下面分别讲解。 全锥NAT ，穿透全锥型NAT 很容易，根本称不上穿透，因为全锥型NAT 将内部主机的映射到确定的地址，不会阻止从外部发送的连接请求，所以可以不用任何辅助手段就可以建立连接。 限制性锥NAT 和端口限制性锥NAT （简称限制性NAT ），穿透限制性锥NAT 会丢弃它未知的源地址发向内部主机的数据包。 所以如果现在ClientA-1 直接发送UDP 数据包到ClientB-1 ，那么数据包将会被NAT-B 无情的丢弃。 所以采用下面的方法来建立ClientA-1 和ClientB-1 之间的通信。 ClientA-1 （202.103.142.29:5000 ）发送数据包给Server ，请求和ClientB-1 （221.10.145.84:6000 ）通信。 Server 将ClientA-1 的地址和端口（202.103.142.29:5000 ）发送给ClientB-1 ，告诉ClientB-1 ，ClientA-1 想和它通信。 ClientB-1 向ClientA-1 （202.103.142.29:5000 ）发送UDP 数据包，当然这个包在到达NAT-A 的时候，还是会被丢弃，这并不是关键的，因为发送这个UDP 包只是为了让NAT-B 记住这次通信的目的地址：端口号，当下次以这个地址和端口为源的数据到达的时候就不会被NAT-B 丢弃，这样就在NAT-B 上打了一个从ClientB-1 到ClientA-1 的孔。 为了让ClientA-1 知道什么时候才可以向ClientB-1 发送数据，所以ClientB-1 在向ClientA-1 （202.103.142.29:5000 ）打孔之后还要向Server 发送一个消息，告诉Server 它已经准备好了。 Server 发送一个消息给ClientA-1 ，内容为：ClientB-1 已经准备好了，你可以向ClientB-1 发送消息了。 ClientA-1 向ClientB-1 发送UDP 数据包。 这个数据包不会被NAT-B 丢弃，以后ClientB-1 向ClientA-1 发送的数据包也不会被ClientA-1 丢弃，因为NAT-A 已经知道是ClientA-1 首先发起的通信。 至此，ClientA-1 和ClientB-1 就可以进行通信了。 使用TCP穿透NAT使用TCP 协议穿透NAT 的方式和使用UDP 协议穿透NAT 的方式几乎一样，没有什么本质上的区别，只是将无连接的UDP 变成了面向连接的TCP 。 值得注意是： ClientB-1 在向ClientA-1 打孔时，发送的SYN 数据包，而且同样会被NAT-A 丢弃。同时，ClientB-1 需要在原来的socket 上监听，由于重用socket ，所以需要将socket 属性设置为SO_REUSEADDR 。 ClientA-1 向ClientB-1 发送连接请求。同样，由于ClientB-1 到ClientA-1 方向的孔已经打好，所以连接会成功，经过3 次握手后，ClientA-1 到ClientB-1 之间的连接就建立起来了。 穿透对称NAT上面讨论的都是怎样穿透锥（Cone ）NAT ，对称NAT 和锥NAT 很不一样。 对于 对称NAT ，当一个私网内主机和外部多个不同主机通信时，对称NAT 并不会像锥（Cone ，全锥，限制性锥，端口限制性锥）NAT 那样分配同一个端口。 而是会新建立一个Session ，重新分配一个端口。 参考上面穿透限制性锥NAT 的过程，在步骤3 时：ClientB-1 （221.10.145.84: ？）向ClientA-1 打孔的时候，对称NAT 将给ClientB-1 重新分配一个端口号，而这个端口号对于Server 、ClientB-1 、ClientA-1 来说都是未知的。 同样， ClientA-1 根本不会收到这个消息，同时在步骤4 ，ClientB-1 发送给Server 的通知消息中，ClientB-1 的socket 依旧是（221.10.145.84:6000 ）。 而且，在步骤6 时：ClientA-1 向它所知道但错误的ClientB-1 发送数据包时，NAT-1 也会重新给ClientA-1 分配端口号。 所以，穿透对称NAT 的机会很小。 下面是两种有可能穿透对称NAT 的策略。 同时开放TCP（ Simultaneous TCP open ）策略如果一个 对称 NAT 接收到一个来自 本地 私有网 络 外面的 TCP SYN 包， 这 个包想 发 起一个 “ 引入” 的 TCP 连 接，一般来 说 ， NAT 会拒 绝这 个 连 接 请 求并扔掉 这 个 SYN 包，或者回送一个TCP RST （connection reset ，重建 连 接）包 给请 求方。 但是，有一 种 情况 却会接受这个“引入”连接。 RFC 规定：对于对称NAT ， 当 这 个接收到的 SYN 包中的源IP 地址 ： 端口、目 标 IP 地址 ： 端口都与NAT 登 记 的一个已 经 激活的 TCP 会 话 中的地址信息相符 时 ， NAT 将会放行 这 个 SYN 包。 需要 特 别 指出 的是：怎样才是一个已经激活的TCP 连接？除了真正已经建立完成的TCP 连接外，RFC 规范指出： 如果 NAT 恰好看到一个 刚刚发 送出去的一个 SYN 包和 随之 接收到的SYN 包中的地址 ：端口 信息相符合的 话 ，那 么 NAT 将会 认为这 个 TCP 连 接已 经 被激活，并将允 许这 个方向的 SYN 包 进 入 NAT 内部。 同时开放TCP 策略就是利用这个时机来建立连接的。 如果 Client A -1 和 Client B -1 能 够 彼此正确的 预 知 对 方的 NAT 将会 给 下一个 TCP 连 接分配的公网 TCP 端口，并且两个客 户 端能 够 同 时 地 发 起一 个面向对方的 “ 外出 ” 的 TCP 连 接 请求 ，并在 对 方的 SYN 包到达之前，自己 刚发 送出去的 SYN 包都能 顺 利的穿 过 自己的 NAT 的 话 ，一条端 对 端的 TCP 连 接就 能 成功地建立了 。 UDP端口猜测策略同时开放TCP 策略非常依赖于猜测对方的下一个端口，而且强烈依赖于发送连接请求的时机，而且还有网络的不确定性，所以能够建立的机会很小，即使Server 充当同步时钟的角色。 下面是一种通过UDP 穿透的方法，由于UDP 不需要建立连接，所以也就不需要考虑“同时开放”的问题。 为了介绍ClientB-1 的诡计，先介绍一下STUN 协议。 STUN （Simple Traversal of UDP Through NATs ）协议是一个轻量级协议，用来探测被NAT 映射后的地址：端口。 STUN 采用C/S 结构，需要探测自己被NAT 转换后的地址：端口的Client 向Server 发送请求，Server 返回Client 转换后的地址：端口。 参考4.2 节中穿透NAT 的步骤2 ，当ClientB-1 收到Server 发送给它的消息后，ClientB-1 即打开3 个socket 。 socket-0 向STUN Server 发送请求，收到回复后，假设得知它被转换后的地址：端口（ 221.10.145.84:600 5 ），socket-1 向ClientA-1 发送一个UDP 包，socket-2 再次向另一个STUN Server 发送请求，假设得到它被转换后的地址：端口（ 221.10.145.84:60 20 ）。 通常，对称NAT 分配端口有两种策略，一种是按顺序增加，一种是随机分配。 如果这里对称NAT 使用顺序增加策略，那么，ClientB-1 将两次收到的地址：端口发送给Server 后，Server 就可以通知ClientA-1 在这个端口范围内猜测刚才ClientB-1 发送给它的socket-1 中被NAT 映射后的地址：端口，ClientA-1 很有可能在孔有效期内成功猜测到端口号，从而和ClientB-1 成功通信。 问题总结从上面两种穿透对称NAT 的方法来看，都建立在了严格的假设条件下。 但是现实中多数的NAT 都是锥NAT ，因为资源毕竟很重要，反观对称NAT ，由于太不节约端口号所以相对来说成本较高。 所以，不管是穿透锥NAT ，还是对称NAT ，现实中都是可以办到的。 除非对称NAT 真的使用随机算法来分配可用的端口。 参考这篇博客]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议六之客户端与服务器的连接]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%9E%84%E5%BB%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%85%AD%E4%B9%8B%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[本篇自我总结这篇文章网上找不到翻译, 我也没时间详细翻译, 大概总结一下吧.本篇主要讲了把本系列之前五篇文章的技术应用到实战中处理客户端与服务器的的连接.请看总结, 不明之处再看文中具体讲解. 简单的连接协议First up we have the client state machine.The client is in one of three states: Disconnected Connecting Connected The goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to. When a client requests a connection, it gets assigned to one of these slots. If a client requests connection, but no slots are available, the server is full and the connection request is denied. On the server, we have the following data structure: 123456789const int MaxClients = 64;class Server&#123; int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients];&#125;; 这个简单协议存在的问题 易被攻击者利用我们的服务器当做DDos放大攻击的工具 攻击者可以很轻松的占满我们的client slots Traffic between the client and server can be read and modified in transit by a third party. 一旦被攻击者知道了客户端或者服务器的地址, 他就可以伪装服务器或客户端来欺骗对方获取利益 没有一个明确的断开连接的方式, 只能等time out 这些问题需要用授权系统和加密系统来解决. 如何改进这个连接协议 we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet. 为了防止攻击者利用我们的服务器当做DDos放大攻击的工具, 我们让客户端发的包比服务器发的包要大些. We’ll add some unique random identifiers, or ‘salts’, to make each client connection unique from previous ones coming from the same IP address and port. 一旦彼此连接上之后, 就用 client salt 和 server salt 的异或值来标识彼此. 以上的防御措施让我们的服务器做到了 no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, 但对于一个经验丰富的会抓包分析的攻击者来说, 还存在以下问题 : This attacker can read and modify packets in flight. This breaks the trivial identification based around salt values… giving an attacker the power to disconnect any client at will. To solve this, we need to get serious with cryptography to encrypt and sign packets so they can’t be read or modified by a third party. 原文原文出处 原文标题 : Client Server Connection (How to create a client/server connection over UDP) IntroductionHi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. So far in this article series we’ve discussed how games read and write packets, how to unify packet read and write into a single function, how to fragment and re-assemble packets, and how to send large blocks of data over UDP. Now in this article we’re going to bring everything together and build a client/server connection on top of UDP. BackgroundDevelopers from a web background often wonder why games go to such effort to build a client/server connection on top of UDP, when for many applications, TCP is good enough. These days even web servers are transitioning to UDP via Google’s QUIC. If you still think TCP is good enough for time critical data in 2016, I encourage you to put that in your pipe and smoke it :) The reason is that games send time critical data. Why don’t games use TCP for time critical data? The answer is that TCP delivers data reliably and in-order, and to do this on top of IP (which is unreliable, unordered) it holds more recent packets hostage in a queue while older packets are resent over the network. This is known as head of line blocking and it’s a huuuuuge problem for games. To understand why, consider a game server broadcasting the state of the world to clients 10 times per-second. Each client advances time forward and wants to display the most recent state it receives from the server. But if the packet containing state for time t = 10.0 is lost, under TCP we must wait for it to be resent before we can access t = 10.1 and 10.2, even though those packets have already arrived and contain the state the client wants to display. Worse still, by the time the resent packet arrives, it’s far too late for the client to actually do anything useful with it. The client has already advanced past 10.0 and wants to display something around 10.3 or 10.4! So why resend dropped packets at all? BINGO! What we’d really like is an option to tell TCP: “Hey, I don’t care about old packets being resent, by they time they arrive I can’t use them anyway, so just let me skip over them and access the most recent data”. Unfortunately, TCP simply does not give us this option :( All data must be delivered reliably and in-order. This creates terrible problems for time critical data where packet loss and latency exist. Situations like, you know, The Internet, where people play FPS games. Large hitches corresponding to multiples of round trip time are added to the stream of data as TCP waits for dropped packets to be resent, which means additional buffering to smooth out these hitches, or long pauses where the game freezes and is non-responsive. Neither option is acceptable for first person shooters, which is why virtually all first person shooters are networked using UDP. UDP doesn’t provide any reliability or ordering, so protocols built on top it can access the most recent data without waiting for lost packets to be resent, implementing whatever reliability they need in radically different ways to TCP. But, using UDP comes at a cost: UDP doesn’t provide any concept of connection. We have to build that ourselves. This is a lot of work! So strap in, get ready, because we’re going to build it all up from scratch using the same basic techniques first person shooters use when creating their protocols over UDP. You can use this client/server protocol for games or non-gaming applications and, provided the data you send is time critical, I promise you, it’s well worth the effort. Client/Server AbstractionThe goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to: When a client requests a connection, it gets assigned to one of these slots: If a client requests connection, but no slots are available, the server is full and the connection request is denied: Once a client is connected, packets are exchanged in both directions. These packets form the basis for the custom protocol between the client and server which is game specific. In a first person shooter, packets are sent continuously in both directions. Clients send input to the server as quickly as possible, often 30 or 60 times per-second, and the server broadcasts the state of the world to clients 10, 20 or even 60 times per-second. Because of this steady flow of packets in both directions there is no need for keep-alive packets. If at any point packets stop being received from the other side, the connection simply times out. No packets for 5 seconds is a good timeout value in my opinion, but you can be more aggressive if you want. When a client slot times out on the server, it becomes available for other clients to connect. When the client times out, it transitions to an error state. Simple Connection ProtocolLet’s get started with the implementation of a simple protocol. It’s a bit basic and more than a bit naive, but it’s a good starting point and we’ll build on it during the rest of this article, and the next few articles in this series. First up we have the client state machine. The client is in one of three states: Disconnected Connecting Connected Initially the client starts in disconnected. When a client connects to a server, it transitions to the connecting state and sends connection request packets to the server: The CRC32 and implicit protocol id in the packet header allow the server to trivially reject UDP packets not belonging to this protocol or from a different version of it. Since connection request packets are sent over UDP, they may be lost, received out of order or in duplicate. Because of this we do two things: 1) we keep sending packets for the client state until we get a response from the server or the client times out, and 2) on both client and server we ignore any packets that don’t correspond to what we are expecting, since a lot of redundant packets are flying over the network. On the server, we have the following data structure: 123456789const int MaxClients = 64; class Server &#123; int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients]; &#125;; Which lets the server lookup a free slot for a client to join (if any are free): 123456789int Server::FindFreeClientIndex() const &#123; for ( int i = 0; i &lt; m_maxClients; ++i ) &#123; if ( !m_clientConnected[i] ) return i; &#125; return -1; &#125; Find the client index corresponding to an IP address and port: 123456789int Server::FindExistingClientIndex( const Address &amp; address ) const &#123; for ( int i = 0; i &lt; m_maxClients; ++i ) &#123; if ( m_clientConnected[i] &amp;&amp; m_clientAddress[i] == address ) return i; &#125; return -1; &#125; Check if a client is connected to a given slot: 1234bool Server::IsClientConnected( int clientIndex ) const &#123; return m_clientConnected[clientIndex]; &#125; … and retrieve a client’s IP address and port by client index: 1234const Address &amp; Server::GetClientAddress( int clientIndex ) const &#123; return m_clientAddress[clientIndex]; &#125; Using these queries we implement the following logic when the server processes a connection request packet: If the server is full, reply with connection denied. If the connection request is from a new client and we have a slot free, assign the client to a free slot and respond with connection accepted. If the sender corresponds to the address of a client that is already connected, also reply with connection accepted. This is necessary because the first response packet may not have gotten through due to packet loss. If we don’t resend this response, the client gets stuck in the connecting state until it times out. The connection accepted packet tells the client which client index it was assigned, which the client needs to know which player it is in the game: Once the server sends a connection accepted packet, from its point of view it considers that client connected. As the server ticks forward, it watches connected client slots, and if no packets have been received from a client for 5 seconds, the slot times out and is reset, ready for another client to connect. Back to the client. While the client is in the connecting state the client listens for connection denied and connection accepted packets from the server. Any other packets are ignored. If the client receives connection accepted, it transitions to connected. If it receives connection denied, or after 5 seconds hasn’t received any response from the server, it transitions to disconnected. Once the client hits connected it starts sending connection payload packets to the server. If no packets are received from the server in 5 seconds, the client times out and transitions to disconnected. Naive Protocol is NaiveWhile this protocol is easy to implement, we can’t use a protocol like this in production. It’s way too naive. It simply has too many weaknesses to be taken seriously: Spoofed packet source addresses can be used to redirect connection accepted responses to a target (victim) address. If the connection accepted packet is larger than the connection request packet, attackers can use this protocol as part of a DDoS amplification attack. Spoofed packet source addresses can be used to trivially fill all client slots on a server by sending connection request packets from n different IP addresses, where n is the number of clients allowed per-server. This is a real problem for dedicated servers. Obviously you want to make sure that only real clients are filling slots on servers you are paying for. An attacker can trivially fill all slots on a server by varying the client UDP port number on each client connection. This is because clients are considered unique on an address + port basis. This isn’t easy to fix because due to NAT (network address translation), different players behind the same router collapse to the same IP address with only the port being different, so we can’t just consider clients to be unique at the IP address level sans port. Traffic between the client and server can be read and modified in transit by a third party. While the CRC32 protects against packet corruption, an attacker would simply recalculate the CRC32 to match the modified packet. If an attacker knows the client and server IP addresses and ports, they can impersonate the client or server. This gives an attacker the power to completely a hijack a client’s connection and perform actions on their behalf. Once a client is connected to a server there is no way for them to disconnect cleanly, they can only time out. This creates a delay before the server realizes a client has disconnected, or before a client realizes the server has shut down. It would be nice if both the client and server could indicate a clean disconnect, so the other side doesn’t need to wait for timeout in the common case. Clean disconnection is usually implemented with a disconnect packet, however because an attacker can impersonate the client and server with spoofed packets, doing so would give the attacker the ability to disconnect a client from the server whenever they like, provided they know the client and server IP addresses and the structure of the disconnect packet. If a client disconnects dirty and attempts to reconnect before their slot times out on the server, the server still thinks that client is connected and replies with connection accepted to handle packet loss. The client processes this response and thinks it’s connected to the server, but it’s actually in an undefined state. While some of these problems require authentication and encryption before they can be fully solved, we can make some small steps forward to improve the protocol before we get to that. These changes are instructive. Improving The Connection ProtocolThe first thing we want to do is only allow clients to connect if they can prove they are actually at the IP address and port they say they are. To do this, we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet. The sequence of operations in a typical connect now looks like this: To implement this we need an additional data structure on the server. Somewhere to store the challenge data for pending connections, so when a challenge response comes in from a client we can check against the corresponding entry in the data structure and make sure it’s a valid response to the challenge sent to that address. While the pending connect data structure can be made larger than the maximum number of connected clients, it’s still ultimately finite and is therefore subject to attack. We’ll cover some defenses against this in the next article. But for the moment, be happy at least that attackers can’t progress to the connected state with spoofed packet source addresses. Next, to guard against our protocol being used in a DDoS amplification attack, we’ll inflate client to server packets so they’re large relative to the response packet sent from the server. This means we add padding to both connection request and challenge response packets and enforce this padding on the server, ignoring any packets without it. Now our protocol effectively has DDoS minification for requests -&gt; responses, making it highly unattractive for anyone thinking of launching this kind of attack. Finally, we’ll do one last small thing to improve the robustness and security of the protocol. It’s not perfect, we need authentication and encryption for that, but it at least it ups the ante, requiring attackers to actually sniff traffic in order to impersonate the client or server. We’ll add some unique random identifiers, or ‘salts’, to make each client connection unique from previous ones coming from the same IP address and port. The connection request packet now looks like this: The client salt in the packet is a random 64 bit integer rolled each time the client starts a new connect. Connection requests are now uniquely identified by the IP address and port combined with this client salt value. This distinguishes packets from the current connection from any packets belonging to a previous connection, which makes connection and reconnection to the server much more robust. Now when a connection request arrives and a pending connection entry can’t be found in the data structure (according to IP, port and client salt) the server rolls a server salt and stores it with the rest of the data for the pending connection before sending a challange packet back to the client. If a pending connection is found, the salt value stored in the data structure is used for the challenge. This way there is always a consistent pair of client and server salt values corresponding to each client session. The client state machine has been expanded so connecting is replaced with two new states: _sending connection request_and sending challenge response, but it’s the same idea as before. Client states repeatedly send the packet corresponding to that state to the server while listening for the response that moves it forward to the next state, or back to an error state. If no response is received, the client times out and transitions to disconnected. The challenge response sent from the client to the server looks like this: The utility of this being that once the client and server have established connection, we prefix all payload packets with the xor of the client and server salt values and discard any packets with the incorrect salt values. This neatly filters out packets from previous sessions and requires an attacker to sniff packets in order to impersonate a client or server. Now that we have at least a basic level of security, it’s not much, but at least it’s something, we can implement a disconnect packet: And when the client or server want to disconnect clean, they simply fire 10 of these over the network to the other side, in the hope that some of them get through, and the other side disconnects cleanly instead of waiting for timeout. ConclusionWe now have a much more robust protocol. It’s secure against spoofed IP packet headers. It’s no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, we are protected against _casual_attackers while client reconnects are much more robust. But it’s still vulnerable to a sophisticated actors who can sniff packets: This attacker can read and modify packets in flight. This breaks the trivial identification based around salt values… … giving an attacker the power to disconnect any client at will. To solve this, we need to get serious with cryptography to encrypt and sign packets so they can’t be read or modified by a third party.]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议五之可靠的有序消息]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%9E%84%E5%BB%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%BA%94%E4%B9%8B%E5%8F%AF%E9%9D%A0%E7%9A%84%E6%9C%89%E5%BA%8F%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[本篇自我总结本篇主要讲了数据包的分包和重组问题, 到底数据包多大才好呢?是不是越大越好呢?包太大了怎么办呢?请看总结, 不明之处再看文中具体讲解. 为什么需要做这个可靠UDP协议网络协议在动作游戏类型（FPS）中的典型特征就是一个持续发送的数据包，在两个方向上以稳定的速度如20或30包每秒发送。这些数据包都包含有不可靠的无序数据例如t时间内的世界状态；所以，当一个数据包丢失，重新发送它并不是特别有用。当重新发送的数据包到达时，时间t已经过去了。 所以这就是我们将要实现可靠性的现状。对于我们90%的数据包，仅丢弃并不再重新发送它会更好。对于10%或更少（误差允许范围内）的情况，我们确实需要可靠性，但这样的数据是非常罕见的，很少被发送而且比不可靠的数据的平均大小要小得多。这个使用案例适用于所有过去十五年来发布的AAA级的FPS游戏。 应答系统是实现可靠UDP的最重要的部分为实现数据包层级的应答，在每个包的前面添加如下的报头： 123456struct Header&#123; uint16_t sequence; uint16_t ack; uint32_t ack_bits;&#125;; 这些报头元素组合起来以创建应答系统： sequence 是一个数字，随每个数据包发送而增长（并且在达到65535后回往复）。 ack 是从另一方接收到的最新的数据包序列号。 ack_bits 是一个位字段，它编码与ack相关的收到的数据包组合：如果位n已经设置，即 ack– n 数据包被接收了。 ack_bits 不仅是一个节省带宽的巧妙的编码，它同样也增加了信息冗余来抵御包的丢失。每个应答码要被发送32次。如果有一个包丢失了，仍然有其他31个包有着相同的应答码。从统计上来说，应答码还是非常有可能送达的。 但突发的传送数据包的丢失还是有可能发生的，所以重要的是要注意： 如果你收到一个数据包n的应答码，那么这个包肯定已经收到了。 如果你没有收到应答码，那么这个包就很有可能 没有被收到。但是…它也许会是，仅是应答码没有送达。这种情况是极其罕见的。 以我的经验，没有必要设计完善的应答机制。在一个极少丢应答码的系统上构建一个可靠性系统并不会增加什么大问题。 发送方如何追踪数据包是否已经被应答为实现这个应答系统，我们在发送方还需要一个数据结构来追踪一个数据包是否已经被应答，这样我们就可以忽略冗余的应答（每个包会通过 ack_bits多次应答)。我们同样在接收方也还需要一个数据结构来追踪那些已经收到的包，这样我们就可以在数据包的报头填写ack_bits的值。 12345678910111213141516171819const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData&#123; bool acked;&#125;; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;&#125; 你在这可以看到的窍门是这个滚动的缓冲区是以序列号来作为索引的： 1const int index = sequence % BufferSize; 当条目被顺序添加，就像一个被发送的队列，对插入所需要做的就是把这个序列缓冲区的值更新为新的序列号并且在该索引处重写这个数据： 123456PacketData &amp; InsertPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];&#125; 原文原文出处 原文标题 : Reliable Ordered Messages (How to implement reliable-ordered messages on top of UDP) IntroductionHi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. Many people will tell you that implementing your own reliable message system on top of UDP is foolish. After all, why reimplement TCP? But why limit ourselves to how TCP works? But there are so many different ways to implement reliable-messages and most of them work nothing like TCP! So let’s get creative and work out how we can implement a reliable message system that’s better and _more flexible_than TCP for real-time games. Different ApproachesA common approach to reliability in games is to have two packet types: reliable-ordered and unreliable. You’ll see this approach in many network libraries. The basic idea is that the library resends reliable packets until they are received by the other side. This is the option that usually ends up looking a bit like TCP-lite for the reliable-packets. It’s not that bad, but you can do much better. The way I prefer to think of it is that messages are smaller bitpacked elements that know how to serialize themselves. This makes the most sense when the overhead of length prefixing and padding bitpacked messages up to the next byte is undesirable (eg. lots of small messages included in each packet). Sent messages are placed in a queue and each time a packet is sent some of the messages in the send queue are included in the outgoing packet. This way there are no reliable packets that need to be resent. Reliable messages are simply included in outgoing packets until they are received. The easiest way to do this is to include all unacked messages in each packet sent. It goes something like this: each message sent has an id that increments each time a message is sent. Each outgoing packet includes the start message id followed by the data for _n_ messages. The receiver continually sends back the most recent received message id to the sender as an ack and only messages newer than the most recent acked message id are included in packets. This is simple and easy to implement but if a large burst of packet loss occurs while you are sending messages you get a spike in packet size due to unacked messages. You can avoid this by extending the system to have an upper bound on the number of messages included per-packet _n_. But now if you have a high packet send rate (like 60 packets per-second) you are sending the same message multiple times until you get an ack for that message. If your round trip time is 100ms each message will be sent 6 times redundantly before being acked on average. Maybe you really need this amount of redundancy because your messages are extremely time critical, but in most cases, your bandwidth would be better spent on other things. The approach I prefer combines packet level acks with a prioritization system that picks the n most important messages to include in each packet. This combines time critical delivery and the ability to send only n messages per-packet, while distributing sends across all messages in the send queue. Packet Level AcksTo implement packet level acks, we add the following packet header: 123456struct Header&#123; uint16_t sequence; uint16_t ack; uint32_t ack_bits;&#125;; These header elements combine to create the ack system: sequence is a number that increases with each packet sent, ack is the most recent packet sequence number received, and ack_bits is a bitfield encoding the set of acked packets. If bit n is set in ack_bits, then ack - n is acked. Not only is ack_bits a smart encoding that saves bandwidth, it also adds redundancy to combat packet loss. Each ack is sent 32 times. If one packet is lost, there’s 31 other packets with the same ack. Statistically speaking, acks are very likely to get through. But bursts of packet loss do happen, so it’s important to note that: If you receive an ack for packet n then that packet was definitely received. If you don’t receive an ack, the packet was most likely not received. But, it might have been, and the ack just didn’t get through. This is extremely rare. In my experience it’s not necessary to send perfect acks. Building a reliability system on top of a system that very rarely drops acks adds no significant problems. But it does create a challenge for testing this system works under all situations because of the edge cases when acks are dropped. So please if you do implement this system yourself, setup a soak test with terrible network conditions to make sure your ack system is working correctly. You’ll find such a soak test in the example source code for this article, and the open source network libraries reliable.io and yojimbo which also implement this technique. Sequence BuffersTo implement this ack system we need a data structure on the sender side to track whether a packet has been acked so we can ignore redundant acks (each packet is acked multiple times via ack_bits. We also need a data structure on the receiver side to keep track of which packets have been received so we can fill in the ack_bits value in the packet header. The data structure should have the following properties: Constant time insertion (inserts may be random, for example out of order packets…) Constant time query if an entry exists given a packet sequence number Constant time access for the data stored for a given packet sequence number Constant time removal of entries You might be thinking. Oh of course, hash table. But there’s a much simpler way: 12345678910111213141516171819const int BufferSize = 1024;uint32_t sequence_buffer[BufferSize];struct PacketData&#123; bool acked;&#125;;PacketData packet_data[BufferSize];PacketData GetPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;&#125; As you can see the trick here is a rolling buffer indexed by sequence number: const int index = sequence % BufferSize; This works because we don’t care about being destructive to old entries. As the sequence number increases older entries are naturally overwritten as we insert new ones. The sequence_buffer[index] value is used to test if the entry at that index actually corresponds to the sequence number you’re looking for. A sequence buffer value of 0xFFFFFFFF indicates an empty entry and naturally returns NULL for any sequence number query without an extra branch. When entries are added in order like a send queue, all that needs to be done on insert is to update the sequence buffer value to the new sequence number and overwrite the data at that index: 123456PacketData &amp; InsertPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];&#125; Unfortunately, on the receive side packets arrive out of order and some are lost. Under ridiculously high packet loss (99%) I’ve seen old sequence buffer entries stick around from before the previous sequence number wrap at 65535 and break my ack logic (leading to false acks and broken reliability where the sender thinks the other side has received something they haven’t…). The solution to this problem is to walk between the previous highest insert sequence and the new insert sequence (if it is more recent) and clear those entries in the sequence buffer to 0xFFFFFFFF. Now in the common case, insert is very close to constant time, but worst case is linear where n is the number of sequence entries between the previous highest insert sequence and the current insert sequence. Before we move on I would like to note that you can do much more with this data structure than just acks. For example, you could extend the per-packet data to include time sent: 12345struct PacketData&#123; bool acked; double send_time;&#125;; With this information you can create your own estimate of round trip time by comparing send time to current time when packets are acked and taking an exponentially smoothed moving average. You can even look at packets in the sent packet sequence buffer older than your RTT estimate (you should have received an ack for them by now…) to create your own packet loss estimate. Ack AlgorithmNow that we have the data structures and packet header, here is the algorithm for implementing packet level acks: On packet send: Insert an entry for for the current send packet sequence number in the sent packet sequence buffer with data indicating that it hasn’t been acked yet Generate ack and ack_bits from the contents of the local received packet sequence buffer and the most recent received packet sequence number Fill the packet header with sequence, ack and ack_bits Send the packet and increment the send packet sequence number On packet receive: Read in sequence from the packet header If sequence is more recent than the previous most recent received packet sequence number, update the most recent received packet sequence number Insert an entry for this packet in the received packet sequence buffer Decode the set of acked packet sequence numbers from ack and ack_bits in the packet header. Iterate across all acked packet sequence numbers and for any packet that is not already acked call OnPacketAcked( uint16_t sequence ) and mark that packet as acked in the sent packet sequence buffer. Importantly this algorithm is done on both sides so if you have a client and a server then each side of the connection runs the same logic, maintaining its own sequence number for sent packets, tracking most recent received packet sequence # from the other side and a sequence buffer of received packets from which it generates sequence, ack and ack_bits to send to the other side. And that’s really all there is to it. Now you have a callback when a packet is received by the other side: OnPacketAcked. The main benefit of this ack system is now that you know which packets were received, you can build any reliability system you want on top. It’s not limited to just reliable-ordered messages. For example, you could use it to implement delta encoding on a per-object basis. Message ObjectsMessages are small objects (smaller than packet size, so that many will fit in a typical packet) that know how to serialize themselves. In my system they perform serialization using a unified serialize function unified serialize function. The serialize function is templated so you write it once and it handles read, write and measure. Yes. Measure. One of my favorite tricks is to have a dummy stream class called MeasureStream that doesn’t do any actual serialization but just measures the number of bits that would be written if you called the serialize function. This is particularly useful for working out which messages are going to fit into your packet, especially when messages themselves can have arbitrarily complex serialize functions. 12345678910111213141516171819202122232425262728293031323334struct TestMessage : public Message&#123; uint32_t a,b,c; TestMessage() &#123; a = 0; b = 0; c = 0; &#125; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; &#125; virtual SerializeInternal( WriteStream &amp; stream ) &#123; return Serialize( stream ); &#125; virtual SerializeInternal( ReadStream &amp; stream ) &#123; return Serialize( stream ); &#125; virtual SerializeInternal( MeasureStream &amp; stream ) &#123; return Serialize( stream ); &#125;&#125;; The trick here is to bridge the unified templated serialize function (so you only have to write it once) to virtual serialize methods by calling into it from virtual functions per-stream type. I usually wrap this boilerplate with a macro, but it’s expanded in the code above so you can see what’s going on. Now when you have a base message pointer you can do this and it just works: 12Message message = CreateSomeMessage();message-&gt;SerializeInternal( stream ); An alternative if you know the full set of messages at compile time is to implement a big switch statement on message type casting to the correct message type before calling into the serialize function for each type. I’ve done this in the past on console platform implementations of this message system (eg. PS3 SPUs) but for applications today (2016) the overhead of virtual functions is neglible. Messages derive from a base class that provides a common interface such as serialization, querying the type of a message and reference counting. Reference counting is necessary because messages are passed around by pointer and stored not only in the message send queue until acked, but also in outgoing packets which are themselves C++ structs. This is a strategy to avoid copying data by passing both messages and packets around by pointer. Somewhere else (ideally on a separate thread) packets and the messages inside them are serialized to a buffer. Eventually, when no references to a message exist in the message send queue (the message is acked) and no packets including that message remain in the packet send queue, the message is destroyed. We also need a way to create messages. I do this with a message factory class with a virtual function overriden to create a message by type. It’s good if the packet factory also knows the total number of message types, so we can serialize a message type over the network with tight bounds and discard malicious packets with message type values outside of the valid range: 1234567891011121314151617181920212223242526272829enum TestMessageTypes&#123; TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES&#125;;// message definitions omittedclass TestMessageFactory : public MessageFactory&#123;public: Message Create( int type ) &#123; switch ( type ) &#123; case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); &#125; &#125; virtual int GetNumTypes() const &#123; return TEST_MESSAGE_NUM_TYPES; &#125;&#125;; Again, this is boilerplate and is usually wrapped by macros, but underneath this is what’s going on. Reliable Ordered Message AlgorithmThe algorithm for sending reliable-ordered messages is as follows: On message send: Measure how many bits the message serializes to using the measure stream Insert the message pointer and the # of bits it serializes to into a sequence buffer indexed by message id. Set the time that message has last been sent to -1 Increment the send message id On packet send: Walk across the set of messages in the send message sequence buffer between the oldest unacked message id and the most recent inserted message id from left -&gt; right (increasing message id order). Never send a message id that the receiver can’t buffer or you’ll break message acks (since that message won’t be buffered, but the packet containing it will be acked, the sender thinks the message has been received, and will not resend it). This means you must never send a message id equal to or more recent than the oldest unacked message id plus the size of the message receive buffer. For any message that hasn’t been sent in the last 0.1 seconds and fits in the available space we have left in the packet, add it to the list of messages to send. Messages on the left (older messages) naturally have priority due to the iteration order. Include the messages in the outgoing packet and add a reference to each message. Make sure the packet destructor decrements the ref count for each message. Store the number of messages in the packet n and the array of message ids included in the packet in a sequence buffer indexed by the outgoing packet sequence number so they can be used to map packet level acks to the set of messages included in that packet. Add the packet to the packet send queue. On packet receive: Walk across the set of messages included in the packet and insert them in the receive message sequence buffer indexed by their message id. The ack system automatically acks the packet sequence number we just received. On packet ack: Look up the set of messages ids included in the packet by sequence number. Remove those messages from the message send queue if they exist and decrease their ref count. Update the last unacked message id by walking forward from the previous unacked message id in the send message sequence buffer until a valid message entry is found, or you reach the current send message id. Whichever comes first. On message receive: Check the receive message sequence buffer to see if a message exists for the current receive message id. If the message exists, remove it from the receive message sequence buffer, increment the receive message id and return a pointer to the message. Otherwise, no message is available to receive. Return NULL. In short, messages keep getting included in packets until a packet containing that message is acked. We use a data structure on the sender side to map packet sequence numbers to the set of message ids to ack. Messages are removed from the send queue when they are acked. On the receive side, messages arriving out of order are stored in a sequence buffer indexed by message id, which lets us receive them in the order they were sent. The End ResultThis provides the user with an interface that looks something like this on send: 12345678TestMessage message = (TestMessage) factory.Create( TEST_MESSAGE );if ( message )&#123; message-&gt;a = 1; message-&gt;b = 2; message-&gt;c = 3; connection.SendMessage( message );&#125; And on the receive side: 1234567891011121314while ( true )&#123; Message message = connection.ReceiveMessage(); if ( !message ) break; if ( message-&gt;GetType() == TEST_MESSAGE ) &#123; TestMessage testMessage = (TestMessage) message; // process test message &#125; factory.Release( message );&#125; Which is flexible enough to implement whatever you like on top of it. 译文译文出处 译者：翁僖骏（∈星际长途←） 审校：侯鹏（叶落&amp;无痕） 嗨，我是格伦费德勒，欢迎来到创建一个游戏网络协议第五篇文章。 从上一篇文章到现在已经有很长一段时间了，上次我已经率先而且实现了余下的这一系列文章所需的源码并创建了开源库libyojimbo，是本系列文章所要描述的网络协议的一个质量有保证的的和经过单元测试的版本。 如果你想要有一个开源库来为自己在UDP之上实现可靠消息或是为了其他更多，看看libyojimbo。但是，如果你像我这样是想理解它具体是怎么工作的并且可能自己去实现它，阅读下去，因为我们将要从头到脚地去建立一个在UDP之上用来发送可靠有序消息的完整的系统！ 说明很多人也许会跟你说，要在UDP之上实现你自己的可靠消息系统是愚蠢的。为什么要撰写你特有的简化版本的TCP？这些人深信，任何可靠性的实现不可避免地 最终会成为一个（简化的）TCP的重实现。 但也有很多不同的方法来在UDP之上实现可靠消息，各有不同的优势和劣势。TCP的方法并不是唯一的选择。事实上，我所了解到的大多数可靠有序信息的选择的原理和TCP并不相同。所以让我们为我们的目标发挥创造力并弄懂我们该如何充分利用我们的现状来实现一个比TCP_更好_ 的可靠性系统。 网络协议在动作游戏类型（FPS）中的典型特征就是一个持续发送的数据包，在两个方向上以稳定的速度如20或30包每秒发送。这些数据包都包含有不可靠的无序数据例如t时间内的世界状态；所以，当一个数据包丢失，重新发送它并不是特别有用。当重新发送的数据包到达时，时间t已经过去了。 所以这就是我们将要实现可靠性的现状。对于我们90%的数据包，仅丢弃并不再重新发送它会更好。对于10%或更少（误差允许范围内）的情况，我们确实需要可靠性，但这样的数据是非常罕见的，很少被发送而且比不可靠的数据的平均大小要小得多。这个使用案例适用于所有过去十五年来发布的AAA级的FPS游戏。 不同的方法可靠性的一个常用的方法是使用两种包类型：可靠有序的和不可靠的。你在众多网络库中都会看到这个方法。它基本的想法是，这个库不断重新发送可靠的数据包直到它的另一方接收到为止。这是一个最终看起来会有一点像TCP方式传输的可靠包的选择。这并没有很糟糕，但你也可以做得更好。 我更愿意去考虑的方法就是消息其实是更小的位包装元素，它知道如何使它们自己序列化。这就显得非常有意义了，因为按位打包的消息中，用于描述下个字节的前缀或者后缀的字节开销在大部分的情况下是不必需的（例如每个包中包含的许多小的消息）。被发送的消息会被放在一个队列并且每次一个包被发送时，发送队列中的一些消息就会被包含在外发的包中。这样一来，就没有可靠的数据包需要被重新发送了。可靠消息也只会包含在数据包里直到它们被接收。 要做到这样最简单的方法就是，把所有未应答的消息包含到每个被发送的包中。它是这样的：每个被发送的消息都有一个随每当一个消息被发送时递增的id。每个输出数据包包含起始消息id ，紧跟着的是n 个消息的数据。接收方不断发回最新收到消息的id给发送方作为一个应答信号，并且消息要当且仅当比最新的应答消息id要更新，才会被包含在数据包中。 这很简单也易于实现，但当你正在发送消息时如果突发一个很大的包丢失情况，你会遇到一个数据包大小的峰值，因为有很多未应答的消息。。正如在数据包分割和重组中讨论的需要按照MTU分割包的方式来发送大的数据包会增加丢包的情况。在高丢包率下你最不想做的就是增大包的规格并引起更多的包的丢失。这是一个潜在的无底洞。 你可以通过扩展系统来给每个包的消息数量n设置一个上限，来避免这种情况。但现在如果你有一个高数据包发送率（如每秒60包）你就要多次发送同样的消息直到你得到该消息的应答信号。如果的往返时间是100ms，每条消息在被应答之前将要平均被多余发送六次。也许你真的需要这些多余的发送数量因为你的消息是对时间极其敏感的，但在大多数情况下，你应该给队列里的其他消息分配合理的带宽。 我比较喜欢的方法是用一个优先次序系统整合每包的应答信号，这个系统检出n条最重要的消息并包含在每个包中。在散布的消息穿过所有在发送队列中的消息发送时，这样就把对时间敏感的递送与每包仅发送n条消息的能力联合起来了。 数据包层级应答让我们行动起来实现它。 这种可靠性系统的基础是每个包的应答。 但为什么应答是在数据包层级而不是在消息层级呢？简要截说原因就是包的数量会远远少于消息的数量。假设每个包中有32或64条消息，显然让一个包含32或64条消息的包来应答会比让每个消息都分别应答要高效得多。 这样同样也增加了灵活性，因为你可以在数据包层级应答上构建其他可靠性系统，不仅仅是为了可靠有序的消息。例如，使用了数据包层级应答，你就知道哪一个时间先决的不可靠状态更新已结束，所以你可以轻易地构建一个系统，在一旦一个数据包所包含的最后一个状态更新已经应答时，停止发送不再改变的对象状态。 为实现数据包层级的应答，在每个包的前面添加如下的报头： 123456struct Header&#123; uint16_t sequence; uint16_t ack; uint32_t ack_bits;&#125;; 这些报头元素组合起来以创建应答系统： sequence 是一个数字，随每个数据包发送而增长（并且在达到65535后回往复）。 ack 是从另一方接收到的最新的数据包序列号。 ack_bits 是一个位字段，它编码与ack相关的收到的数据包组合：如果位n已经设置，即 ack– n 数据包被接收了。 ack_bits 不仅是一个节省带宽的巧妙的编码，它同样也增加了信息冗余来抵御包的丢失。每个应答码要被发送32次。如果有一个包丢失了，仍然有其他31个包有着相同的应答码。从统计上来说，应答码还是非常有可能送达的。 但突发的传送数据包的丢失还是有可能发生的，所以重要的是要注意： 如果你收到一个数据包n的应答码，那么这个包肯定已经收到了。 如果你没有收到应答码，那么这个包就很有可能 没有被收到。但是…它也许会是，仅是应答码没有送达。这种情况是极其罕见的。 以我的经验，没有必要设计完善的应答机制。在一个极少丢应答码的系统上构建一个可靠性系统并不会增加什么大问题。但对于在所有情况下来测试这个系统的工作将会成为很大的挑战，因为还要考虑应答码丢失的边界情况。 所以如果你自己实现这个系统的话，请设置一个浸泡测试来覆盖糟糕的网络情况，用来确保你的应答系统是在正确的工作，相关地，你的消息系统的执行实际上是在这些网络情况下可靠地而且有序的交付可靠有序消息。以我之见（并且我已经写了许多这样的系统的变式至少有十次了），这是确保正确行为的一个必要步骤。 你在这篇文章的示例源代码中会找到这样一个浸泡测试，它对patreon支持是有效的，并且也在开源网络库libyojimbo中。 序列缓冲区为实现这个应答系统，我们在发送方还需要一个数据结构来追踪一个数据包是否已经被应答，这样我们就可以忽略冗余的应答（每个包会通过 ack_bits多次应答）。我们同样在接收方也还需要一个数据结构来追踪那些已经收到的包，这样我们就可以在数据包的报头填写ack_bits的值。 这个数据结构应该具有以下属性： 常量时间内插入（插入可能会是_随机_的，例如乱序数据包…） 给定的数据包的序列号在常量时间内查询一个条目是否存在 对给定的数据包序列号，在常量时间内访问数据存储 常量时间内删除条目 你可能会想。哦，当然，哈希表。但还有一个更简单的方法： 12345678910111213141516171819const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData&#123; bool acked;&#125;; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;&#125; 你在这可以看到的窍门是这个滚动的缓冲区是以序列号来作为索引的： 1const int index =sequence % BufferSize; 这是可行的，因为我们并不在意旧条目破坏。随着序列号的递增，旧的条目也自然而然地随着我们插入了新条目而被重写。sequence_buffer[index]的值是用来测试该索引的条目是否实际上与你所搜寻的序列号相符。一个缓冲序列的值是0xFFFFFFFF 就表示一个空的条目并自然地对任何序列号查询返回NULL，没有任何其他（代码）分支。 当条目被顺序添加，就像一个被发送的队列，对插入所需要做的就是把这个序列缓冲区的值更新为新的序列号并且在该索引处重写这个数据： 123456PacketData &amp; InsertPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];&#125; 但在接收端数据包以乱序到达并且有一部分丢失。在高得离谱的丢包率下（99%），我就会看到旧的序列缓冲区条目还存在，但是新条目的序列号已经超过了65535并且循环到达了旧条目之前，并且打破了我的应答逻辑（导致错误应答并打破了可靠性，这时发送方会真的认为对方已经接收到了一些东西但其实并不是…） 解决这个问题的办法是遍历上一个最高的插入序列与最新收到的插入序列之间的条目（如果它是更加新的话）并在缓冲区清除这些条目即都置为0xFFFFFFFF。现在，在一般情况下，插入操作是非常接近 时间常量的，但最糟的情况是，在先前最高的序列号和当前插入的序列号之间线性遍历的次数n等于缓冲区的长度。 在我们继续之前，我想指出，你可以用这个数据结构做更多事情而不仅是对于应答码。例如，你可以加入发送时间，来扩展每个包的数据： 12345struct PacketData&#123; bool acked; double send_time;&#125;; 有了这些信息你可以对往返时间通过做指数级的平滑取平均数做修正，最终得到合理的预期往返时间。你甚至可以看到在发送数据包的序列缓存区的数据包会比你RTT预计的（你现在应该已经收到了它们的应答码…）要旧，通过这个往返时间对还没有应答的包做判断，来决定创建你的数据包丢失预计。 应答算法现在我们来把注意力集中在数据包层级应答的实际算法上。该算法如下： 在数据包发送端： 在数据包发送缓冲区插入一个为当前发送的数据包序列号的条目，并且带着表示它还没有被应答的字段 从本地接收到的数据包序列缓存和最新接收到的数据包序列号中生成 ack 和ack_bits 填写数据包报头的sequence, ack 和 ack_bits 值 发送数据包并递增发送数据包的序列号 在数据包接收端： 从数据包报头读取 sequence 如果 sequence 比之前的最新收到的数据包序列号要新，就更新最新的接收到的数据包序列号 在接收数据包序列缓冲区中为这个数据包插入一个条目 从数据包报头中的ack和ack_bits解码应答的数据包序列号组合 迭代应答的数据包序列号以及任何还没有被应答的数据包调用 OnPacketAcked( uint16_t sequence ) 在数据包发送缓冲区把这个数据包设置为‘已应答的’。 重要的一点是这个算法是在两端都可以执行的，所以如果你有一个客户端和一个服务端，然后每一方的连接运行着同样的逻辑，维护自己的序列号发送的数据包，跟踪最新从另一方收到的数据包序列#还有从一个序列缓冲区里接收到的数据包中生成sequence, ack 和ack_bits 来发送到另一方。 并且这真的就是和它有关的全部了。现在当一个数据包被另一方接收到时，你有一个回调：OnPacketAcked 。这个可靠性系统的关键就在于你得知道哪个数据包被接收，你可以在你想的媒介之上创建_任何_ 可靠性系统。它不仅限于可靠有序的消息。例如，你可以用它确认哪个不可靠的状态更新已经完成了，用以实现基于每个物体的增量编码。 消息对象消息是小型的对象（比数据包大小要小，所以很多消息装配在一个典型的数据包中）并且知道如何将它们自己序列化。在我的系统里，它们使用一个统一的序列化函数来执行序列化。 这个序列化的函数是模板化的，所以你只要写它一次它就会处理读、写以及_测量_ 。 是的。测量。我喜欢的一个技巧就是有一个虚拟流类叫做MeasureStream，如果你调用了序列化函数，它不参与任何的序列化，而只是测量_可能_被写入的比特数。这对于解决哪个消息要装载到你的数据包里，特别是当消息可以有任意复杂的序列化函数的情况时是特别有用的。 12345678910111213141516171819202122232425262728293031323334struct TestMessage : public Message&#123; uint32_t a,b,c; TestMessage() &#123; a = 0; b = 0; c = 0; &#125; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; &#125; virtual SerializeInternal( WriteStream &amp; stream ) &#123; return Serialize( stream ); &#125; virtual SerializeInternal( ReadStream &amp; stream ) &#123; return Serialize( stream ); &#125; virtual SerializeInternal( MeasureStream &amp; stream ) &#123; return Serialize( stream ); &#125;&#125;; 这里的技巧是桥接统一模板的序列化函数（所以你只需要写一次）与虚拟序列化方法，这通过从虚函数每个流类型中调入它。我通常用一个宏来打包这个引用，但它在上文的代码中这个宏已经被展开，所以你可以看到发生了什么。 现在，假设你有一个基于消息的指针可以让你做到这样并且它只是通过重载来工作： 12Message message = CreateSomeMessage();message-&gt;SerializeInternal( stream ); 另外一个就是如果你在编译时间知道了消息的完整组合，就可以为每个类型在被调入序列化函数之前实现一个关于消息类型转换为确切消息类型的大的switch语句。我在过去已经在控制台平台实现的这个消息系统这么做了（如PS3 SPUs），但对于现在（2016）的应用程序，虚函数的总开销是忽略不计的。 消息从一个基类派生，这个基类提供一个通用的接口例如序列化、消息的查询类型还有引用计数。引用计数是必要的，因为消息是通过指针传递的并且在应答之前不只是存储在消息发送队列，而且也存储在外发的数据包中，包本身是C++结构体。 这是一个策略，就是避免通过指针传递消息和数据包来复制数据。别的一些场景（理想的情况是在一个单独的线程）它们里面的数据包和消息会序列化到一个缓冲区。最终，当不再有对存在消息发送队列的消息的引用时（消息已经被应答）并且没有数据包包含保留在数据包发送队列里的消息，消息即是被销毁的。 我们也需要一种方式来创建消息。我用一个消息的工厂类来做这件事情，它有一个被复写的虚函数来根据类型创建一个消息。如果这个数据包工厂还知道消息类型的总数量就好了，那样我们就可以在网络上序列化一个消息类型，因为有严格的界限和在有效范围之外的消息类型值的包的恶意丢弃： 1234567891011121314151617181920212223242526272829enum TestMessageTypes&#123; TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES&#125;; // message definitions omitted class TestMessageFactory : public MessageFactory&#123; public: Message Create( int type ) &#123; switch ( type ) &#123; case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); &#125; &#125; virtual int GetNumTypes() const &#123; return TEST_MESSAGE_NUM_TYPES; &#125;&#125;; 再次重申，这是一个引用并且通常是被包裹在宏里面的，但下面要说明的就是它具体是怎么回事了。 可靠的有序消息算法现在让我们来着手于如何在应答系统中实现可靠有序消息的细节。 发送可靠有序消息的算法如下： 对于消息发送： 使用测量流测量消息序列化后的大小 插入消息指针和它序列化的位数到一个序列缓冲区，它以消息id为索引。设置消息最后被发送的时间为-1 递增发送的消息的id 对于数据包发送： 从左-&gt;右（递增的消息id顺序）遍历在最早的未应答消息id和最新插入的消息id之间的发送消息序列缓冲区的这组消息。 超级重要的： 不要发送一个接收方不能缓冲的消息id，不然你会破坏消息的应答（由于这个消息不能被缓冲，但包含它的数据包会被应答，发送方就会认为这个消息已经被接收了，就不再重新发送它了）。这意味着你必须不能发送一个消息id等于或比最早的未应答消息的id加上消息接收缓冲区大小要新。 对于那些在最后0.1秒没有被发送的消息并且适合我们留在数据包的有效空间，就把它追加到消息列表去发送。根据迭代顺序得到优先级。 包括在外发数据包中的消息，并且要为每个消息添加一个引用。确保每个数据包的析构函数中减了引用计数。 在数据包n存储消息的数量并且消息的标识数组包含在一个序列缓冲区的数据包中，以外发数据包的序列号为索引。 把数据包添加到数据包发送队列。 对于数据包接收： 遍历包含在数据包中的消息组合并且把它们插入到消息接收队列缓冲区，以它们的消息id为索引。 前面的应答系统自动地应答我们刚刚收到的数据包序列号。 对于数据包应答： 用序列号查找包含在数据包中消息组合的标识部分。 从消息发送队列中移除那些已经存在的消息，并减少它们的引用计数。 通过从发送消息队列缓冲区中之前未应答消息的id的转寄来更新最后一个未应答的消息的id，直到发现一个有效的消息条目，或者你会到达当前发送消息的id。以先到者为准。 对于消息接收： 检查接受消息缓冲区确保当前收到消息的id对应的消息是否存在。 如果消息存在，将它从消息队列缓冲区中移除，递增接收消息的id并给这个消息返回一个指针。 如果否，就是没有有效的消息可接收。返回NULL。 总之，消息要保持被包含在数据包中直到这个数据包包含的消息得到应答。我们在发送者方使用一个数据结构来给消息标识的组合映射数据包序列号以便应答。当消息被应答时，要从发送队列中移除。对于接收方，以乱序到达的消息会被存储在一个序列缓冲区，并以消息id为索引，这个id会让我们以它们被发送的顺序接收它们。 最终的结果在发送方，这为用户提供了一个像这样的接口： 12345678TestMessage message = (TestMessage) factory.Create( TEST_MESSAGE );if ( message )&#123; message-&gt;a = 1; message-&gt;b = 2; message-&gt;c = 3; connection.SendMessage( message );&#125; 还有在接收方： 1234567891011121314while ( true )&#123; Message message = connection.ReceiveMessage(); if ( !message ) break; if ( message-&gt;GetType() == TEST_MESSAGE ) &#123; TestMessage testMessage = (TestMessage*) message; // process test message &#125; factory.Release( message );&#125; 正如你所看到的，它已经是简单得不能再简单了。 如果这几个接口有引起你的兴趣，请看看我的新开源库 libyojimbo。 我希望你到现在为止对这个系列的文章是享受的请在 patreon上支持我的写作，并且我将更快写新的文章，再者你会在加州大学伯克利分校软件的开源许可证下获得这篇文章的示例源代码。谢谢你的支持！ 即将到来：**客户端与服务器的连接** 在“创建一个游戏网络协议”的下一篇文章会展示你如何在UDP之上创建你自己的客户端/服务器连接层，它会实现挑战/响应，会在服务器上分配客户端插槽，当服务器爆满或检测超时就拒绝客户端的连接。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议四之发送大块数据]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%9E%84%E5%BB%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%9B%9B%E4%B9%8B%E5%8F%91%E9%80%81%E5%A4%A7%E5%9D%97%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[本篇自我总结有了本系列上篇文章中的分包和重组系统为何还要这个发送大块数据系统?是否是多余的?是雷同的吗?请看总结概要理清思路, 再细看文章. 为什么需要做这个发送大块数据系统第一眼看上去，这种替代性的技术似乎非常类似于数据包的分包和重组，但是它的实现是完全不同的。这种实现上的差异的目的是为了解决数据包分包和重组的一个关键弱点 : 一个片段的丢失就会导致整个数据包都要被丢弃掉然后重新分包重发。 你可能需要这样做的一些常见的例子包括：客户端在首次加入的时候，服务器需要下发一个大的数据块给客户端(可能是世界的初始状态)、一开始用来做增量编码的基线或者是在一个多人在线网络游戏里面客户端在加载界面所等待的大块数据。 在这些情况下非常重要的是不仅要优雅地处理数据包的丢失，还要尽可能的利用可用的带宽并尽可能快的发送大块数据。 这个发送大块数据系统大致可以理解为是一个在原来分包和重组系统的基础上增加了分包确认功能, 也就是说增加了可靠性的部分. 本篇基本术语In this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments). 块 : 在这个新系统中，大块的数据被称为”块”(chunks) 片段 : 而块被分成的分包被称为”片段”(slices) 数据包的结构设计这个系统在网络上发送的数据包类型一共有两种类型： Slice packet片段数据包 : 这包括了一个块的片段，最多大小为1k。 1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk;struct SlicePacket : public protocol2::Packet&#123; uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) &#123; serialize_int( stream, sliceBytes, 1, SliceSize ); &#125; else if ( Stream::IsReading ) &#123; sliceBytes = SliceSize; &#125; serialize_bytes( stream, data, sliceBytes ); return true; &#125;&#125;; Ack packet确认数据包 : 一个位域bitfield指示哪些片段已经收到, we just send the entire state of all acked slices in each ack packet. When the ack packet is received (including the slice that was just received). 1234567891011121314struct AckPacket : public protocol2::Packet &#123; uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &lt; numSlices; ++i ) serialize_bool( stream, acked[i] ); return true; &#125; &#125;; &#125;&#125;; 发送方的实现与之前文章介绍的数据包的分包和重组系统不同，块系统在同一时间只能由一个块正在传输。发送方的策略是： 持续的发送片段数据包，直到所有的片段数据包都被确认。 不再对已经确认过的片段数据包进行发送。 对于发送方而言有一点比较微妙，实现一个片段数据包重新发送的最小延迟是一个很棒的主意，如果不这么做的话，就可能会出现这种一样情况，对于很小的块数据或者一个块的最后几个片段数据包，很容易不停的发送它们把整个网络都塞满。正是因为这一原因，我们使用了一个数组来记录每个片段数据包的上一次发送时间。重新发送延迟的一个选择是使用一个估计的网络往返时延，或者只有在超过上一次发送时间网络往返时延*1.25还没有收到确认数据包的情况才会重新发送。或者，你可以说“这根本就无所谓”，只要超过上一次发送时间100毫秒了就重新发送。我只是列举适合我自己的方案！ 我们使用以下的数据结构来描述发送方： 123456789101112class ChunkSender&#123; bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];&#125;; 接收方的实现思路首先，接收方的设置会从块0开始。当一个片段数据包从网络上传递过来，并且能够匹配这个块id的话，“receiving”状态会从false翻转为true，第一个片段数据包的数据会插入” chunkData“变量的合适位置，片段数据包的数量会根据第一个片段数据包里面的数据进行正确的设置，已经接收到的片段数据包的数量会加一，也就是从0到1，针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 随着这个块数据的其他片段数据包的到来，会对每一个片段数据包进行检测，判断它们的id是否与当前块的id相同，如果不相同的话就会被丢弃。如果这个片段数据包已经收到过的话，那么这个包也会被丢弃。否则，这个片段数据包的数据会插入” chunkData“变量的合适位置、已经接收到的片段数据包的数量会加一、针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 这一过程会持续进行，直到接收到所有的片段数据包。一旦接收到所有的片段数据包（也就是已经接收到的片段数据包的数量等于片段数据包的数量的时候），接收方会把“receiving “状态改为false，而把”readyToRead“状态改为true。当”readyToRead”状态为true的时候，所有收到的片段数据包都会被丢弃。在这一点上，这个处理过程通常非常的短，会在收到片段数据包的同一帧进行处理，调用者会检查”我有一块数据要读取么？“并处理块数据。然后会重置数据块接收器的所有数据为默认值，除了块数据的id从0增加到1，这样我们就准备好接收下一个块了。 1234567891011class ChunkReceiver&#123; bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];&#125;; 防DDos如果你对每个收到的片段数据包都会回复一个确认数据包的话，那么发送方能够构造一个很小的片段数据包发送给你，而你会回复一个比发送给你的片段数据包还大的确认数据包，这样你的服务器就变成了一个可以被人利用来进行DDos放大攻击的工具。 永远不要设计一个包含对接收到的数据包进行一对一的映射响应的协议。让我们举个简单例子来说明一下这个问题。如果有人给你发送1000个片段数据包，永远不要给他回复1000个确认数据包。相反只发一个确认数据包，而且最多每50毫秒或者100毫秒才发送一个确认数据包。如果你是这样设计的话，那么DDos攻击完全不可能的。 原文原文出处 原文标题 : Sending Large Blocks of Data (How to send blocks quickly and reliably over UDP) IntroductionHi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In the previous article we implemented packet fragmentation and reassembly so we can send packets larger than MTU. This approach works great when the data block you’re sending is time critical and can be dropped, but in other cases you need to send large blocks of quickly and reliably over packet loss, and you need the data to get through. In this situation, a different technique gives much better results. BackgroundIt’s common for servers to send large block of data to the client on connect, for example, the initial state of the game world for late join. Let’s assume this data is 256k in size and the client needs to receive it before they can join the game. The client is stuck behind a load screen waiting for the data, so obviously we want it to be transmitted as quickly as possible. If we send the data with the technique from the previous article, we get packet loss amplification because a single dropped fragment results in the whole packet being lost. The effect of this is actually quite severe. Our example block split into 256 fragments and sent over 1% packet loss now has a whopping 92.4% chance of being dropped! Since we just need the data to get across, we have no choice but to keep sending it until it gets through. On average, we have to send the block 10 times before it’s received. You may laugh but this actually happened on a AAA game I worked on! To fix this, I implemented a new system for sending large blocks, one that handles packet loss by resends fragments until they are acked. Then I took the problematic large blocks and piped them through this system, fixing a bunch of players stalling out on connect, while continuing to send time critical data (snapshots) via packet fragmentation and reassembly. Chunks and SlicesIn this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments). The basic idea is that slices are sent over the network repeatedly until they all get through. Since we are implementing this over UDP, simple in concept becomes a little more complicated in implementation because have to build in our own basic reliability system so the sender knows which slices have been received. This reliability gets quite tricky if we have a bunch of different chunks in flight, so we’re going to make a simplifying assumption up front: we’re only going to send one chunk over the network at a time. This doesn’t mean the sender can’t have a local send queue for chunks, just that in terms of network traffic there’s only ever one chunk in flight at any time. This makes intuitive sense because the whole point of the chunk system is to send chunks reliably and in-order. If you are for some reason sending chunk 0 and chunk 1 at the same time, what’s the point? You can’t process chunk 1 until chunk 0 comes through, because otherwise it wouldn’t be reliable-ordered. That said, if you dig a bit deeper you’ll see that sending one chunk at a time does introduce a small trade-off, and that is that it adds a delay of RTT between chunk n being received and the send starting for chunk n+1 from the receiver’s point of view. This trade-off is totally acceptable for the occasional sending of large chunks like data sent once on client connect, but it’s definitely not acceptable for data sent 10 or 20 times per-second like snapshots. So remember, this system is useful for large, infrequently sent blocks of data, not for time critical data. Packet StructureThere are two sides to the chunk system, the sender and the receiver. The sender is the side that queues up the chunk and sends slices over the network. The receiver is what reads those slice packets and reassembles the chunk on the other side. The receiver is also responsible for communicating back to the sender which slices have been received via acks. The netcode I work on is usually client/server, and in this case I usually want to be able to send blocks of data from the server to the client and from the client to the server. In that case, there are two senders and two receivers, a sender on the client corresponding to a receiver on the server and vice-versa. Think of the sender and receiver as end points for this chunk transmission protocol that define the direction of flow. If you want to send chunks in a different direction, or even extend the chunk sender to support peer-to-peer, just add sender and receiver end points for each direction you need to send chunks. Traffic over the network for this system is sent via two packet types: Slice packet - contains a slice of a chunk up to 1k in size. Ack packet - a bitfield indicating which slices have been received so far. The slice packet is sent from the sender to the receiver. It is the payload packet that gets the chunk data across the network and is designed so each packet fits neatly under a conservative MTU of 1200 bytes. Each slice is a maximum of 1k and there is a maximum of 256 slices per-chunk, therefore the largest data you can send over the network with this system is 256k. 1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk;struct SlicePacket : public protocol2::Packet&#123; uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) &#123; serialize_int( stream, sliceBytes, 1, SliceSize ); &#125; else if ( Stream::IsReading ) &#123; sliceBytes = SliceSize; &#125; serialize_bytes( stream, data, sliceBytes ); return true; &#125;&#125;; There are two points I’d like to make about the slice packet. The first is that even though there is only ever one chunk in flight over the network, it’s still necessary to include the chunk id (0,1,2,3, etc…) because packets sent over UDP can be received out of order. Second point. Due to the way chunks are sliced up we know that all slices except the last one must be SliceSize (1024 bytes). We take advantage of this to save a small bit of bandwidth sending the slice size only in the last slice, but there is a trade-off: the receiver doesn’t know the exact size of a chunk until it receives the last slice. The other packet sent by this system is the ack packet. This packet is sent in the opposite direction, from the receiver back to the sender. This is the reliability part of the chunk network protocol. Its purpose is to lets the sender know which slices have been received. 12345678910111213141516struct AckPacket : public protocol2::Packet&#123; uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp;amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &amp;lt; numSlices; ++i ) &#123; serialize_bool( stream, acked[i] ); return true; &#125; &#125;; &#125; &#125;&#125;; Acks are short for ‘acknowledgments’. So an ack for slice 100 means the receiver is acknowledging that it has received slice 100. This is critical information for the sender because not only does it let the sender determine when all slices have been received so it knows when to stop, it also allows the sender to use bandwidth more efficiently by only sending slices that haven’t been acked. Looking a bit deeper into the ack packet, at first glance it seems a bit redundant. Why are we sending acks for all slices in every packet? Well, ack packets are sent over UDP so there is no guarantee that all ack packets are going to get through. You certainly don’t want a desync between the sender and the receiver regarding which slices are acked. So we need some reliability for acks, but we don’t want to implement an ack system for acks because that would be a huge pain in the ass. Since the worst case ack bitfield is just 256 bits or 32 bytes, we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet. This last step, biasing in the direction of non-acked to ack, like a fuse getting blown, means we can handle out of order delivery of ack packets. Sender ImplementationLet’s get started with the implementation of the sender. The strategy for the sender is: Keep sending slices until all slices are acked Don’t resend slices that have already been acked We use the following data structure for the sender: 123456789101112class ChunkSender&#123; bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];&#125;; As mentioned before, only one chunk is sent at a time, so there is a ‘sending’ state which is true if we are currently sending a chunk, false if we are in an idle state ready for the user to send a chunk. In this implementation, you can’t send another chunk while the current chunk is still being sent over the network. If you don’t like this, stick a queue in front of the sender. Next, we have the id of the chunk we are currently sending, or, if we are not sending a chunk, the id of the next chunk to be sent, followed by the size of the chunk and the number of slices it has been split into. We also track, per-slice, whether that slice has been acked, which lets us count the number of slices that have been acked so far while ignoring redundant acks. A chunk is considered fully received from the sender’s point of view when numAckedSlices == numSlices. We also keep track of the current slice id for the algorithm that determines which slices to send, which works like this. At the start of a chunk send, start at slice id 0 and work from left to right and wrap back around to 0 again when you go past the last slice. Eventually, you stop iterating across because you’ve run out of bandwidth to send slices. At this point, remember our current slice index via current slice id so you can pick up from where you left off next time. This last part is important because it distributes sends across all slices, not just the first few. Now let’s discuss bandwidth limiting. Obviously you don’t just blast slices out continuously as you’d flood the connection in no time, so how do we limit the sender bandwidth? My implementation works something like this: as you walk across slices and consider each slice you want to send, estimate roughly how many bytes the slice packet will take eg: roughly slice bytes + some overhead for your protocol and UDP/IP header. Then compare the amount of bytes required vs. the available bytes you have to send in your bandwidth budget. If you don’t have enough bytes accumulated, stop. Otherwise, subtract the bytes required to send the slice and repeat the process for the next slice. Where does the available bytes in the send budget come from? Each frame before you update the chunk sender, take your target bandwidth (eg. 256kbps), convert it to bytes per-second, and add it multiplied by delta time (dt) to an accumulator. A conservative send rate of 256kbps means you can send 32000 bytes per-second, so add 32000 dt to the accumulator. A middle ground of 512kbit/sec is 64000 bytes per-second. A more aggressive 1mbit is 125000 bytes per-second. This way each update you accumulate a number of bytes you are allowed to send, and when you’ve sent all the slices you can given that budget, any bytes left over stick around for the next time you try to send a slice. One subtle point with the chunk sender and is that it’s a good idea to implement some minimum resend delay per-slice, otherwise you get situations where for small chunks, or the last few slices of a chunk that the same few slices get spammed over the network. For this reason we maintain an array of last send time per-slice. One option for this resend delay is to maintain an estimate of RTT and to only resend a slice if it hasn’t been acked within RTT * 1.25 of its last send time. Or, you could just resend the slice it if it hasn’t been sent in the last 100ms. Works for me! Kicking it up a notchDo the math you’ll notice it still takes a long time for a 256k chunk to get across: 1mbps = 2 seconds 512kbps = 4 seconds 256kbps = 8 seconds :( Which kinda sucks. The whole point here is quickly and reliably. Emphasis on quickly. Wouldn’t it be nice to be able to get the chunk across faster? The typical use case of the chunk system supports this. For example, a large block of data sent down to the client immediately on connect or a block of data that has to get through before the client exits a load screen and starts to play. You want this to be over as quickly as possible and in both cases the user really doesn’t have anything better to do with their bandwidth, so why not use as much of it as possible? One thing I’ve tried in the past with excellent results is an initial burst. Assuming your chunk size isn’t so large, and your chunk sends are infrequent, I can see no reason why you can’t just fire across the entire chunk, all slices of it, in separate packets in one glorious burst of bandwidth, wait 100ms, and then resume the regular bandwidth limited slice sending strategy. Why does this work? In the case where the user has a good internet connection (some multiple of 10mbps or greater…), the slices get through very quickly indeed. In the situation where the connection is not so great, the burst gets buffered up and most slices will be delivered as quickly as possible limited only by the amount bandwidth available. After this point switching to the regular strategy at a lower rate picks up any slices that didn’t get through the first time. This seems a bit risky so let me explain. In the case where the user can’t quite support this bandwidth what you’re relying on here is that routers on the Internet strongly prefer to buffer packets rather than discard them at almost any cost. It’s a TCP thing. Normally, I hate this because it induces latency in packet delivery and messes up your game packets which you want delivered as quickly as possible, but in this case it’s good behavior because the player really has nothing else to do but wait for your chunk to get through. Just don’t go too overboard with the spam or the congestion will persist after your chunk send completes and it will affect your game for the first few seconds. Also, make sure you increase the size of your OS socket buffers on both ends so they are larger than your maximum chunk size (I recommend at least double), otherwise you’ll be dropping slices packets before they even hit the wire. Finally, I want to be a responsible network citizen here so although I recommend sending all slices once in an initial burst, it’s important for me to mention that I think this really is only appropriate, and only really _borderline appropriate_behavior for small chunks in the few 100s of k range in 2016, and only when your game isn’t sending anything else that is time-critical. Please don’t use this burst strategy if your chunk is really large, eg: megabytes of data, because that’s way too big to be relying on the kindness of strangers, AKA. the buffers in the routers between you and your packet’s destination. For this it’s necessary to implement something much smarter. Something adaptive that tries to send data as quickly as it can, but backs off when it detects too much latency and/or packet loss as a result of flooding the connection. Such a system is outside of the scope of this article. Receiver ImplementationNow that we have the sender all sorted out let’s move on to the reciever. As mentioned previously, unlike the packet fragmentation and reassembly system from the previous article, the chunk system only ever has one chunk in flight. This makes the reciever side of the chunk system much simpler: 1234567891011class ChunkReceiver&#123; bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];&#125;; We have a state whether we are currently ‘receiving’ a chunk over the network, plus a ’readyToRead’ state which indicates that a chunk has received all slices and is ready to be popped off by the user. This is effectively a minimal receive queue of length 1. If you don’t like this, of course you are free to add a queue. In this data structure we also keep track of chunk size (although it is not known with complete accuracy until the last slice arrives), num slices and num received slices, as well as a received flag per-slice. This per-slice received flag lets us discard packets containing slices we have already received, and count the number of slices received so far (since we may receive the slice multiple times, we only increase this count the first time we receive a particular slice). It’s also used when generating ack packets. The chunk receive is completed from the receiver’s point of view when numReceivedSlices == numSlices. So what does it look like end-to-end receiving a chunk? First, the receiver sets up set to start at chunk 0. When the a slice packet comes in over the network matching the chunk id 0, ‘receiving’ flips from false to true, data for that first slice is inserted into ‘chunkData’ at the correct position, numSlices is set to the value in that packet, numReceivedSlices is incremented from 0 -&gt; 1, and the received flag in the array entry corresponding to that slice is set to true. As the remaining slice packets for the chunk come in, each of them are checked that they match the current chunk id and numSlices that are being received and are ignored if they don’t match. Packets are also ignored if they contain a slice that has already been received. Otherwise, the slice data is copied into the correct place in the chunkData array, numReceivedSlices is incremented and received flag for that slice is set to true. This process continues until all slices of the chunk are received, at which point the receiver sets receiving to ‘false’ and ‘readyToRead’ to true. While ‘readyToRead’ is true, incoming slice packets are discarded. At this point, the chunk receive packet processing is performed, typically on the same frame. The caller checks ‘do I have a chunk to read?’ and processes the chunk data. All chunk receive data is cleared back to defaults, except chunk id which is incremented from 0 -&gt; 1, and we are ready to receive the next chunk. ConclusionThe chunk system is simple in concept, but the implementation is certainly not. I encourage you to take a close look at the source code for this article for further details. 译文译文出处 翻译：张华栋 (wcby) 审校：王磊(未来的未来) 大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《构建游戏网络协议》的第四篇文章。在之前的文章中，我们讨论了如何在游戏协议这一层实现对数据包的分包和重组。 现在在这篇文章里面，我们将继续通过探索在UDP协议上发送大块数据的替代方案来继续我们构建一个专业级别的游戏网络协议的征程。 第一眼看上去，这种替代性的技术似乎非常类似于数据包的分包和重组，但是它的实现是完全不同的。这种实现上的差异的目的是为了解决数据包分包和重组的一个关键弱点-一个片段的丢失就会导致整个数据包都要被丢弃掉。这种行为是非常不好的，因为它会随着分包数量的增加而放大数据包丢失的概率。当你遇到大块数据包的时候，这种放大是如此的明显，加入256 k大小的分包丢失率是1%的话，那么原始数据包就有92.4%的概率被丢弃。平均来说，你需要发送原始数据包10次，它才能顺利的到达网络的另外一端！ 如果你需要在一个可能会有数据包丢失的网络上比如说互联网，快速和可靠地发送大量的数据，很明显，这样的方法是完全不可接受的。你可能需要这样做的一些常见的例子包括：客户端在首次加入的时候，服务器需要下发一个大的数据块给客户端(可能是世界的初始状态)、一开始用来做增量编码的基线或者是在一个多人在线网络游戏里面客户端在加载界面所等待的大块数据。 在这些情况下非常重要的是不仅要优雅地处理数据包的丢失，还要尽可能的利用可用的带宽并尽可能快的发送大块数据。这正是我要在这篇文章里面告诉你该如何做的内容。 块和片段让我们开始使用基本术语。在这个新系统中，大块的数据被称为”块“，而它们被分成的分包被称为”片段”。 这个名字上的改变使的块系统的术语(块和片段)不同于数据包分包和重组的术语(数据包和分包)。这是我认为很重要的一个事情，因为这些系统是在解决不同的问题，没有理由你不能在相同的网络协议中同时这两个系统。事实上，我经常把这两个结合起来，在时间比较关键的增量数据包里面使用数据包的分包和重组，当客户端加入游戏的时候，使用块系统来下发整个游戏世界的初始状态下(非常大的数据包)。 块系统的基本思想，真是一点都不复杂，是把块分成很多片段，然后通过网络多次发送片段，直到他们都顺利的到达网络的另外一端。当然，因为我们正在UDP协议上实现这个功能，同时还有可能数据包会丢失、数据包乱序到达以及数据包重复到达的情况，简单的概念在实现中也会变得非常复杂，因为我们必须在UDP协议上建立我们自己的具有基本可靠性的系统，这样发送方才能知道这个片段已经被网络的另外一端成功收到。 如果我们有一组不同的块正在传输过程中(就像我们在数据包的分包和重组中所做的那样)，那么可靠性的问题就会变得非常棘手，所以我们要做一个简化的假设。我们一次只会通过网络发送一个块的数据。这并不意味着发送者不能在本地有一个块的发送队列，这只是意味着在实际的网络传输中只有一个块的数据会正在传递。 这么做之所以有意义，是因为有了这一点假设以后就能保证块系统能够可靠有序的发送块。如果你因为某些原因在同一时间发送块0和块1，这会发生什么？你不能在块0到来之前处理块1，否则这个传输过程就不是有序可靠了。也就是说，如果你挖得深一些的话，你会发现一次只能发送一个块确实引入了一个小的权衡，它给正在接收的块Ｎ增加了一个网络往返延迟，以及从接收方的角度看块Ｎ＋１的发送开始时间也被延迟了一个网络往返延迟。 这个代价是完全可以接受的，因为发送大块数据是一个非常偶然的事情（举些简单的例子来说，当客户端连接上来的时候会发送大块数据，当新的关卡需要进行加载的时候才会发送大块数据。。。），但是如果1秒钟内10次或者20次发送块数据的话这就是绝对不能被接受的了。所以我希望你能看到这个系统是专为什么目的设计的以及不是为什么目的设计的。 数据包的结构在块系统中有两方会参与，分别是发送方和接收方。 发送方是负责将块压入队列并通过网络发送片段。接收方是负责在网络的另外一端读取这些片段并进行重组。接收方还负责通过发送“确认”数据包给发送方来与发送方交流表明这个片段已经收到。 我工作过的网络模式通常是客户端与服务端之间的通信，在这种情况下，我通常希望能够从服务器往客户端发送大块数据，以及从客户端到服务器发送大块数据。所以在这种情况下，有两个发送方和两个接收方，一个发送方在客户端对应着在服务器那边有一个接收方，反过来也是如此。可以把发送方和接收方认为是块传输协议的终点，这样也就定义了网络流的方向。如果你想在不同的方向发送块，甚至是扩展块的发送方来支持点对点的发送，只需要在你需要发送块的每个方向添加一个发送方和一个接收方作为终点。 这个系统在网络上发送的数据包类型一共有两种类型： 1）片段数据包-这包括了一个块的片段，最多大小为1k。 2）确认数据包-一个位域指示哪些片段已经收到。 片段数据包是从发送方发送到接收器的。这是通过网络对块数据进行传递的有效载荷数据包，在设计的时候每个片段数据包的大小都贴近一个保守的最大传输单元的大小，也就是 1200字节。每个片段数据包最大是1 k，每个块最多有256个片段数据包，所以通过这个系统可以通过网络发送的最大的数据是256k（如果你愿意的话，你可以增加这个片段的最大数目）。我建议保持片段的大小为1k，这主要是基于最大传输单元方面的考虑。 1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet&#123; uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) &#123; serialize_int( stream, sliceBytes, 1, SliceSize ); &#125; else if ( Stream::IsReading ) &#123; sliceBytes = SliceSize; &#125; serialize_bytes( stream, data, sliceBytes ); return true; &#125;&#125;; 在这里我想对片段数据包进行两点说明。第一点是即使只有一个块在网络上进行传输，仍然是有必要在数据包里面包含一个块的id(比如说，0、1、2、3、等等等)，这是,因为通过UDP协议发送的数据包可以是乱序到达的。通过这种方式的话，如果一个片段数据包到达的时候对应着一个已经接受过的块，举个简单的例子来说明，你正在接受块2的数据，但是块1的一个片段数据包现在到达了，你可以直接拒绝这个数据包，而不是接受它的数据包并把它的数据插入到块2从而把块2的数据给弄混了。 第二点。由于我们知道块分成片段的方法会把所有的片段除了最后一个以外都弄成必须SliceSize的大小(也就是1024字节)。我们利用这一点来节省一点带宽，我们只在最后一个片段里面发送片段的大小，但这是一种权衡:接收方不知道块的确切大小到底是多少字节，直到它接收到最后一个片段才能知道。 可以让这个系统继续往后发送新的数据包的机制是确认数据包。这个数据包是沿着另外一个方向进行发送的，也就是从接收方发回给发送方，这也是块网络协议中负责可靠性的部分。它存在的目的是让发送方知道这个片段已经被发送方收到。 1234567891011121314struct AckPacket : public protocol2::Packet &#123; uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &lt; numSlices; ++i ) serialize_bool( stream, acked[i] ); return true; &#125; &#125;; &#125;&#125;; ack是“确认”的缩写。所以一个对片段100的确认数据包意味着接收方确认它已经接收到了片段100。这对于发送方来说是一条关键信息，因为它不仅让发送方知道什么时候所有的片段都已经被成功接收，这样发送方就可以停止发送了，它还允许发送方只重发那些还没有被确认的片段，这样就能让发送方更有效率的利用带宽。 让我们对确认数据包再深入一点思考，似乎在一开始看上去对于每个数据包的所有分片都发送确认包似乎有点多余。我们为什么要这么做?是的，这是因为确认数据包是通过UDP协议发送的，所以没有办法保证所有的确认数据包都会成功的到达网络的另外一端，你当然不会希望发送方和接收方之间对于目前确认到那个片段的信息都是不同步的。 所以我们需要一些确认数据包传输的可靠性，但是我们不希望实现一个确认数据包的确认系统，因为这将会是一个非常痛苦和麻烦的过程。因为在最坏的情况下，确认数据包的大小是256位或32字节，最简单的方法是也是最好的。we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet. 基本的发送方实现现在我们已经了解了这个系统背后的基本概念，让我们从发送方的实现开始实现整个系统。 发送方的策略是： 1）持续的发送片段数据包，直到所有的片段数据包都被确认。 2）不再对已经确认过的片段数据包进行发送。 我们使用以下的数据结构来描述发送方： 12345678910111213class ChunkSender&#123; bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];&#125;; 正如之前提到的那样，一次只会发送一个块的数据，如果我们正在发送一个块的数据的时候，那么关于“发送”的状态是true，假如我们处于闲置状态、正在准备发送一个块的数据的时候，那么关于“发送”的状态是false。在这个实现中，如果当前有一个块的数据仍在通过网络进行发送的话，你不能发送另外一个块的数据。你必须等待当前块的数据发送完毕之后才可以发送另外一个块的数据。如果你不喜欢的话，在块的发送器的前端按照你的意愿可以放置一个发送队列。 接下来，我们有我们正在发送的块数据的id，或者如果我们没有在发送块数据的话，那么我们有要发送的下一个块数据的id、以及这个块所分成的片段数据包的数量。我们也会跟踪每个片段数据包，来记录这个片段数据包是否已经被确认，这可以让我们避免重发那些已经收到的片段数据包，并且我们还会记录迄今为止已经确认收到的片段数据包的数量，这个数量会去掉冗余的确认，也就是每个片段数据包的确认只算一次。从发送方的观点来看，只有当确认的片段数据包的数量等于这个块所分成的片段数据包的数量的时候才会这个块数据已经被完全收到了。 我们还需要为这个算法记录当前发送的片段数据包的id，因为这将决定了哪些片段数据包将被发送。它的工作机制大致是这样：一个块数据开始发送的时候，是从id为0的片段数据包开始发送的，然后依次从左到右开始发送直到经过最后一个片段数据包(也就是id为分包数量的大小-1)的时候会回头从id为0的片段数据包继续发送。最终，你会停止这个迭代因为发送的片段数据包已经耗尽了带宽。在这一点上，我们通过记录当前发送的片段数据包的id就能记住我们当前遍历的片段数据包的索引，这样在下一次开始遍历的时候你就可以继续从这个位置开始发送片段数据包。最后一部分是非常重要的，这是因为它可以把发送一个块数据所有的片段数据包这个事情是分散开，而不是在一起就全部发出去。 现在让我们讨论下带宽限制。显然你不能把所有的片段数据包一次全部发完，因为如果这么做的话，会把整个链接堵住，那么，我们该如何限制发送方所使用的带宽?我的实现机制大概是这样的：当你对全部的片段数据包进行遍历并且考虑你想要发送的每个片段数据包的时候，大概估计下这个片段数据包会需要占据多少字节，比如可以用这种估计算法：大概这个片段的字节数+一些协议的开销和UDP / IP的报头。然后用所需的字节数和你带宽预算里面可用来进行发送的字节数进行比较。如果带宽预算里面没有足够可用的字节数，那么就停止发送。否则的话，从带宽预算里面减去发送这个片段数据包所需的字节数，然后对于下个片段数据包重复整个过程。 带宽预算里面可用的字节发送预算是从哪里计算得来的？在每一帧更新块的发送方之前，把你的目标带宽（比如说每秒256KB）转换成每秒可以发送的字节数，然后用它乘以更新时间来把记过放到一个累加器里面。每秒256KB是一个比较保守的发送速率，这意味你可以每秒发送32000个字节，所以把32000 dt这个值添加到累加器里面。每秒512KB是一个比较适中的估计，意味你可以每秒发送64000个字节。每秒1MB是一个比较激进的估计，意味你可以每秒发送125000个字节。通过这种方法，在每次更新的时候你就可以累加你被允许发送的字节数了，这样当你可以按照预算来发送最大数量的片段数据包，如果还有数据没有发完的话，会等到下一帧的时候再尝试发送。 对于发送方而言有一点比较微妙，实现一个片段数据包重新发送的最小延迟是一个很棒的主意，如果不这么做的话，就可能会出现这种一样情况，对于很小的块数据或者一个块的最后几个片段数据包，很容易不停的发送它们把整个网络都塞满。正是因为这一原因，我们使用了一个数组来记录每个片段数据包的上一次发送时间。重新发送延迟的一个选择是使用一个估计的网络往返时延，或者只有在超过上一次发送时间网络往返时延*1.25还没有收到确认数据包的情况才会重新发送。或者，你可以说“这根本就无所谓”，只要超过上一次发送时间100毫秒了就重新发送。我只是列举适合我自己的方案！ 把发送方实现的更完美一点如果你仔细用数学计算一下的话，你会注意到对于一个256K 的数据块而言，它要在网络上发送完毕仍然需要发送很长的时间： 如果网络速率是每秒1M的话，就需要2秒钟的时间。 如果网络速率是每秒512KB的话，就需要4秒钟的时间。 如果网络速率是每秒256KB的话，就需要8秒钟的时间。 这可有点糟糕。我们实现系统的重点是快速和可靠性。再次强调下需要能够快速传递。如果块系统的传输不能做到快速的话，这是不是会不太好？块系统的一些典型用例会支持这一点。举个简单的例子来说明，当客户端第一次连接上服务器的时候，一大块数据需要立刻发送给客户端，或者在客户端退出加载界面开始游戏的时候需要能够大量数据快速下发给客户端。你想要尽快的传递完需要的数据，而且在这两种情况下，用户对于自己的带宽并没有什么太多其他的用途，那么为什么不使用尽可能多的带宽? 在过去我曾经尝试过一个方法，就是在一开始的时候尽量传递，这取得了很好的效果。假设你的块大小并不是那么大，而且你的块发送频率并不那么频繁，我没找到什么理由为什么你不能在一开始就把所有的片段数据包都发送出去，填充满贷款，然后等待100毫秒，在恢复成正常的带宽受限的片段数据包发送策略。 为什么这样会取得良好的效果？如果用户有一个良好的网络连接（可以每秒发送超过10MB的数据甚至更多。。。），事实上，片段数据包在网络上的传输非常的快速。如果是连接的情况并不是那么好的情况下，大部分的片段数据包会得到缓冲，大部分的片段数据包受限于带宽但是会尽可能快的发送出去。处理完这些数据包之后，就会切换到常规的策略，从那些第一次没有发送出去的片段数据包选择合适的进行发送。 这似乎有点冒险，所以让我来解释一下。如果出现大量数据需要传输但是已经超过带宽限制的情况，互联网上的路由器会倾向于缓冲这些数据包，而不是不惜代价的抛弃它们。这就是TCP协议会做的事情。通常情况下，我讨厌这个机制因为它会诱发延迟而且会弄乱那些你想要尽快交付的游戏数据包，但在这种情况下它是一个非常好的行为，这是因为玩家真的没有其他事情可以做，智能等待你的块数据赶紧传输完毕。只是在你的块数据传输完毕以后，会有一些垃圾数据或者交通拥堵，它会影响你的游戏开始的几秒钟。另外，请确保你增加了网络两端的加操作系统的套接字缓冲区的大小，这样它们才可以比你最大的块数据的大小要大(我建议至少增加一倍)，否则在超过网络带宽的限制之前你就会出现丢弃段数据包的情况。 最后，我想成为一个负责任的网络公民，虽然在这里我推荐在最开始连接的时候一次发送所有的片段数据包，所以对我来说介绍下我认为这真的是适当的是非常非常重要的，在2016年的网络环境下，发送几百个KB量级的数据包是没什么大不了的行为，而且只会发生在没有其他关键数据同时发送的情况下。让我们举个简单的例子来说明，如果用户正在玩你的游戏，那么当你发送大块数据的时候，使用保守的策略。如果不这么做的话，就会冒影响用户游戏体验的风险，这是因为你的发送行为可能会诱导额外的网络延迟或者出现数据包丢失的情况。 同样，如果你的块数据非常大的情况下，比如说是十几MB的情况，那么请不要使用这种野蛮发送的策略，这是因为这种方法太过于依赖陌生人的仁慈，也就是在你和你的数据包目的地之间的路由器缓冲区。如果要持续发送非常大的数据块保持一个高吞吐量有必要实施一些更聪明的方法。这是某种自适应的方法，它会试图尽快发送数据，但是一旦检测到因为连接上有太多的数据在传输导致太多的延迟或者数据包的丢失，就能切换回一个低速的方式。这样一个系统超出了本文的范围。 接收方的实现现在我们已经解决了发送方实现的所有细节和小问题，那么让我们开始实现接收方。正如之前提到的那样，与之前文章介绍的数据包的分包和重组系统不同，块系统在同一时间只能由一个块正在传输。 这使得块系统的接收方可以实现的更加简单，你可以看下面的实现: 1234567891011class ChunkReceiver&#123; bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];&#125;; 我们有一个状态来记录我们是否正在网络上“接收”一个块数据，加上“readyToRead’”状态来表明是否已经有一个块的所有片段数据包都已经收到、已经准备好被用户弹出进行读取处理了。接收队列的最小长度是1，这是非常有效的。如果你不喜欢这个的话，你当然可以立即从块数据接收器里面将这个数据弹出并把它插入实际的接收队列。 在这个数据结构中我们还记录了块数据的大小（尽管不是完全准确，直到收到最后一个片段数据包才能准确的计算块数据的大小）、片段数据包的数量、已经接收到的片段数据包的数量还有针对每个片段数据包的一个接收标记。针对每个片段数据包的接收标记可以让我们丢弃那些我们已经收到的片段数据包，并计算到目前为止我们已经收到的片段数据包的数量（因为我们可能会多次收到同一个片段数据包，但是我们只会在第一次收到这个片段数据包的才会增加计数器的值）。它也被用在生成确认数据包上。当已经接收到的片段数据包的数量等于片段数据包的数量的时候，从接收方的角度看这个块数据的接收才算完成。 首先，接收方的设置会从块0开始。当一个片段数据包从网络上传递过来，并且能够匹配这个块id的话，“receiving”状态会从false翻转为true，第一个片段数据包的数据会插入” chunkData“变量的合适位置，片段数据包的数量会根据第一个片段数据包里面的数据进行正确的设置，已经接收到的片段数据包的数量会加一，也就是从0到1，针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 随着这个块数据的其他片段数据包的到来，会对每一个片段数据包进行检测，判断它们的id是否与当前块的id相同，如果不相同的话就会被丢弃。如果这个片段数据包已经收到过的话，那么这个包也会被丢弃。否则，这个片段数据包的数据会插入”chunkData“变量的合适位置、已经接收到的片段数据包的数量会加一、针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 这一过程会持续进行，直到接收到所有的片段数据包。一旦接收到所有的片段数据包（也就是已经接收到的片段数据包的数量等于片段数据包的数量的时候），接收方会把“receiving “状态改为false，而把”readyToRead“状态改为true。当”readyToRead”状态为true的时候，所有收到的片段数据包都会被丢弃。在这一点上，这个处理过程通常非常的短，会在收到片段数据包的同一帧进行处理，调用者会检查”我有一块数据要读取么？“并处理块数据。然后会重置数据块接收器的所有数据为默认值，除了块数据的id从0增加到1，这样我们就准备好接收下一个块了。 浸泡测试的重要性和确认数据包第一眼看上去，确认数据包这个系统似乎很简单： 1）记录已经接收到的片段数据包。 2）当一个片段数据包收到以后，回复一个包含所有确认收到的片段数据包信息的确认数据包。 这看上去实现起来似乎相当的简单，但是像大多数发生在UDP协议的事情一样，当涉及到数据包丢失的时候，就有一些微妙的点让它的处理有点棘手。 一个对于确认数据包比较天真的实现可能是这样子的。每次收到片段数据包，就回复一个包含所有确认收到的片段数据包信息的确认数据包（也会包括刚收到的片段数据包的信息）。这看上去非常符合逻辑，但是这使得块协议给恶意发送者一个漏洞使得它们可以块协议作为一个DDos的工具。如何作为一个DDos的工具?如果你对每个收到的片段数据包都会回复一个确认数据包的话，那么发送方能够构造一个很小的片段数据包发送给你，而你会回复一个比发送给你的片段数据包还大的确认数据包，这样你的服务器就变成了一个可以被人利用来进行DDos放大攻击的工具。 现在也许是因为我对DDos这个事情有一点偏执（我确实是有一点），但是一般来说你可以防止对DDos的放大，永远不要设计一个包含对接收到的数据包进行一对一的映射响应的协议。让我们举个简单例子来说明一下这个问题。如果有人给你发送1000个片段数据包，永远不要给他回复1000个确认数据包。相反只发一个确认数据包，而且最多每50毫秒或者100毫秒才发送一个确认数据包。如果你是这样设计的话，那么滥用你的UDP协议对DDos进行放大就是完全不可能的。 还有其他的方法让这个确认系统容易出错，而这些都往往表现为”发送挂起“。换句话说，接收方已经知道这个块已经发送完毕了，但是由于程序员的错误，发送方错过了一个确认数据包(可能是针对最后一个片段数据包的确认数据包)并且卡入到一个状态，会不停的反复重发这个片段数据包而没有得到一个确认数据包的响应。 在过去10年里，我可能至少5次从头开始实现这个块系统，每次我都找到新的和令人兴奋的方式来让发送方挂起。我开发和测试块系统的策略是首先进行编码确认它能够跑起来，然后设置一个测试工具在有大量的数据包丢失、数据包乱序和重复的情况下随机发送随机大小的块。这往往会清除任何挂起。我曾经实现过的块系统都至少有一个挂起存在，通常会有2到3个挂起。所以如果你是打算从头开始实现这个块系统的话，请不要轻敌。请设置一个浸泡测试。你会感谢我在这里的提醒的。 我通常遇到的第一次挂起是由于对同一个片段数据包的多次收到不会回复一个确认数据包。它有点像这样：” 哦，这个片段数据包已经收到过了么？已经收到过了就丢弃它”，然后忘记在确认数据包里面设置标记。这对于发送方来说是一个困扰，因为这样的话就不会有一个确认数据包，那么如果出现这种情况的话，又恰好遇到第一次收到这个片段数据包的时候发送的确认数据包出现丢包的情况，发送方根本就不知道这个他在反复发送的片段数据包其实已经被收到了。如果你就是这么不巧，遇上了第一次收到这个片段数据包的时候发送的确认数据包出现丢包的情况，那么就遇上了挂起的情况。如果你想在你的代码里面重现这个情况的话，可以在收到最后一个片段数据包的时候不发送确认数据包，那么出现的情况就是这种挂起了。 下一个挂起会发生在接收方在发送方知道之前就已经知道块发送完毕并切换它的状态变量“readyToRead”来丢弃后续传入的片段数据包。在这种状态下，即使接收方认为块已经完全接收完毕，但是发送方还不知道这一点，所以有必要设置确认数据包对应的标志位，即使块已经完全接收完毕，这样发送方才能一直接收到提示块已经全部发送完毕的确认数据包。 通常遇到的最后一个挂起情况是在读取完块数据以后的状态切换里面，那个时候状态变量“readyToRead”已经切回false而块的id也加一了。让我们举个简单例子来说明一下这个问题，块0已经完成接收，用户已经完成对块0的读取并且块id已经递增到1了，所以我们已经准备好接收块1的片段数据包了（我们会丢弃任何与我们当前正在接收块ID不同的片段数据包）。 再一次出现这种情况，就是这里的发送方因为确认数据包的丢失导致信息有一点滞后，可能是因为没有收到第一个确认数据包。在这种情况下，有必要关注片段数据包，如果我们正处于这么一个状态：我们尚未收到第n个片段数据包，但是前面n – 1个片段数据包都已经收到了，我们必须设置一个特殊的标记位然后我们会发送一个包含所有前面n – 1个片段数据包都已经收到信息的确认数据包，否则发送方不会意识到块数据已经收到并且发送方已经准备挂起了。 正如你所看到的那样，确认数据包的实现是有一点微妙的，这是一个有点奇怪的过程因为当片段数据包在网络的一端收到的时候，需要设置一个标记位来发送确认数据包直到发送方知道都有哪些发送的片段数据包被成功接收为止。如果你打破了片段数据包-&gt;确认数据包这个链接的话，那么整个系统就将挂起。我鼓励你仔细看看这篇文章的源代码搞清楚进一步的细节。 总结块系统在概念上是很简单的，但是它的具体实现肯定不是微不足道的。在我看来，实现发送者设计这一块是一个很好的学习经验，当你从头开始实现这样的系统的时候一定有很多东西需要学习。 我希望你喜欢这个系统的设计，并试着自己动手从头开始实现它。这是一个很好的学习经历。此外，我鼓励你在patreon上支持我，作为回报，你会得到本文的示例源代码(以及本系列的其他文章的示例源代码)，还包括我在GDC 2015上关于网络物理的演讲的源代码。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议三之数据包的分包和重组]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%9E%84%E5%BB%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%B8%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%8C%85%E7%9A%84%E5%88%86%E5%8C%85%E5%92%8C%E9%87%8D%E7%BB%84%2F</url>
    <content type="text"><![CDATA[本篇自我总结本篇主要讲了数据包的分包和重组问题, 到底数据包多大才好呢?是不是越大越好呢?包太大了怎么办呢?请看总结, 不明之处再看文中具体讲解. 为什么需要做这个分包和重组系统每台计算机(路由器)会沿着路由强制要求数据包的大小会有一个最大的上限，这个上限就是所谓的最大传输单元MTU。如果任意一个路由器收到一个数据包的大小超过这个最大传输单元的大小，它有这么两个选择，a)在IP层对这个数据包进行分包，并将分包后的数据包继续传递，b)丢弃这个数据包然后告诉你数据包被丢弃了，你自己负责摆平这个问题。 实例 : 这儿有一个我会经常遇到的情况。人们在编写多人在线游戏的时候，数据包的平均大小都会非常的小，让我们假设下，这些数据包的平均大小大概只有几百字节，但时不时会在他们的游戏中同时发生大量的事情并且发出去的数据包会出现丢失的情况，这个时候数据包会比通常的情况下要大。突然之间，游戏的数据包的大小就会超过最大传输单元的大小，这样就只有很少一部分玩家能够收到这个数据包，然后整个通信就崩溃了。 本篇基本术语 数据包packets 分包fragments 分包的数据结构我们将允许一个数据包最多可以分成256个数据包，并且每个分包后的数据包的大小不会超过1024个字节。这样的话，我们就可以通过这样一个系统来发送大小最大为256k的数据包 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) // 数据包序号 [packet type = 0] (2 bits) [fragment id] (8 bits) // 分包ID [num fragments] (8 bits) [pad zero bits to nearest byte index] // 用于字节对齐的bits &lt;fragment data&gt; 发送分包后的数据包发送分包以后的数据包是一件非常容易的事情。如果数据包的大小小于保守估计的最大传输单元的大小。那么就按正常的方法进行发送。否则的话，就计算这个数据包到底该分成多少个1024字节的数据包分包，然后构建这些分包并按照之前发送正常数据包的方法进行发送。 发送出去以后也不记录发送的数据包的内容，这种发送以后不记录发送的数据包的内容的方法有一个后果，就是数据包的任意一个分包如果丢失的话，那么整个数据包就都要丢弃。随着分包数量的增加，整个数据包被丢弃的概率也随之增加.由此可见，当你需要发送要给256K的数据包的时候要发送256个分包，如果有一个分包丢失的话，你就要重新把这个256k的数据包再分一次包然后再发送出去。 什么时候用这个分包和重组系统呢因为发送出去以后也不记录发送的数据包, 随着分包数量的增加，整个数据包被丢弃的概率也随之增加, 而一个片段的丢失就会导致整个数据包都要被丢弃掉.所以我建议你要小心分包以后的数量。 这个分包和重组系统最好是只对2-4个分包的情况进行使用，而且最好是针对那种对时间不怎么敏感的数据使用或者是就算分包lost了也无所谓的情况。绝对不要只是为了省事就把一大堆依赖顺序的事件打到一个大数据包里面然后依赖数据包的分包和重组机制进行发送。这会让事情变得更加麻烦。 数据包分包和重组系统的关键弱点是一个片段的丢失就会导致整个数据包都要被丢弃掉, 想要解决这个弱点得使用大块数据发送策略, 见下一篇文章 构建游戏网络协议四之发送大块数据. 接收分包后的数据包之所以对分包后的数据包进行接收很困难的原因是我们不仅需要给缓冲区建立一个数据结构还要把这些分包重组成原始的数据包，我们也要特别小心如果有人试图让我们的程序产生崩溃而给我们发送恶意的数据包。 要非常小心检查一切可能的情况。除此之外，还有一个非常简单的事情要注意：让分包保存在一个数据结构里面，当一个数据包的所有分包都到达以后（通过计数来判断是否全部到达），将这些分包重组成一个大的数据包，并把这个重组后的大数据包返回给接收方。 什么样的数据结构在这里是有意义的?这里并没有什么特别的数据结构!我使用的是一种我喜欢称之为序列缓冲区的东西。我想和你分享的最核心的技巧是如何让这个数据结构变得高效： 12345678const int MaxEntries = 256; struct SequenceBuffer&#123; bool exists[MaxEntries]; uint16_t sequence[MaxEntries]; Entry entries[MaxEntries];&#125;; 原文原文出处 原文标题 : Packet Fragmentation and Reassembly (How to send and receive packets larger than MTU) IntroductionHi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In the previous article we discussed how to unify packet read and write into a single serialize function and added a bunch of safety features to packet read. Now we are ready to start putting interesting things in our packets and sending them over the network, but immediately we run into an interesting question: how big should our packets be? To answer this question properly we need a bit of background about how packets are actually sent over the Internet. BackgroundPerhaps the most important thing to understand about the internet is that there’s no direct connection between the source and destination IP address. What actually happens is that packets hop from one computer to another to reach their destination. Each computer along this route enforces a maximum packet size called the maximum transmission unit, or MTU. According to the IP standard, if any computer recieves a packet larger than its MTU, it has the option of a) fragmenting that packet, or b) dropping the packet. So here’s how this usually goes down. People write a multiplayer game where the average packet size is quite small, lets say a few hundred bytes, but every now and then when a lot of stuff is happening in their game and a burst of packet loss occurs, packets get a lot larger than usual, going above MTU for the route, and suddenly all packets start getting dropped! Just last year (2015) I was talking with Alex Austin at Indiecade about networking in his game Sub Rosa. He had this strange networking bug he couldn’t reproduce. For some reason, players would randomly get disconnected from the game, but only when a bunch of stuff was going on. It was extremely rare and he was unable to reproduce it. Alex told me looking at the logs it seemed like packets just stopped getting through. This sounded exactly like an MTU issue to me, and sure enough, when Alex limited his maximum packet size to a reasonable value the bug went away. MTU in the real worldSo what’s a reasonable maximum packet size? On the Internet today (2016, IPv4) the real-world MTU is 1500 bytes. Give or take a few bytes for UDP/IP packet header and you’ll find that the typical number before packets start to get dropped or fragmented is somewhere around 1472. You can try this out for yourself by running this command on MacOS X: 1ping -g 56 -G 1500 -h 10 -D 8.8.4.4 On my machine it conks out around just below 1500 bytes as expected: 12345678910111404 bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms1414 bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms1424 bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms1434 bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms1444 bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms1454 bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms1464 bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms1474 bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 msping: sendto: Message too longping: sendto: Message too longRequest timeout for icmp_seq 142 Why 1500? That’s the default MTU for MacOS X. It’s also the default MTU on Windows. So now we have an upper bound for your packet size assuming you actually care about packets getting through to Windows and Mac boxes without IP level fragmentation or a chance of being dropped: 1472 bytes. So what’s the lower bound? Unfortunately for the routers in between your computer and the destination the IPv4 standard says 576. Does this mean we have to limit our packets to 400 bytes or less? In practice, not really. MacOS X lets me set MTU values in range 1280 to 1500 so considering packet header overhead, my first guess for a conservative lower bound on the IPv4 Internet today would be 1200 bytes. Moving forward, in IPv6 this is also a good value, as any packet of 1280 bytes or less is guaranteed to get passed on without IP level fragmentation. This lines up with numbers that I’ve seen throughout my career. In my experience games rarely try anything complicated like attempting to discover path MTU, they just assume a reasonably conservative MTU and roll with that, something like 1000 to 1200 bytes of payload data. If a packet larger than this needs to be sent, it’s split up into fragments by the game protocol and re-assembled on the other side. And that’s exactly what I’m going to show you how to do in this article. Fragment Packet StructureLet’s get started with implementation. The first thing we need to decide is how we’re going to represent fragment packets over the network so they are distinct from non-fragmented packets. Ideally, we would like fragmented and non-fragmented packets to be compatible with the existing packet structure we’ve already built, with as little overhead as possible in the common case when we are sending packets smaller than MTU. Here’s the packet structure from the previous article: [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [packet type] (2 bits for 3 distinct packet types) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) In our protocol we have three packet types: A, B and C. Let’s make one of these packet types generate really large packets: 123456789101112131415161718192021222324static const int MaxItems = 4096 * 4;struct TestPacketB : public Packet&#123; int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) &#123; numItems = random_int( 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) items[i] = random_int( -100, +100 ); &#125; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) &#123; serialize_int( stream, items[i], -100, +100 ); &#125; return true; &#125;&#125;; This may seem somewhat contrived but these situations really do occur. For example, if you have a strategy where you send all un-acked events from server to client and you hit a burst of packet loss, you can easily end up with packets larger than MTU, even though your average packet size is quite small. Another common case is delta encoded snapshots in a first person shooter. Here packet size is proportional to the amount of state changed between the baseline and current snapshots for each client. If there are a lot of differences between the snapshots the delta packet is large and there’s nothing you can do about it except break it up into fragments and re-assemble them on the other side. Getting back to packet structure. It’s fairly common to add a sequence number at the header of each packet. This is just a packet number that increases with each packet sent. I like to use 16 bits for sequence numbers even though they wrap around in about 15 minutes @ 60 packets-per-second, because it’s extremely unlikely that a packet will be delivered 15 minutes late. Sequence numbers are useful for a bunch of things like acks, reliability and detecting and discarding out of order packets. In our case, we’re going to use the sequence number to identify which packet a fragment belongs to: [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type] (2 bits) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) Here’s the interesting part. Sure we could just add a bit is_fragment to the header, but then in the common case of non-fragmented packets you’re wasting one bit that is always set to zero. What I do instead is add a special fragment packet type: 12345678enum TestPacketTypes&#123; PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES&#125;; And it just happens to be free because four packet types fit into 2 bits. Now when a packet is read, if the packet type is zero we know it’s a fragment packet, otherwise we run through the ordinary, non-fragmented read packet codepath. Lets design what this fragment packet looks like. We’ll allow a maximum of 256 fragments per-packet and have a fragment size of 1024 bytes. This gives a maximum packet size of 256k that we can send through this system, which should be enough for anybody, but please don’t quote me on this. With a small fixed size header, UDP header and IP header a fragment packet be well under the conservative MTU value of 1200. Plus, with 256 max fragments per-packet we can represent a fragment id in the range [0,255] and the total number of fragments per-packet [1,256] with 8 bits. [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] &lt;fragment data&gt; Notice that we pad bits up to the next byte before writing out the fragment data. Why do this? Two reasons: 1) it’s faster to copy fragment data into the packet via memcpy than bitpacking each byte, and 2) we can now save a small amount of bandwidth by inferring the fragment size by subtracting the start of the fragment data from the total size of the packet. Sending Packet FragmentsSending packet fragments is easy. For any packet larger than conservative MTU, simply calculate how many 1024 byte fragments it needs to be split into, and send those fragment packets over the network. Fire and forget! One consequence of this is that if any fragment of that packet is lost then the entire packet is lost. It follows that if you have packet loss then sending a 256k packet as 256 fragments is not a very good idea, because the probability of dropping a packet increases significantly as the number of fragments increases. Not quite linearly, but in an interesting way that you can read more about here. In short, to calculate the probability of losing a packet, you must calculate the probability of all fragments being delivered successfully and subtract that from one, giving you the probability that at least one fragment was dropped. 11 - probability_of_fragment_being_delivered ^ num_fragments For example, if we send a non-fragmented packet over the network with 1% packet loss, there is naturally a 1⁄100chance the packet will be dropped. As the number of fragments increase, packet loss is amplified: Two fragments: 1 - (99⁄100) ^ 2 = 2% Ten fragments: 1 - (99⁄100) ^ 10 = 9.5% 100 fragments: 1 - (99⁄100) ^ 100 = 63.4% 256 fragments: 1 - (99⁄100) ^ 256 = 92.4% So I recommend you take it easy with the number of fragments. It’s best to use this strategy only for packets in the 2-4 fragment range, and only for time critical data that doesn’t matter too much if it gets dropped. It’s definitely not a good idea to fire down a bunch of reliable-ordered events in a huge packet and rely on packet fragmentation and reassembly to save your ass. Another typical use case for large packets is when a client initially joins a game. Here you usually want to send a large block of data down reliably to that client, for example, representing the initial state of the world for late join. Whatever you do, don’t send that block of data down using the fragmentation and re-assembly technique in this article. Instead, check out the technique in next article which handles packet loss by resending fragments until they are all received. Receiving Packet FragmentsIt’s time to implement the code that receives and processed packet fragments. This is a bit tricky because we have to be particularly careful of somebody trying to attack us with malicious packets. Here’s a list of all the ways I can think of to attack the protocol: Try to send out of bound fragments ids trying to get you to crash memory. eg: send fragments [0,255] in a packet that has just two fragments. Send packet n with some maximum fragment count of say 2, and then send more fragment packets belonging to the same packet n but with maximum fragments of 256 hoping that you didn’t realize I widened the maximum number of fragments in the packet after the first one you received, and you trash memory. Send really large fragment packets with fragment data larger than 1k hoping to get you to trash memory as you try to copy that fragment data into the data structure, or blow memory budget trying to allocate fragments Continually send fragments of maximum size (256⁄256 fragments) in hope that it I could make you allocate a bunch of memory and crash you out. Lets say you have a sliding window of 256 packets, 256 fragments per-packet max, and each fragment is 1k. That’s 64 mb per-client. Can I fragment the heap with a bunch of funny sized fragment packets sent over and over? Perhaps the server shares a common allocator across clients and I can make allocations fail for other clients in the game because the heap becomes fragmented. Aside from these concerns, implementation is reasonably straightforward: store received fragments somewhere and when all fragments arrive for a packet, reassemble them into the original packet and return that to the user. Data Structure on Receiver SideThe first thing we need is some way to store fragments before they are reassembled. My favorite data structure is something I call a sequence buffer: 123456789101112131415const int MaxEntries = 256;struct SequenceBuffer&#123; uint32_t sequence[MaxEntries]; Entry entries[MaxEntries];&#125;;``` Indexing into the arrays is performed with modulo arithmetic, giving us a fast O(1) lookup of entries by sequence number: ```cppconst int index = sequence % MaxEntries; A sentinel value of 0xFFFFFFFF is used to represent empty entries. This value cannot possibly occur with 16 bit sequence numbers, thus providing us with a fast test to see if an entry exists for a given sequence number, without an additional branch to test if that entry exists. This data structure is used as follows. When the first fragment of a new packet comes in, the sequence number is mapped to an entry in the sequence buffer. If an entry doesn’t exist, it’s added and the fragment data is stored in there, along with information about the fragment, eg. how many fragments there are, how many fragments have been received so far, and so on. Each time a new fragment arrives, it looks up the entry by the packet sequence number. When an entry already exists, the fragment data is stored and number of fragments received is incremented. Eventually, once the number of fragments received matches the number of fragments in the packet, the packet is reassembled and delivered to the user. Since it’s possible for old entries to stick around (potentially with allocated blocks), great care must be taken to clean up any stale entries when inserting new entries in the sequence buffer. These stale entries correspond to packets that didn’t receive all fragments. And that’s basically it at a high level. For further details on this approach please refer to the example source code for this article. Click here to get the example source code for this article series. Test Driven DevelopmentOne thing I’d like to close this article out on. Writing a custom UDP network protocol is hard. It’s so hard that even though I’ve done this from scratch at least 10 times, each time I still manage to fuck it up in a new and exciting ways. You’d think eventually I’d learn, but this stuff is complicated. You can’t just write low-level netcode and expect it to just work. You have to test it! My strategy when testing low-level netcode is as follows: Code defensively. Assert everywhere. These asserts will fire and they’ll be important clues you need when something goes wrong. Add functional tests and make sure stuff is working as you are writing it. Put your code through its paces at a basic level as you write it and make sure it’s working as you build it up. Think hard about the essential cases that need to be property handled and add tests that cover them. But just adding a bunch of functional tests is not enough. There are of course cases you didn’t think of! Now you have to get really mean. I call this soak testing and I’ve never, not even once, have coded a network protocol that hasn’t subsequently had problems found in it by soak testing. When soak testing just loop forever and just do a mix of random stuff that puts your system through its paces, eg. random length packets in this case with a huge amount of packet loss, out of order and duplicates through a packet simulator. Your soak test passes when it runs overnight and doesn’t hang or assert. If you find anything wrong with soak testing. You may need to go back and add detailed logs to the soak test to work out how you got to the failure case. Once you know what’s going on, stop. Don’t fix it immediately and just run the soak test again. Instead, add a unit test that reproduces that problem you are trying to fix, verify your test reproduces the problem, and that it problem goes away with your fix. Only after this, go back to the soak test and make sure they run overnight. This way the unit tests document the correct behavior of your system and can quickly be run in future to make sure you don’t break this thing moving forward when you make other changes. Add a bunch of logs. High level errors, info asserts showing an overview of what is going on, but also low-level warnings and debug logs that show what went wrong after the fact. You’re going to need these logs to diagnose issues that don’t occur on your machine. Make sure the log level can be adjusted dynamically. Implement network simulators and make sure code handles the worst possible network conditions imaginable. 99% packet loss, 10 seconds of latency and +/- several seconds of jitter. Again, you’ll be surprised how much this uncovers. Testing is the time where you want to uncover and fix issues with bad network conditions, not the night before your open beta. Implement fuzz tests where appropriate to make sure your protocol doesn’t crash when processing random packets. Leave fuzz tests running overnight to feel confident that your code is reasonably secure against malicious packets and doesn’t crash. Surprisingly, I’ve consistently found issues that only show up when I loop the set of unit tests over and over, perhaps these issues are caused by different random numbers in tests, especially with the network simulator being driven by random numbers. This is a great way to take a rare test that fails once every few days and make it fail every time. So before you congratulate yourself on your tests passing 100%, add a mode where your unit tests can be looped easily, to uncover such errors. Test simultaneously on multiple platforms. I’ve never written a low-level library that worked first time on MacOS, Windows and Linux. There are always interesting compiler specific issues and crashes. Test on multiple platforms as you develop, otherwise it’s pretty painful fixing all these at the end. This about how people can attack the protocol. Implement code to defend against these attacks. Add functional tests that mimic these attacks and make sure that your code handles them correctly. This is my process and it seems to work pretty well. If you are writing a low-level network protocol, the rest of your game depends on this code working correctly. You need to be absolutely sure it works before you build on it, otherwise it’s basically a stack of cards. In my experience, game neworking is hard enough without having suspicions that that your low-level network protocol has bugs that only show up under extreme network conditions. That’s exactly where you need to be able to trust your code works correctly. So test it! 译文译文出处 译者：崔嘉艺(milan21) 审校：崔国军（星际迷航） 大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《构建游戏网络协议》的第三篇文章。在之前的文章中，我们讨论了如何将数据包的读取和写入用一个单独的序列化函数来实现。 现在我们要开始把一些有趣的事情放到这些数据包中，,但正如你即将开始编码的令人惊叹的多人在线动作、第一人称射击、大型多人在线角色扮演游戏、多人在线战术竞技游戏会发生的那样，当你以每秒120次的频率发送8k大小的数据包，游戏网络中会传来一个声音呼喊着你:“不要发送超过1200字节大小的数据包!” 但是都已经2016年了，你真的要注意最大传输单元这个东西么? 不幸的是，答案是是的！ 最大传输单元MTU你可能已经听说过最大传输单元了。在网络程序员中流传着大量有关最大传输单元问题的故事。那么这到底是怎么回事呢？究竟什么是最大传输单元?为什么你要在乎最大传输单元这个事情？ 当你通过互联网来发送数据包的时候到底背后发生了什么？这些数据包要从一台计算机(路由器)跳到另一个计算机(路由器)上，如此往复多次才能到达自己的目的地。这是一个分组交换网络具体如何运作的方式。在大部分时间里，它的工作方式不像是在源和目的地之间存在一条直接的连接。 但意外的是：每台计算机(路由器)会沿着路由强制要求数据包的大小会有一个最大的上限，这个上限就是所谓的最大传输单元。如果任意一个路由器收到一个数据包的大小超过这个最大传输单元的大小，它有这么两个选择，a)在IP层对这个数据包进行分包，并将分包后的数据包继续传递，b)丢弃这个数据包然后告诉你数据包被丢弃了，你自己负责摆平这个问题。 这儿有一个我会经常遇到的情况。人们在编写多人在线游戏的时候，数据包的平均大小都会非常的小，让我们假设下，这些数据包的平均大小大概只有几百字节，但时不时会在他们的游戏中同时发生大量的事情并且发出去的数据包会出现丢失的情况，这个时候数据包会比通常的情况下要大。突然之间，游戏的数据包的大小就会超过最大传输单元的大小，这样就只有很少一部分玩家能够收到这个数据包，然后整个通信就崩溃了。 就在去年(2015年)，我与亚历克斯·奥斯汀在Indiecade谈论他的游戏” Sub Rosa “中的网络部分。他遇到了一些奇怪的无法重现的网络bug。出于某种原因，一些客户端（在所有玩家里面总是有那么一个或者两个）会随机的从游戏中断开连接并且其他人都能够正常游戏。查看日志的话，亚历克斯又觉得一切都是正常的，只是看上去好像数据包突然停止进行传递了。 对我来说，这听上去就像是一个最大传输单元所引起的问题，并且我非常确信。当亚历克斯把他最大的数据包大小限制在一个合理的值之内，这个bug就再也没有出现了。 真实世界中的最大传输单元MTU所以什么是“一个合理的数据包的大小”？ 在今天的互联网上(2016年，还是基于IPv4)，典型的最大传输单元的大小是1500字节。在UDP/IP数据包的包头添加或者去掉几个字节，你会发现在数据包开始出现被丢弃或者被分包情况的一个典型的边界值是1472。 你可以自己在MacOS X尝试运行下下面这个命令： 1ping -g 56 -G 1500 -h 10 -D 8.8.4.4 在我的机器上，这个结果在略微低于1500字节的大小上下浮动，符合预期： 1234567891011121314151617181920211404bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms1414bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms1424bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms1434bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms1444bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms1454bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms1464bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms1474bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 msping:sendto: Message too longping:sendto: Message too longRequesttimeout for icmp_seq 142 为什么是1500字节？这是MacOS X上默认的最大传输单元的大小。这也是Windows平台上默认的最大传输单元的大小。所以现在我们对数据包的大小有了一个上限（也就是不能超过这个默认的最大传输单元的大小），假如你真的关心数据包通过Windows平台或者Mac平台进行传播而不希望数据包在IP层进行分包或者有被丢弃的可能的话，那么就要保证数据包的大小不能超过这个上限：1472个字节。 那么这个最大传输单元的大小的下限是多少呢？MacOS X允许设置的最大传输单元的大小的值是在1280到1500，所以我对现在互联网上通过IPv4进行传播的数据包的最大传输单元的大小的下限有一个比较保守的估计，就是1200字节。如果是在通过IPv4进行传播的情况下这个比较保守的大传输单元的大小的下限也会是一个很好的估计值，任意数据包只要大小小于1280字节都能保证在没有IP层分包的情况下顺利的传播。 这个估计与我在职业生涯中感受到的情况是比较一致的。以我的经验来看，很少有游戏会试图做这些复杂的事情，诸如尝试发现路径上的最大传输单元的大小之类的，它们一般都是假定一个合理又保守的最大传输单元的大小然后一直遵循这个值。如果出现一个要发送的数据包比这个保守的最大传输单元的大小要大的情况，游戏协议会将这个数据包分成几个包然后在网络的另外一侧进行重组。 而这正是我要在这篇文章里面要向你传授的内容。 分包后的数据包的结构让我们从决定该对网络上传输的数据分包后采用什么的结构进行表示来开始构建我们的数据包分包和重组机制。在理想状态下，我们希望分包以后的数据包和未分包的数据包兼容我们现在已经建立好的数据包结构，这样当我们发送小于最大传输单元的大小的数据包的时候，网络协议没有任何多余的负载。 下面是在前一篇文章结束的时候得到的数据包的结构： [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [packet type] (2 bits for 3 distinct packet types) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) 在我们这个例子之中，我们一共有三个数据包的类型，分别是Ａ、Ｂ和Ｃ。 让我们用这三个数据包类型中的一个制造一个比最大传输单元的大小还要大一些的数据包： 1234567891011121314151617181920212223static const int MaxItems = 4096 * 4; struct TestPacketB : public protocol2::Packet&#123; int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) &#123; numItems = random_int( 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) items[i] = random_int( -100, +100 ); &#125; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) serialize_int( stream, items[i], -100, +100 ); return true; &#125;&#125;; 这可能看起来不怎么自然，但在现实世界中这些情况真的会发生。举个简单的例子来说，如果你有一个策略，这个策略是从服务器往客户端发送所有的未确认的事件，你会得到一组可信赖也有序的事件，但是也会遇到大量数据包的丢失的情况，你会轻易的遇到数据包比最大传输单元的大小还要大的情况，即使你的数据包的平均大小非常小。（译注：这是由于丢包重传导致的不停重发，而重发的数据包在UDP或者TCP上会进行合并。所以即使数据包的平均大小远小于最大传输单元的大小，但是由于大量这样的数据包的合并，还是很容易出现超过最大传输单元的大小的情况）。 在大多数的情况下，通过实现这么一个策略：在一个数据包里面只包含很少一组事件或者状态更新来避免数据包的大小超过最大传输单元的大小，采用这种策略以后可以有效的规避上面的那种情况。这种规划在很多情况下都工作的很棒。。。但是有一种情况下这种策略也是存在问题的，这种情况就是增量编码。 由一个增量编码器创建的数据包的大小是与当前状态与之前状态之间所发生的状态变化的数量成正比的。如果这两个状态之间有大量的不同的话，那么增量也将很大并且你对这种情况其实是没有什么办法的。如果出现一个增量恰好比最大传输单元的大小要大的情况，当然这是一个坏运气下才会出现的情况，但是你仍然要发送这个超过最大传输单元的大小的数据包！所以你可以看到，在增量编码的情况下，你真的不能限制数据包的最大大小一定小于最大传输单元的大小，在这种情况下，数据包的分包和重组策略就有了用武之地。 让我们回到数据包的结构上面来。在每个数据包的包头的地方添加一个序号是一种非常常见的做法。这并没有什么复杂的。这只是一个数据包的序号，会在每个数据包进行发送的时候依次加一。举个例子来说，就是0、1、2、3这么简单。我喜欢用16比特来表示这个序号，即使在每秒发送60个数据包的情况下，只要15分钟序号就会被用完一遍，需要再次从头开始重用，但是这么做也没有什么关系，主要是因为你在网络上收到一个15分钟之前发送出去的数据包是一个非常罕见的事情，所以你很少会有机会困惑这到底是一个新的数据包还是一个旧的数据包（因为IP层在包头的地方有个跳转计数，超出一定跳转次数的数据包会被丢弃掉，所以基本不用担心这种情况）。如果你确实关心这个问题的话，请使用32比特的序号进行代替。 无论你选择用多少比特来表示这个序号，它们对于很多事情都是有用的，比如说可依赖性、检测和丢弃乱序的数据包等等。除此之外，我们需要一个数据包序号的原因是在对数据包进行分包的时候，我们需要某个方法来确定这个数据包的分包到底是属于哪个数据包的。 所以，让我们在我们的数据包的结构里面加上序号这个东西： [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type] (2 bits) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) 这是最有趣的部分。我们可以在数据包的头部只添加一个比特的标识 is_fragment，但是对于通常情况下根本不需要分包的数据包来说，你就浪费了一个比特，因为它总是要被置为0。这不是很好。 相反，我的选择是在数据包的结构里面添加了一个特殊的”分包后的数据包“的类型： 12345678enum TestPacketTypes&#123; PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES&#125;; 这恰好不需要占据任何的空间，是因为四个数据包类型正好可以用两个比特来表示，而这两个比特的空间已经用于表示数据包类型了，我们只是在原来的枚举上新加了一个类型。这么处理以后，每次当我们读取一个数据包的时候，如果这个数据包的类型是0的话，我们就知道这个数据包是一个特殊的分包以后的数据包，它的内存布局可以通过数据包的类型得知，否则的话，我们就走回原来的通用的对非分包的数据包进行读取和处理的方法。 让我们设计下这个分包后的数据包看起来应该是什么样子的。我们将允许一个数据包最多可以分成256个数据包，并且每个分包后的数据包的大小不会超过1024个字节。这样的话，我们就可以通过这样一个系统来发送大小最大为256k的数据包，这对于任意系统任意情况来说都应该是足够的，但是这只是我个人的一个看法，如果有特殊的情况，还请结合实际情况进行具体分析。 有了这么一个不大的固定大小的数据包包头结构，再加上UDP的包头结构以及IP的包头结构，一个分包以后的数据包会小于之前我们保守估计的最大传输单元的大小：1200字节。除此之外，因为一个数据包最多可以分包成256个数据包，我们可以用【0，255】这个范围来表示分包的id和序号，这样每个分包里面还需要有8比特来表示这个序号。 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] &lt;fragment data&gt; 请注意，我们把这几个比特对齐到了下一个字节，然后才把对齐以后的数据写入到数据包里面。我们为什么要这么做？这么做其实是有两个原因的: 1) 这么处理以后可以通过memcpy函数更快的把分包的数据拷贝到数据包里面而不需要使用位打包器来对每个字节进行处理。2) 我们通过不发送分包数据的大小节省了一小部分带宽。我们可以通过从数据包的整体大小减去分包数据起始位置的字节序号来推断出这个分包的大小。 发送分包以后的数据包发送分包以后的数据包是一件非常容易的事情。如果数据包的大小小于保守估计的最大传输单元的大小。那么就按正常的方法进行发送。否则的话，就计算这个数据包到底该分成多少个1024字节的数据包分包，然后构建这些分包并按照之前发送正常数据包的方法进行发送。发送出去以后也不记录发送的数据包的内容，这没什么困难的！ 这种发送以后不记录发送的数据包的内容的方法有一个后果，就是数据包的任意一个分包如果丢失的话，那么整个数据包就都要丢弃。由此可见，当你需要发送要给256K的数据包的时候要发送256个分包，如果有一个分包丢失的话，你就要重新把这个256k的数据包再分一次包然后再发送出去。这绝对不是一个好主意。 这显然是一个很糟糕的办法，因为随着分包数目的增多，发生丢失的概率肯定是显著的增大。这种增长关系不是线性的，而是一种相当复杂的关系，如果你对这个计算感兴趣的话，你可以读下这篇文章。 简而言之，如果你要计算一个数据包会被丢弃的概率，你必须要计算所有分包被成功发送到目的地的概率，然后从1中减去这个概率，得到的结果就是至少有一个分包丢失的概率。 下面这个公式可以用来计算因为分包丢失导致整个数据包被丢弃的概率： 11- ( probability of fragment being delivered ) ^ num_fragments 让我们举个简单的例子对上面这个公式进行说明，如果我们发送的一个不需要分包的数据包，如果它在网络上传说的时候丢失的概率是1%，那么只有百分之一的概率会出现这个数据包被丢弃的情况，或者我们不要嫌麻烦，把这些数据代入到上面这个公式里面: 1 – (99/100) ^ 1 = 1/100 = 1%。 随着分包数量的增加，整个数据包被丢弃的概率也随之增加： 两个分包的情况： 1 – (99/100) ^ 2 = 2%**。** 十个分包的情况： 1 – (99/100) ^ 10 = 9.5%**。** 一百个分包的情况： 1 – (99/100) ^ 100 = 63.4%**。** 二百五十六个分包的情况： 1 – (99/100) ^ 256 = 92.4%**。** 所以我建议你要小心分包以后的数量。这个策略最好是只对2-4个分包的情况进行使用，而且最好是针对那种对时间不怎么敏感的数据使用或者是就算分包lost了也无所谓的情况。绝对不要只是为了省事就把一大堆依赖顺序的事件打到一个大数据包里面然后依赖数据包的分包和重组机制进行发送。这会让事情变得更加麻烦。 大数据包的另外一种典型的用途是当客户端新连入一个游戏服务器的时候。游戏服务器通常会把大块的数据以一种可靠的方式下发给客户端。对于后期才加入的客户端而言，这些大块的数据也许代表了世界的初始状态。无论这些数据包含了怎么样的信息，请不要使用本篇文章的分包技巧来给客户端下发大块的数据。相反，请查阅这个系列教程的下篇文章，在那篇文章里面将讲解如何在有数据包可能发生丢失的情况下，快速可靠的发送大块的数据直到这些大块的数据完全被确认接收。 对分包后的数据包的接收尽管发送分包以后的数据包是一件相同简单的事情，但是对分包后的数据包进行接收就相对需要技巧了。 之所以对分包后的数据包进行接收很困难的原因是我们不仅需要给缓冲区建立一个数据结构还要把这些分包重组成原始的数据包，我们也要特别小心如果有人试图让我们的程序产生崩溃而给我们发送恶意的数据包。 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] &lt;fragment data&gt; 下面列出的是我能想到的如何攻击你的协议以便让你的服务器崩溃的所有方法： 尝试发送分包ID在一个限制范围内的分包，看看能不能让你的程序崩溃。举个例子来说，也许这个数据包只有两个分包，但是我会发送多个分包ID在【０，２５５】之内的分包。 发送一个数据包，让我们假设这个书包的最大分包数量是2，然后发送多个属于这个数据包的分包，但是在分包的数据包报头里面把最大分包的数量改为256，来希望你在接收完第一个分包以后，不会发现分包的最大数量信息被改变了，这样就有可能造成你的内存崩溃。 发送非常大的分包，让分包里面的数据超过1k，测试下你在试图把分包数据拷贝到数据结构的时候是否有良好的判断，如果没有良好的判断的话，这也许会让你的内存崩溃，或者占用大量的内容让你在分配新的分包的时候没有空间。 持续的给你的程序发送最大分包数目的分包（也就是如果最大分包数目是256的话，就持续不断的发送分包ID是256的数据包），希望你会分配大量的内容来容纳这些分包然后让你的内存崩溃。让我们假设你的程序中有一个滑动窗口，这个滑动窗口有256个数据包，每个数据包最多可以有256个分包，每个分包预留的空间是1k。那么也就是会给每个客户端预留67,108,864字节或者64mb的空间。我可以通过这种方法让服务器崩溃么？我能用一堆大小有趣的分包来耗尽的你地址空间的heap空间么？因为服务器的程序是你来设计实现的，所以只有你才知道这个问题的确切答案。它取决于你的内存预算以及如何分配内存来存储分包。如果你考虑过了觉得这会是一个问题，那么就限制下缓冲区中分包的数目或考虑减少每个数据包的分包的最大数目。考虑给每个分包静态分配数据结果或者使用一个内存池来减少内存碎片。 所以你可以看到，在接收端代码是非常脆弱的，要非常小心检查一切可能的情况。除此之外，还有一个非常简单的事情要注意：让分包保存在一个数据结构里面，当一个数据包的所有分包都到达以后（通过计数来判断是否全部到达），将这些分包重组成一个大的数据包，并把这个重组后的大数据包返回给接收方。 什么样的数据结构在这里是有意义的?这里并没有什么特别的数据结构!我使用的是一种我喜欢称之为序列缓冲区的东西。我想和你分享的最核心的技巧是如何让这个数据结构变得高效： 12345678const int MaxEntries = 256; struct SequenceBuffer&#123; bool exists[MaxEntries]; uint16_t sequence[MaxEntries]; Entry entries[MaxEntries];&#125;; 这里面有几件事情值得注意。首先，使用类似数组的结构可以允许对给定条目是否存在于一个序列缓冲区的存在性进行快速测试并且将测试结果进行缓存，即使每个数据包的条目结构是非常大的（而且这种结构对于数据包的分包和重组来说是非常棒的）。 那么我们该如何使用这个数据结构？当你接收到一个分包以后的数据包以后，这个数据包会携带一个序号指定它是属于哪个数据包的分包。这个序号会随着发送而不停的增大(所有序号全部用完导致发生环绕除外)， 所以最关键的技巧是你要对序号进行散列让散列后的序号进入一个数组中某个给定的位置，具体处理过程如下所示： 1int index = sequence % MaxEntries; 现在你可以用O(1)的时间复杂度进行一个快速测试，来通过序号看下一个给定的条目是否存在，并且判断下这个条目是否和你想要读取或者写入的数据包序号相匹配。也就是说，你既需要测试存在性又需要测试序号是否是预期的序号，这是因为所有的序号都是有效的数字(比如说是0)，还有就是因为根据一个特定的数据包序号查找到的条目可能是存在的，但是它属于过去的一个序号（比如说，这是某个其他的序号，但是恰巧通过取模的计算得到相同的序号）。 所以当一个新的数据包的第一个分包到达的时候，你需要把数据包的序号散列成一个索引，如果发现这个索引对应的内容还不存在的话，你需要设置exists[index] = 1，并设置sequence[index]来匹配你正在处理的数据包，并把这个分包储存在序号缓冲区对应的条目里面。这样当下一次有分包实际到达的时候，你会得到相同的序号，然后得到一个相当的索引，在查找的时候会发现对应这个索引的内容已经存在了，并且这个条目的序号正好能和刚刚接收到的数据包的序号匹配，所以你就可以把这个分包累加到这个条目上，这个过程会一直重复直到这个数据包的所有分包都被接收到为止。 如果从比较抽象的层次来解释这个事情的话，基本原理大概就是这样的。这种方法的一个更完整的解释请参阅本文的示例源代码。在地址https://www.patreon.com/gafferongames可以获取本系列文章示例的源代码。 网络协议的测试驱动开发还有一件事情我想在文章的末尾进行补充说明。我感觉如果我不向我的读者提及这个方法的话，就是对他们的一个伤害。 编写一个定制的网络协议是非常困难的。这个过程是如此的苦难以至于我从头开始至少编写了10次网络协议，但是每次我都觉得我在用一种全新的有趣的方法在做这个事情。也许你会认为是我在挑战自己，用一些新奇的方法来实现这个过程，但其实是这个过程太复杂了，完全没有办法按照预期的那样写完代码就期待它们能够正确的工作。你需要对写出来的代码进行测试！ 当编写底层网络协议层的代码的时候，我的策略是: 1、防御性编程。在一切可能的地方进行断言。当某些地方出现错误的时候这些断言会起作用，并且将成为查找问题的重要线索。 2、添加函数的功能测试，确保它们是如你的预期那样工作的。把你的代码放到一个可以运行和测试的环境下，这样可以不时地对它们进行测试以便可以确保它一直会像你起初创建它们时候那样良好的工作。仔细考虑有哪些情况需要正确的处理并给这些情况添加测试。 3、虽然函数测试非常有必要，但是只是添加一些函数测试是远远不够的。无论如何都有遇到你没有预料到的情况! 现在你必须把它们放到一个真实的环境下看看到底会发生什么。我把这个称之为浸泡测试，并且在我之前编写网络协议的过程中还从来没有过在浸泡测试的过程中没有发现问题的情况。浸泡测试只是在不停的循环，并会随机的做一些事情让你的系统在它的空间中处理一些情况，让我们举些简单的例子对它进行一些简单的说明，比如说构造出随机长度的数据包，而且这些数据包有一个非常高的丢失率，通过数据包模拟器发出的大量乱序并且重复的数据包等等。如果你的程序能够在一晚上的时间里面不挂起或者遇到断言，那么就算你的程序通过了浸泡测试。 4、如果你的程序在浸泡测试的过程中发现了某些问题。你可能需要在代码里面添加一些详细的日志以便下次在浸泡测试的时候如果遇到了同样的问题你可以找到出现问题的原因。一旦你知道发生了什么，就可以停止了。不要立即的修复这个问题并且再次运行浸泡测试。这种做法非常的愚蠢。相反，利用单元测试来不停的重现你需要修复的问题，确保单元测试能够重现问题，而且这个问题因为你的修复已经彻底修好了。只有在这样的处理流程之后，才能回到浸泡测试并确保程序在浸泡测试能正常运转一整夜。通过这种方式，单元测试能够记录你的系统的正确的行为并且在以后需要的时候可以快速的运行起来，确保当你做其他改变的时候不会导致一些原来修过的问题重复的出现。 这就是我的数据包分包和重组的处理流程了，它似乎工作的不错。如果你在进行一些底层的网络协议的设计，你的游戏的其他的部分将依赖于底层的网络协议的设计。在你继续构建其他的功能之前，你需要绝对的确认底层的网络协议是否能够正常的工作，否则就像一堆胡乱堆积的卡片，很容易就散架了。多人在线游戏的网络部分是非常困难的，如果不小心设计的话，很容易就会出现底层网络协议可能无法正常的工作或者存在缺陷。所以请确保你是知道你的底层网络协议是如何工作的！ 即将到来的文章的预告 下一篇文章是: 《发送大块的数据》 请继续阅读本系列的下一篇文章，在哪篇文章里面我将向你展示如何通过数据包快速可信赖的发送大块的数据，如果其中一块数据丢失了也不需要丢弃整个数据包！ 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议二之序列化策略]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%9E%84%E5%BB%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%BA%8C%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[自我总结本篇概要 读取数据的时候要特别小心， 因为可能会有攻击者发送过来的恶意的数据包以及错误的包， 在写入数据的时候你可能会轻松很多，因为如果有任何事情出错了，那几乎肯定是你自己导致的错误 统一的数据包序列化功能 ：诀窍在于让流类型的序列化函数模板化。在我的系统中有两个流类型：ReadStream类和WriteStream类。每个类都有相同的一套方法，但实际上它们没有任何关系。一个类负责从比特流读取值到变量中，另外一个类负责把变量的值写到流中。在模板里类似这样写, 通过 Stream::IsWriting 和 Stream::IsReading 模板会自动区分,然后帮你生产你想要的代码, 简洁漂亮 1234567891011121314151617181920212223242526272829303132333435363738class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; // ...&#125;;class ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; // ..&#125;template &lt;typename Stream&gt;bool serialize( Stream &amp; stream, float &amp; value )&#123; union FloatInt &#123; float float_value; uint32_t int_value; &#125;; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;&#125; 边界检查和终止读取 ： 把允许的大小范围也传给序列化函数而不仅仅是所需的比特数量。 序列化浮点数和向量 ： 计算机根本不知道内存中的这个32位的值到底是一个整数还是一个浮点数还是一个字符串的部分。它知道的就是这仅仅是一个32位的值。代码如下(可以通过一个联合体来访问看上去是整数的浮点数).有些时候，你并不想把一个完整精度的浮点数进行传递。那么该如何压缩这个浮点值？第一步是将它的值限制在某个确定的范围内然后用一个整数表示方式来将它量化。举个例子来说，如果你知道一个浮点类型的值是在区间[-10,+10]，对于这个值来说可以接受的精确度是0.01，那么你可以把这个浮点数乘以100.0让它的值在区间[-1000,+1000]并在网络上将其作为一个整数进行序列化。而在接收的那一端，仅仅需要将它除以100.0来得到最初的浮点值. 123456789union FloatInt&#123; float float_value; uint32_t int_value;&#125;; FloatInt tmp;tmp.float_value= 10.0f;printf(“float value as an integer: %x\n”, tmp.int_value ); 序列化字符串和数组 : 为什么要费那么大精力把一个字节数组按比特打包到你的比特流里?为什么不在序列化写入之前进行按字节进行对齐？Why not align to byte so you can memcpy the array of bytes directly into the packet?如何将比特流按字节对齐？只需要在流的当前位置做些计算就可以了，找出还差写入多少个比特就能让当前比特流的比特数量被8整除，然后按照这个数字插入填充比特（比如当前比特流的比特数量是323，那么323+5才能被8整除，所以需要插入5个填充比特）。对于填充比特来说，填充的比特值都是0，这样当你序列化读取的时候你可以进行检测，如果检测的结果是正确的，那么就确实是在读取填充的部分，并且填充的部分确实是0。一直读取到下一个完整字节的比特起始位置（可以被8整除的位置）。如果检测的结果是在应该填充的地方发现了非0的比特值，那么就中止序列化读取并丢弃这个数据包。 序列化数组的子集 : 当实现一个游戏网络协议的时候，或早或晚总会需要序列化一个对象数组然后在网络上传递。比如说服务器也许需要把所有的物体发送给客户端，或者有时候需要发送一组事件或者消息。如果你要发送所有的物体到客户端，这是相当简单直观的，但是如果你只是想发送一个数组的一个子集怎么办？最先想到也是最容易的办法是遍历数组的所有物体然后序列化一个bool数组，这个bool数组标记的是对应的物体是否通过网络发送。如果bool值为1那么后面会跟着物体的数据，否则就会被忽略然后下一个物体的bool值取决于流的下一个值。如果有大量的物体需要发送，举个例子来说，整个场景中有4000个物体，有一半的物体也就是2000个需要通过网络进行发送。每个物体需要一个序号，那么就需要2000个序号，每个序号需要12比特。。。。这就是说数据包里面24000比特或者说接近30000比特（几乎是30000，不是严格是，译注：原文如此）的数据被序号浪费掉了.可以把序号的编码方式修改下来节省数据，序号不再是全局序号，而是相对上一个物体的相对序号。 如何应对恶意数据包和错误包 : 如果某些人发送一些包含随机信息的恶意数据包给你的服务器。你会不会在解析的时候把服务器弄崩溃掉？有三种技术应对 : 协议ID : 在你的数据包里面包含协议ID。一般典型的做法是，头4个字节你可以设定一些比较罕见而且独特的值，你可以通过这３２比特的数据判断出来根本就不是你的应用程序的包，然后就可以直接丢弃了。 CRC32 : 对你的数据包整体做一个CRC32的校验，并把这个校验码放到数据包的包头。可以不发送这个协议ID，但是发送方和接收方提前确认过这个协议ID是什么，并在计算数据包CRC32值的时候装作这个数据包带上了这个协议ID的前缀来参与计算。这样如果发送方使用的协议ID与接收方不一致的时候，CRC32的校验就会失败，这将为每个数据包节省4个字节. 序列化检测 : 是在包的中间，在一段复杂的序列化写入之前或者之后写上一个已知的32比特整数，并在另外一端序列化读取的时候用相同的值进行检测判断。如果序列化检查值是不正确的，那么就中止序列化读取并丢弃这个数据包。. . . 原文原文出处 原文标题 : Serialization Strategies (Smart tricks that unify packet read and write) IntroductionHi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In the previous article, we created a bitpacker but it required manual checking to make sure reading a packet from the network is safe. This is a real problem because the stakes are particularly high - a single missed check creates a vulnerability that an attacker can use to crash your server. In this article, we’re going to transform the bitpacker into a system where this checking is automatic. We’re going to do this with minimal runtime overhead, and in such a way that we don’t have to code separate read and write functions, performing both read and write with a single function. This is called a serialize function. Serializing BitsLet’s start with the goal. Here’s where we want to end up: 123456789101112struct PacketA&#123; int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; &#125;&#125;; Above you can see a simple serialize function. We serialize three integer variables x,y,z with 32 bits each. 123456789101112131415struct PacketB&#123; int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) &#123; serialize_bits( buffer, elements[i], 32 ); &#125; return true; &#125;&#125;; And now something more complicated. We serialize a variable length array, making sure that the array length is in the range [0,MaxElements]. Next, we serialize a rigid body with an simple optimization while it’s at rest, serializing only one bit in place of linear and angular velocity: 1234567891011121314151617181920212223242526struct RigidBody&#123; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? ( velocity.length() == 0 ) : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) &#123; serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); &#125; else if ( Stream::IsReading ) &#123; linear_velocity = vec3f(0,0,0); angularvelocity = vec3f(0,0,0); &#125; return true; &#125;&#125;; Notice how we’re able to branch on Stream::IsWriting and Stream::IsReading to write code for each case. These branches are removed by the compiler when the specialized read and write serialize functions are generated. As you can see, serialize functions are flexible and expressive. They’re also safe, with each serialize call performing checks and aborting read if anything is wrong (eg. a value out of range, going past the end of the buffer). Most importantly, this checking is automatic, so you can’t forget to do it! Implementation in C++The trick to making this all work is to create two stream classes that share the same interface: ReadStream and WriteStream. The write stream implementation writes values using the bitpacker: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; WriteStream( uint8_t buffer, int bytes ) : m_writer( buffer, bytes ) &#123;&#125; bool SerializeInteger( int32_t value, int32_t min, int32_t max ) &#123; assert( min &lt; max ); assert( value &gt;= min ); assert( value &lt;= max ); const int bits = bits_required( min, max ); uint32_t unsigned_value = value - min; m_writer.WriteBits( unsigned_value, bits ); return true; &#125; // …private: BitWriter m_writer;&#125;;``` And the read stream implementation _reads values in_: ```cppclass ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; ReadStream( const uint8_t buffer, int bytes ) : m_reader( buffer, bytes ) &#123;&#125; bool SerializeInteger( int32_t &amp; value, int32_t min, int32_t max ) &#123; assert( min &lt; max ); const int bits = bits_required( min, max ); if ( m_reader.WouldReadPastEnd( bits ) ) &#123; return false; &#125; uint32_t unsigned_value = m_reader.ReadBits( bits ); value = (int32_t) unsigned_value + min; return true; &#125; // …private: BitReader mreader;&#125;; With the magic of C++ templates, we leave it up to the compiler to specialize the serialize function to the stream class passed in, producing optimized read and write functions. To handle safety serialize calls are not actually functions at all. They’re actually macros that return false on error, thus unwinding the stack in case of error, without the need for exceptions. For example, this macro serializes an integer in a given range: 123456789101112131415161718192021222324#define serialize_int( stream, value, min, max ) \ do \ &#123; \ assert( min &lt; max ); \ int32_t int32_value; \ if ( Stream::IsWriting ) \ &#123; \ assert( value &gt;= min ); \ assert( value &lt;= max ); \ int32_value = (int32_t) value; \ &#125; \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ &#123; \ return false; \ &#125; \ if ( Stream::IsReading ) \ &#123; \ value = int32_value; \ if ( value &lt; min || value &gt; max ) \ &#123; \ return false; \ &#125; \ &#125; \ &#125; while (0) If a value read in from the network is outside the expected range, or we read past the end of the buffer, the packet read is aborted. Serializing Floating Point ValuesWe’re used to thinking about floating point numbers as being different to integers, but in memory they’re just a 32 bit value like any other. The C++ language lets us work with this fundamental property, allowing us to directly access the bits of a float value as if it were an integer: 123456789union FloatInt&#123; float float_value; uint32_t int_value;&#125;;FloatInt tmp;tmp.float_value = 10.0f;printf( “float value as an integer: %x\n”, tmp.int_value ); You may prefer to do this with an aliased uint32_t pointer, but this breaks with GCC -O2. Friends of mine point out that the only truly standard way to get the float as an integer is to cast a pointer to the float value to char and reconstruct the integer from the bytes values accessed through the char pointer. Meanwhile in the past 5 years I’ve had no problems in the field with the union trick. Here’s how I use it to serialize an uncompressed float value: 123456789101112131415161718192021template &lt;typename Stream&gt;bool serialize_float_internal( Stream &amp; stream, float &amp; value )&#123; union FloatInt &#123; float float_value; uint32_t int_value; &#125;; FloatInt tmp; if ( Stream::IsWriting ) &#123; tmp.float_value = value; &#125; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) &#123; value = tmp.float_value; &#125; return result;&#125; This is of course wrapped with a serialize_float macro for error checking: 12345678#define serialize_float( stream, value ) \ do \ &#123; \ if ( !serialize_float_internal( stream, value ) ) \ &#123; \ return false; \ &#125; &#125; while (0) We can now transmit full precision floating point values over the network. But what about situations where you don’t need full precision? What about a floating point value in the range [0,10] with an acceptable precision of 0.01? Is there a way to send this over the network using less bits? Yes there is. The trick is to simply divide by 0.01 to get an integer in the range [0,1000] and send that value over the network. On the other side, convert back to a float by multiplying by 0.01. Here’s a general purpose implementation of this basic idea: 12345678910111213141516171819202122232425262728293031template &lt;typename Stream&gt;bool serialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res )&#123; const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) &#123; float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue maxIntegerValue + 0.5f ); &#125; if ( !stream.SerializeBits( integerValue, bits ) ) &#123; return false; &#125; if ( Stream::IsReading ) &#123; const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue delta + min; &#125; return true;&#125; Of course we need error checking, so we wrap this with a macro: 12345678#define serialize_compressed_float( stream, value, min, max ) \ do \ &#123; \ if ( !serialize_float_internal( stream, value, min, max ) ) \ &#123; \ return false; \ &#125; \ &#125; while (0) And now the basic interface is complete. We can serialize both compressed and uncompressed floating point values over the network. Serializing Vectors and QuaternionsOnce you can serialize float values it’s trivial to serialize vectors over the network. I use a modified version of the vectorial library in my projects and implement serialization for its vector type like this: 12345678910111213141516171819202122232425262728template &lt;typename Stream&gt;bool serialize_vector_internal( Stream &amp; stream, vec3f &amp; vector )&#123; float values[3]; if ( Stream::IsWriting ) &#123; vector.store( values ); &#125; serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) &#123; vector.load( values ); &#125; return true;&#125;#define serialize_vector( stream, value ) \ do \ &#123; \ if ( !serialize_vector_internal( stream, value ) ) \ &#123; \ return false; \ &#125; \ &#125; \ while(0) If your vector is bounded in some range, then you can compress it: 123456789101112131415161718192021template &lt;typename Stream&gt;bool serialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res )&#123; float values[3]; if ( Stream::IsWriting ) &#123; vector.store( values ); &#125; serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) &#123; vector.load( values ); &#125; return true;&#125; Notice how we are able to build more complex serialization using the primitives we’re already created. Using this approach you can easily extend the serialization to support anything you need. Serializing Strings and ArraysWhat if you need to serialize a string over the network? Is it a good idea to send a string over the network with null termination? Not really. You’re just asking for trouble! Instead, serialize the string as an array of bytes with the string length in front. Therefore, in order to send a string over the network, we have to work out how to send an array of bytes. First observation. Why waste effort bitpacking an array of bytes into your bit stream just so they are randomly shifted by [0,7] bits? Why not align to byte so you can memcpy the array of bytes directly into the packet? To align a bitstream just work out your current bit index in the stream and how many bits of padding are needed until the current bit index divides evenly into 8, then insert that number of padding bits. For bonus points, pad up with zero bits to add entropy so that on read you can verify that yes, you are reading a byte align and yes, it is indeed padded up with zero bits to the next whole byte bit index. If a non-zero bit is discovered in the padding, abort serialize read and discard the packet. Here’s my code to align a bit stream to byte: 123456789101112131415161718192021222324252627282930void BitWriter::WriteAlign()&#123; const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) &#123; uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); &#125;&#125;bool BitReader::ReadAlign()&#123; const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) &#123; uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; &#125; return true;&#125;#define serialize_align( stream ) \ do \ &#123; \ if ( !stream.SerializeAlign() ) \ return false; \ &#125; while (0) Now we can align to byte prior to writing an array of bytes, letting us use memcpy for the bulk of the array data. The only wrinkle is because the bitpacker works at the word level, it’s necessary to have special handling for the head and tail portions. Because of this, the code is quite complex and is omitted for brevity. You can find it in the sample codefor this article. The end result of all this is a serialize_bytes primitive that we can use to serialize a string as a length followed by the string data, like so: 1234567891011121314151617181920212223242526272829template &lt;typename Stream&gt;bool serialize_string_internal( Stream &amp; stream, char string, int buffer_size )&#123; uint32_t length; if ( Stream::IsWriting ) &#123; length = strlen( string ); assert( length &lt; buffer_size - 1 ); &#125; serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t)string, length ); if ( Stream::IsReading ) &#123; string[length] = ‘\0’; &#125;&#125;#define serialize_string( stream, string, buffer_size ) \do \&#123; \ if ( !serialize_string_internal( stream, \ string, \ buffer_size ) ) \ &#123; \ return false; \ &#125; \&#125; while (0) This is an ideal string format because it lets us quickly reject malicious data, vs. having to scan through to the end of the packet searching for ‘\0’ before giving up. This is important because otherwise protocol level attacks could be crafted to degrade your server’s performance by making it do extra work. Serializing Array SubsetsWhen implemeting a game network protocol, sooner or later you need to serialize an array of objects over the network. Perhaps the server needs to send object state down to the client, or there is an array of messages to be sent. This is straightforward if you are sending all objects in the array - just iterate across the array and serialize each object in turn. But what if you want to send a subset of the array? The simplest approach is to iterate across all objects in the array and serialize a bit per-object if that object is to be sent. If the value of the bit is 1 then the object data follows in the bit stream, otherwise it’s ommitted: 123456789101112131415161718template &lt;typename Stream&gt;bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene )&#123; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) &#123; if ( Stream::IsReading ) &#123; memset( &amp;scene.objects[i], 0, sizeof( Object ) ); &#125; continue; &#125; serialize_object( stream, scene.objects[i] ); &#125; return true;&#125; This approach breaks down as the size of the array gets larger. For example, for an array size of size 4096, then 4096 / 8 = 512 bytes spent on skip bits. That’s not good. Can we switch it around so we take overhead propertional to the number of objects sent instead of the total number of objects in the array? We can but now, we’ve done something interesting. We’re walking one set of objects in the serialize write (all objects in the array) and are walking over a different set of objects in the serialize read (subset of objects sent). At this point the unified serialize function concept starts to breaks down, and in my opinion, it’s best to separate the read and write back into separate functions, because they have so little in common: 12345678910111213141516171819202122232425262728293031323334bool write_scene_b( WriteStream &amp; stream, Scene &amp; scene )&#123; int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( scene.objects[i].send ) num_objects_sent++; &#125; write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( !scene.objects[i].send ) &#123; continue; &#125; write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); &#125; return true;&#125;bool read_scene_b( ReadStream &amp; stream, Scene &amp; scene )&#123; memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) &#123; int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); &#125; return true;&#125; One more point. The code above walks over the set of objects twice on serialize write. Once to determine the number of changed objects and a second time to actually serialize the set of changed objects. Can we do it in one pass instead? Absolutely! You can use another trick, rather than serializing the # of objects in the array up front, use a sentinel value to indicate the end of the array: 1234567891011121314151617181920212223242526272829bool write_scene_c( WriteStream &amp; stream, Scene &amp; scene )&#123; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( !scene.objects[i].send ) &#123; continue; &#125; write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); &#125; write_int( stream, MaxObjects, 0, MaxObjects ); return true;&#125;bool read_scene_c( ReadStream &amp; stream, Scene &amp; scene )&#123; memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) &#123; int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) &#123; break; &#125; read_object( stream, scene.objects[index] ); &#125; return true;&#125; The above technique works great if the objects sent are a small percentage of total objects. But what if a large number of objects are sent, lets say half of the 4000 objects in the scene. That’s 2000 object indices with each index costing 12 bits… that’s 24000 bits or 3000 bytes (almost 3k!) in your packet wasted on indexing. You can reduce this overhead by encoding each object index relative to the previous object index. Think about it, you’re walking from left to right along an array, so object indices start at 0 and go up to MaxObjects - 1. Statistically speaking, you’re quite likely to have objects that are close to each other and if the next index is +1 or even +10 or +30 from the previous one, on average, you’ll need quite a few less bits to represent that difference than an absolute index. Here’s one way to encode the object index as an integer relative to the previous object index, while spending less bits on statistically more likely values: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163template &lt;typename Stream&gt;bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current )&#123; uint32_t difference; if ( Stream::IsWriting ) &#123; assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); &#125; // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) &#123; plusOne = difference == 1; &#125; serialize_bool( stream, plusOne ); if ( plusOne ) &#123; if ( Stream::IsReading ) &#123; current = previous + 1; &#125; previous = current; return true; &#125; // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) &#123; twoBits = difference &lt;= 5; &#125; serialize_bool( stream, twoBits ); if ( twoBits ) &#123; serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) &#123; current = previous + difference; &#125; previous = current; return true; &#125; // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) &#123; threeBits = difference &lt;= 13; &#125; serialize_bool( stream, threeBits ); if ( threeBits ) &#123; serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) &#123; current = previous + difference; &#125; previous = current; return true; &#125; // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) &#123; fourBits = difference &lt;= 29; &#125; serialize_bool( stream, fourBits ); if ( fourBits ) &#123; serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) &#123; current = previous + difference; &#125; previous = current; return true; &#125; // [30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) &#123; fiveBits = difference &lt;= 61; &#125; serialize_bool( stream, fiveBits ); if ( fiveBits ) &#123; serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) &#123; current = previous + difference; &#125; previous = current; return true; &#125; // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) &#123; sixBits = difference &lt;= 125; &#125; serialize_bool( stream, sixBits ); if ( sixBits ) &#123; serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) &#123; current = previous + difference; &#125; previous = current; return true; &#125; // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) &#123; current = previous + difference; &#125; previous = current; return true;&#125;template &lt;typename Stream&gt;bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene )&#123; int previous_index = -1; if ( Stream::IsWriting ) &#123; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( !scene.objects[i].send ) &#123; continue; &#125; write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); &#125; write_object_index( stream, previous_index, MaxObjects ); &#125; else &#123; while ( true ) &#123; int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) &#123; break; &#125; read_object( stream, scene.objects[index] ); &#125; &#125; return true;&#125; But what about the worst case? Won’t we spent more bits when indices are &gt;= +126 apart than on an absolute index? Yes we do, but how many of these worst case indices fit in an array of size 4096? Just 32. It’s nothing to worry about. Protocol IDs, CRC32 and Serialization ChecksWe are nearly at the end of this article, and you can see by now that we are sending a completely unattributed binary stream. It’s essential that read and write match perfectly, which is of course why the serialize functions are so great, it’s hard to desync something when you unify read and write. But accidents happen, and when they do this system can seem like a stack of cards. What if you somehow desync read and write? How can you debug this? What if somebody tries to connect to your latest server code with an old version of your client? One technique to protect against this is to include a protocol id in your packet. For example, it could be a combination of a unique number for your game, plus the hash of your protocol version and a hash of your game data. Now if a packet comes in from an incompatible game version, it’s automatically discarded because the protocol ids don’t match: 12[protocol id] (64bits)(packet data) The next level of protection is to pass a CRC32 over your packet and include that in the header. This lets you pick up corrupt packets (these do happen, remember that the IP checksum is just 16 bits…). Now your packet header looks like this: 123[protocol id] (64bits)[crc32] (32bits)(packet data) At this point you may be wincing. Wait. I have to take 8+4 = 12 bytes of overhead per-packet just to implement my own checksum and protocol id? Well actually, you don’t. You can take a leaf out of how IPv4 does their checksum, and make the protocol id a magical prefix. This means you don’t actually send it, and rely on the fact that if the CRC32 is calculated as if the packet were prefixed by the protocol id, then the CRC32 will be incorrect if the sender does not have the same protocol id as the receiver, thus saving 8 bytes per-packet: [protocol id] (64bits) // not actually sent, but used to calc crc32 [crc32] (32bits) (packet data) One final technique, perhaps as much a check against programmer error on your part and malicious senders (although redundant once you encrypt and sign your packet) is the serialization check. Basically, somewhere mid-packet, either before or after a complicated serialization section, just write out a known 32 bit integer value, and check that it reads back in on the other side with the same value. If the serialize check value is incorrect abort read and discard the packet. I like to do this between sections of my packet as I write them, so at least I know which part of my packet serialization has desynced read and write as I’m developing my protocol. Another cool trick I like to use is to always serialize a protocol check at the very end of the packet, to detect accidental packet truncation (which happens more often than you would think). Now the packet looks something like this: [protocol id] (64bits) // not actually sent, but used to calc crc32 [crc32] (32bits) (packet data) [end of packet serialize check] (32 bits) This is great packet structure to use during development. 译文注意 ：这篇译文对应的是下面的原作者原文旧版本。 译文出处 译者：崔嘉艺（milan21） 审校：陈敬凤(nunu) 在这个系列文章中，我将完全从头开始构建一个专业级别的客户端/服务器游戏网络协议，只使用了C++编译器和一组UDP套接字。如果你正在寻找一个关于如何实现你自己的游戏网络协议方面的详细、实用实现，那么这个系列的文章对你来说就再适合不过了。 大家好，我是Glenn Fiedler，欢迎阅读《构建游戏网络协议》系列教程的第二篇文章。 在前面的文章里，我们讨论了在多人在线网络游戏里面读取和写入网络包的不同方法。我们很快就否决了通过文本的格式比如XML和JSON来发送游戏状态的办法因为它们确实在效率上存在比较大的问题，因此我们决定用自定义的二进制格式进行代替。 我们实现了一个位打包器(bitpacker)，所以我们无需手动将几个布尔变量聚成一个8位比特值(以便为了节省空间)，也无需考虑大端小端问题，可以每次写入一个完整的单词而不需要将单词拆成一个个字符，再考虑如何用字节表示它们，这使得位打包器既非常简单也工作的非常快，也无需考虑与平台有关的细节。 但是我们仍然遗留了以下这些问题需要解决： 我们需要实现一个方法来判断整数值是否超出预期范围，如果超出了就要中止网络包的读取和解析，因为会有一些不怀好意的人给我们发送恶意网络包希望我们的程序和内存崩溃掉。网络包的读取和解析的中止必须是自动化的，而且不能使用异常处理，因为异常处理太慢了会拖累我们的程序。 如果独立的读取和写入函数是手动编解码的，那么维护它们真的是一个噩梦。我们希望能够为包一次性的编写好序列化代码并且没有任何运行时的性能消耗（主要是额外的分支、虚化等等）。 我们应该如何实现上面的这些目标? 请继续阅读，我将向你展示如何用Ｃ＋＋来实现这些功能。开发和完善这些技术花费了我不少时间，所以我希望这些内容对你来说是有帮助的，至少是一个很好的选择值得考虑是否要替换你目前采用的方案，或者可以与你在其他游戏看到的这个问题解决方案相结合，看是否能得到更好的解决方案。 统一的数据包序列化功能让我们从我们的目标开始。这就是我们在本文结束的时候希望得到的东西： 1234567891011121314151617181920212223242526272829303132333435363738394041struct PacketA&#123; int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; &#125;&#125;; struct PacketB&#123; int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) serialize_bits( buffer, elements[i], 32 ); return true; &#125;&#125;; struct PacketC&#123; bool x; short y; int z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_int( stream, x, 8 ); serialize_int( stream, y, 16 ); serialize_int( stream, z, 32 ); return true; &#125;&#125;; 看下上面的代码片段，可以看到每个数据包结构里面都只有一个单独的序列化函数，而不是有互相独立的序列化读取和序列化写入函数。这非常的棒！它把整个序列化代码一分为二，你可能需要做很多努力来实现序列化读取和写入（因为读取的过程是数据解析并装入本地内存，写入的的过程是将本地的数据写到消息体，会有比较大的差异，所以代码基本是一分为二，一半用于读取，一半用于写入）。 如果要让这项工作变得有效，诀窍在于让流类型的序列化函数模板化。在我的系统中有两个流类型：ReadStream类和WriteStream类。每个类都有相同的一套方法，但实际上它们没有任何关系。一个类负责从比特流读取值到变量中，另外一个类负责把变量的值写到流中。ReadStream和WriteStream只是上一篇文章中BitReader和BitWriter类的一个高层次封装。 当然也有其他方法可以用来代替。如果你不喜欢用模板的话，你可以使用一个纯虚的基类作为流的接口，然后分别实现读取和写入类来实现这个流接口。但是如果你这么做的话，就要在每个序列化调用的时候发生了一次虚函数调用。这种方法对我来说开销似乎比较大。 实现这个功能的另外一种方法是实现一个超级棒的流类型，可以通过配置而在运行时进入读取或者写入模式。这种方法会比虚函数方法快一些，但是仍然会在每次序列化调用的时候存在分支判断到底应该是读还是写，所以它不如硬编码读取和写入函数那么快。 我更喜欢模板方法，因为它可以让编译器为项目产生经过优化的读取/写入函数。你甚至可以把序列化代码也这样实现以便让编译器为特定的读取和写入优化一大堆东西： 123456789101112131415161718192021222324252627282930struct RigidBody&#123; vec3f position; quat3f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? velocity.length() == 0 : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) &#123; serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); &#125; else if ( Stream::IsReading ) &#123; linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); &#125; return true; &#125;&#125;; 虽然这看起来很没有效率，但是实际上并不是！这个函数经过模板特化以后会根据流的类型优化了所有分支。这很整齐漂亮吧？ 而ReadStream和WriteStream是这样的 ： 1234567891011121314151617class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; // ...&#125;;class ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; // ..&#125; 边界检查和终止读取现在我们通过编译器实现了生成优化过的序列化读取/写入函数，我们还需要一些方法来做序列化读取时候的自动错误检测以便让我们不受恶意网络包的影响。 我们要做的第一件事情是把允许的大小范围也传给序列化函数而不仅仅是所需的比特数量。试想一下如果有了最小和最大的范围，序列化函数就能自己算出所需的比特数量： 1serialize_int(stream, numElements, 0, MaxElements ); 这种做法开辟了一类新的方法，可以非常容易地支持带符号整数的序列化并且序列化函数可以检测从网络中读取的值并确保这个值一定在期望的范围内。如果这个值超出范围了，就立即中止序列化读取并丢弃这个数据包。 因为我们没有办法使用异常来处理这种中止（因为异常太慢了），所以上面的方式是我比较喜欢的处理方式。 在我的环境中，serialize_int其实并不是一个函数，它实际是一个如下面代码所示的宏： 1234567891011121314151617181920#define serialize_int( stream, value, min, max) \ do \ &#123; \ assert( min &lt; max); \ int32_tint32_value; \ if ( Stream::IsWriting) \ &#123; \ assert( value &gt;= min); \ assert( value &lt;= max); \ int32_value = (int32_t)value; \ &#125; \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ return false; \ if ( Stream::IsReading) \ &#123; \ value =int32_value; \ if ( value &lt; min || value &gt; max) \ return false; \ &#125; \ &#125; while (0) 我让人觉得恐怖害怕的原因是我竟然使用了宏来插入代码来检测SerializeInteger函数的结果以及在发生错误的时候返回false。这会让人感觉到这种行为和异常处理很像，它会在出错的时候回溯堆栈到序列化调用堆栈的最顶上，但是这种处理不会带来任何的问题比如性能的消耗。在回溯的时候出现分支是非常罕见的（序列化错误非常少见）所以分支预测应该不会带来什么性能上的问题。 还有一种情况我们也需要中止序列化读取：如果流读取超出了结尾。这种情况其实也是非常罕见的，但是我们必须在每次序列化操作都进行这个检查，这是因为流读取超出结尾会造成的影响是未定义的（也就是说我们对于它能造成什么样子的结果完全是未知的，最糟糕的情况并不是代码崩溃，而是把我们的内容数据完全搞乱了，相信我，你会无比痛恨这件事情）。如果我们没有做这个检测，可能会出现程序无限循环的情况，因为读取的位置超出了缓冲区的结尾。虽然在读取的时候如果发现超出比特流结尾的时候返回0值是很常见的做法（如以前的文章提到的那样），但是返回0值也不能保证序列化函数在有循环的时候能够正确的中止。如果要确保程序是有良好定义的行为，那么这种缓冲溢出检测总是必须的。 最后一点，在序列化写入的时候如果遇到范围检测失败或者写入的地址超出流的结尾的时候，我并没有采用中止这种做法。在写入数据的时候你可能会轻松很多，因为如果有任何事情出错了，那几乎肯定是你自己导致的错误。在序列化写入的时候我们只是对每个序列化写入做了断言来确保一切是符合预期的（在范围内、写入的地址没有超出流的结尾），其他的一切都任由你来发挥。 序列化浮点数和向量这个比特流现在只序列化类型整数的值。如果我们要序列化一个类型为浮点数的值，我们该怎么做？ 我们的做法虽然看上去有点投机取巧但实际上并不是。在内存中浮点数也是像整数那样保存成一个32位的值。你的计算机根本不知道内存中的这个32位的值到底是一个整数还是一个浮点数还是一个字符串的部分。它知道的就是这仅仅是一个32位的值。幸运的是，C++语言使得我们可以直接对这个基础属性进行控制（其他语言不行，因为底层被封装掉了，这也是C++被认为不好的地方之一，很多现代语言都禁止了这种做法）。 你可以通过一个联合体来访问看上去是整数的浮点数： 123456789union FloatInt&#123; float float_value; uint32_t int_value;&#125;; FloatInt tmp;tmp.float_value= 10.0f;printf(“float value as an integer: %x\n”, tmp.int_value ); 你也可以通过别名uint32_t的指针来做到这一点，但是因为GCC -O2会导致这种做法有些性能问题，所以我更倾向于使用联合体这种做法。我的朋友们指出（很有可能是正确的）从一个整数类型的值转换到浮点值的唯一真正标准的做法是将浮点数指针转换成uint8_t指针然后通过这个字节指针来分别引用4个字节来对这个值进行重建。虽然这对我来说似乎是一个非常愚蠢的做法。女士们，先生们。。。这毕竟是C++啊！（作者的意思是C++提供了很多接触底层的方法，我们可以尽情利用这一优势，只要能保证结果是正确的就可以了，哪怕使用一些取巧的办法也无所谓！）。 与此同时，在过去的5年里，在使用联合体这个技巧方面我还没有遇到过什么问题。下面是我如何序列化一个未压缩的浮点值： 123456789101112131415161718192021template &lt;typename Stream&gt;bool serialize_float_internal( Stream &amp; stream, float &amp; value )&#123; union FloatInt &#123; float float_value; uint32_t int_value; &#125;; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;&#125; 通过一个serialize_float宏来包装这个部分以方便在序列化读取的时候方便进行一致的错误检测： 123456#define serialize_float( stream, value) \ do \ &#123; \ if ( !protocol2::serialize_float_internal( stream, value )) \ return false; \ &#125; while(0) 有些时候，你并不想把一个完整精度的浮点数进行传递。那么该如何压缩这个浮点值？第一步是将它的值限制在某个确定的范围内然后用一个整数表示方式来将它量化。 举个例子来说，如果你知道一个浮点类型的值是在区间[-10,+10]，对于这个值来说可以接受的精确度是0.01，那么你可以把这个浮点数乘以100.0让它的值在区间[-1000,+1000]并在网络上将其作为一个整数进行序列化。而在接收的那一端，仅仅需要将它除以100.0来得到最初的浮点值。 下面是这个概念用序列化实现的版本： 12345678910111213141516171819202122232425262728293031323334template &lt;typename Stream&gt;boolserialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res )&#123; const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) &#123; float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue maxIntegerValue + 0.5f ); &#125; if ( !stream.SerializeBits( integerValue, bits ) ) return false; if ( Stream::IsReading ) &#123; const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue delta + min; &#125; return true;&#125; 一旦你实现了对浮点数的序列化，那么将方法拓展下通过网络序列化向量和四元数就非常容易了。我在我自己的项目中使用了这个超赞的针对向量数学的向量库（https://github.com/scoopr/vectorial）的一个修改版本，并且我对这些类型实现的序列化方法如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546template &lt;typename Stream&gt;boolserialize_vector_internal( Stream &amp; stream, vec3f &amp; vector )&#123; float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) vector.load( values ); return true;&#125; template &lt;typename Stream&gt;boolserialize_quaternion_internal( Stream &amp; stream, quat4f &amp; quaternion )&#123; float values[4]; if ( Stream::IsWriting ) quaternion.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); serialize_float( stream, values[3] ); if ( Stream::IsReading ) quaternion.load( values ); return true;&#125; #defineserialize_vector( stream, value) \ do \ &#123; \ if ( !serialize_vector_internal( stream, value )) \ return false; \ &#125; \ while(0) #defineserialize_quaternion( stream, value) \ do \ &#123; \ if ( !serialize_quaternion_internal( stream, value ) ) \ return false; \ &#125; \ while(0) 如果你知道你的向量的取值会限制在某个范围内，你可以像下面这样对它进行压缩： 123456789101112131415161718192021222324252627282930313233template &lt;typename Stream&gt; bool serialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res ) &#123; float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) vector.load( values ); return true; &#125; 你如果想要在网络上压缩一个方向，不要把它视为四个取值范围在[-1,+1]的成员变量的结构。如果使用这个四元数的三个最小值来表示它效果会好的多，请看下这篇文章的示例代码(地址在https://www.patreon.com/gafferongames?ty=h)来得到一个这方面的实现。 四元数是简单的超复数。 复数是由实数加上虚数单位 i 组成，其中i^2 = -1。 相似地，四元数都是由实数加上三个虚数单位 i、j、k 组成，而且它们有如下的关系： i^2 = j^2 = k^2 = -1， i^0 = j^0 = k^0 = 1 , 每个四元数都是 1、i、j 和 k 的线性组合，即是四元数一般可表示为a + bk+ cj + di，其中a、b、c 、d是实数。 对于i、j、k本身的几何意义可以理解为一种旋转，其中i旋转代表X轴与Y轴相交平面中X轴正向向Y轴正向的旋转，j旋转代表Z轴与X轴相交平面中Z轴正向向X轴正向的旋转，k旋转代表Y轴与Z轴相交平面中Y轴正向向Z轴正向的旋转，-i、-j、-k分别代表i、j、k旋转的反向旋转。 序列化字符串和数组如果你想序列化字符串并通过网络传输该怎么办？ 在网络上发送字符串的时候用Null作为终止符是个好主意么？我不这么认为。我认为这么做只是在自找麻烦！我们应该把字符串作为带长度作为前缀的字符数组。所以，要通过网络发送字符串，我们必须解决如何有效的发送字符数组的问题。 观察到的第一个事情：为什么要费那么大精力把一个字节数组按比特打包到你的比特流里?只是为了让它们随机的偏移[0,7]比特？为什么不在序列化写入之前进行按字节进行对齐？Why not align to byte so you can memcpy the array of bytes directly into the packet?如果这么处理的话，数据包里面的字节数组数据就很对齐的很准，数组的每个字节都对应着数据包里面的一个实际字节。对于每个要序列化的字节数组，你只损失了[0,7]个比特，这取决于对齐的方式，但是以我的观点来看这没什么好在意的。 如何将比特流按字节对齐？只需要在流的当前位置做些计算就可以了，找出还差写入多少个比特就能让当前比特流的比特数量被8整除，然后按照这个数字插入填充比特（比如当前比特流的比特数量是323，那么323+5才能被8整除，所以需要插入5个填充比特）。对于填充比特来说，填充的比特值都是0，这样当你序列化读取的时候你可以进行检测，如果检测的结果是正确的，那么就确实是在读取填充的部分，并且填充的部分确实是0。一直读取到下一个完整字节的比特起始位置（可以被8整除的位置）。如果检测的结果是在应该填充的地方发现了非0的比特值，那么就中止序列化读取并丢弃这个数据包。 下面是我用来将比特流按比特对齐的代码： 123456789101112131415161718192021222324252627282930void BitWriter::WriteAlign()&#123; const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) &#123; uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); &#125;&#125; bool BitReader::ReadAlign()&#123; const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) &#123; uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; &#125; return true;&#125; #define serialize_align( stream) \ do \ &#123; \ if ( !stream.SerializeAlign() ) \ return false; \ &#125; while(0) 现在我们可以使用这个对齐操作来有效率的将字节数组写入比特流：因为我们已经将比特流按照字节对齐了，所以我们可以使用memcpy方法来做大部分的工作。唯一的问题在于比特读取器和比特写入器是按照双字进行工作的，所以需要一些特殊的代码来处理字节数组的头部和尾部，以确保头部的零散比特会被写入内存，并且在头部处理完毕以后，读取的位置会被正确设置到下一个字节。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798void BitWriter::WriteBytes( const uint8_t data, int bytes )&#123; assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8|| ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) &#123; assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data+headBytes, numWords4 ); m_bitsWritten += numWords 32; m_wordIndex += numWords; m_scratch = 0; &#125; assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords 4 + tailBytes == bytes );&#125; void ReadBytes( uint8_t data, int bytes )&#123; assert( GetAlignBits() == 0 ); assert( m_bitsRead + bytes 8 &lt;= m_numBits ); assert( ( m_bitsRead % 32 ) == 0 || ( m_bitsRead % 32 ) == 8 || ( m_bitsRead % 32 ) == 16 || ( m_bitsRead % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsRead % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) data[i] = ReadBits( 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) &#123; assert( ( m_bitsRead % 32 ) == 0 ); memcpy( data + headBytes, &amp;m_data[m_wordIndex], numWords 4 ); m_bitsRead += numWords 32; m_wordIndex += numWords; m_scratchBits = 0; &#125; assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) data[tailStart+i] = ReadBits( 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords 4 + tailBytes == bytes );&#125; template &lt;typename Stream&gt;bool serialize_bytes_internal( Stream &amp; stream, uint8_t data, int bytes )&#123; return stream.SerializeBytes( data, bytes );&#125; #define serialize_bytes( stream, data, bytes) \ do \ &#123; \ if ( !serialize_bytes_internal( stream, data, bytes ) ) \ return false; \ &#125; while(0) 现在，我们可以通过先序列化字符串长度然后序列化字符串数据的方法来序列化一个字符串： 123456789101112131415161718192021222324template &lt;typename Stream&gt;bool serialize_string_internal(Stream &amp; stream, char string, int buffer_size )&#123; uint32_t length; if ( Stream::IsWriting ) &#123; length = strlen( string ); assert( length &lt; buffer_size - 1 ); &#125; serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) string[length] = ‘\0’;&#125; #define serialize_string( stream, string, buffer_size) \do \&#123; \ if ( !serialize_string_internal(stream, \ string,buffer_size ) ) \ return false; \&#125; while (0) 正如你看到的那样，可以从基本元素的序列化开始构建一个相当复杂的序列化体系。 序列化数组的子集当实现一个游戏网络协议的时候，或早或晚总会需要序列化一个对象数组然后在网络上传递。比如说服务器也许需要把所有的物体发送给客户端，或者有时候需要发送一组事件或者消息。如果你要发送所有的物体到客户端，这是相当简单直观的，但是如果你只是想发送一个数组的一个子集怎么办？ 最先想到也是最容易的办法是遍历数组的所有物体然后序列化一个bool数组，这个bool数组标记的是对应的物体是否通过网络发送。如果bool值为1那么后面会跟着物体的数据，否则就会被忽略然后下一个物体的bool值取决于流的下一个值。 12345678910111213141516171819template &lt;typename Stream&gt;bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene )&#123; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) &#123; if ( Stream::IsReading ) memset( &amp;scene.objects[i], 0, sizeof( Object ) ); continue; &#125; serialize_object( stream, scene.objects[i] ); &#125; return true;&#125; 但是如果物体的数组很大怎么办？举个例子，比如场景中有4000个物体。4000 / 8 = 500。光是标记物体是否发送的BOOL数组就要500个字节的开销，即使你只发送了一两个物体也是这样！这种方法。。。。不是太好。所以我们是否能够找到一种办法来让额外的开销正比于发送的物体数目而不是正比于数组中的物体数目？ 我们可以找到这么一个方法，但是现在我们已经做了一些有意思的事情。我们在序列化写入的时候遍历一个物体的集合（数组里面的所有物体）但是序列化读取的时候遍历的是一个不同的物体集合（发送物体数组的子集）。在这一点上统一的序列化函数概念就不能维系了。对于这种情况最好是把读取和写入分解成单独的函数： 123456789101112131415161718192021222324252627282930313233343536373839bool write_scene_b( protocol2::WriteStream &amp; stream, Scene &amp; scene )&#123; int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( scene.objects[i].send ) num_objects_sent++; &#125; write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); &#125; return true;&#125; bool read_scene_b( protocol2::ReadStream &amp; stream, Scene &amp; scene )&#123; memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) &#123; int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); &#125; return true;&#125; 此外，你可以用发生变化的对象集合来生成一个单独的数据结构，并且针对发生变化的对象集合实现序列化。但是对每个你期望能够序列化的数据结构都产生C++代码对应的数据结构体是一件非常痛苦的事情。最终你可能想要同时遍历几个数据结构然后高效的将一个动态数据结构写入比特流。这在写一些更高级的序列化方法比如增量编码的时候是一种非常平常的做法。只要你采用了这种做法，统一序列化这种做法就不再有什么意义。 我对此的建议是如果任何时候你想这么做，那么请不要担心，就把序列化读取和序列化写入分开好了。将序列化读取和序列化写入统一起来是一种非常简单的方式，但是这种方式带来的简单易用与序列化写入时动态生成数据结构的痛苦相比是不划算的。我的经验是复杂的序列化功能有时候可能会需要单独的序列化读取和序列化写入功能，但是如果可能的话，尽量让具体的序列化函数是统一读取和写入的（举个例子来说，实际的物体和事件无论何时序列化都尽量保持序列化读取和序列化写入是统一的）。 多说一点。上面的代码在一次序列化写入的时候对物体集合进行了两次遍历。一次遍历用来确定发生变化的物体数目，第二次遍历用来对发生变化的物体集合进行实际的序列化。我们是否能只用一次遍历就能处理好发生变化的物体集合的序列化？当然可以！你可以使用另外一个技巧，用一个哨兵值（sentinel value）来标记数组的结尾位置，而不是一直序列化数组中的物体直到遇到#。使用这种方法你可以在发送的时候只遍历整个数组一遍，当没有更多物体需要发送的时候，就把哨兵值序列化进数据包以表示数组结束了： 1234567891011121314151617181920212223242526272829bool write_scene_c( protocol2::WriteStream &amp; stream, Scene &amp; scene )&#123; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); &#125; write_int( stream, MaxObjects, 0, MaxObjects ); return true;&#125; bool read_scene_c( protocol2::ReadStream &amp; stream, Scene &amp; scene )&#123; memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) &#123; int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); &#125; return true;&#125; 这种做法非常的简单，并且在发送的物体集合相比较全部物体集合比例非常小的时候工作的很棒。但是如果有大量的物体需要发送，举个例子来说，整个场景中有4000个物体，有一半的物体也就是2000个需要通过网络进行发送。每个物体需要一个序号，那么就需要2000个序号，每个序号需要12比特。。。。这就是说数据包里面24000比特或者说接近30000比特（几乎是30000，不是严格是，译注：原文如此）的数据被序号浪费掉了。 可以把序号的编码方式修改下来节省数据，序号不再是全局序号，而是相对上一个物体的相对序号。想下这个问题，我们从左到右遍历一个数组，所以数组中物体的序号从0开始并且逐步增大到MaxObjects – 1。从统计学的角度来说，要发送的物体有可能是挨着很近的，这样下一个序号可能就是+1或者+10再或者是+30这样的小数字，因为我们这里用的序号是相对上一个发送的物体的，所以数字从统计意义上来说都会比较小，所以平均来讲，相比较之前的解决方案你可能需要更少的比特来表示物体的序号。（其实最差情况下我们所需的比特位也只是和前一个方案相同而已，可以证明每个序号，后一方案都比前一方案的要小，那么每个序号花费的比特位无疑不会更多，但是这种方案的主要问题在于健壮性，需要确保关于数据集合的数据包中间都不能丢，一旦中间某个包被丢掉了，那么后面的解析就完全乱掉了，实现起来更加困难一些)。 下面就是这么一种编码物体序号的方式，每个序号都是相对上一个物体序号而言的，不再是全局序号，从统计的角度来讲它们会消耗更少的比特位（但是如果非常大的集合，但是发送的数组所占的比例很小，那么两种方法的差异其实是比较小的）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133template &lt;typename Stream&gt;bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current )&#123; uint32_t difference; if ( Stream::IsWriting ) &#123; assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); &#125; // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) plusOne = difference == 1; serialize_bool( stream, plusOne ); if ( plusOne ) &#123; if ( Stream::IsReading ) current = previous + 1; previous = current; return true; &#125; // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) twoBits = difference &lt;= 5; serialize_bool( stream, twoBits ); if ( twoBits ) &#123; serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; &#125; // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) threeBits = difference &lt;= 13; serialize_bool( stream, threeBits ); if ( threeBits ) &#123; serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; &#125; // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) fourBits = difference &lt;= 29; serialize_bool( stream, fourBits ); if ( fourBits ) &#123; serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; &#125; //[30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) fiveBits = difference &lt;= 61; serialize_bool( stream, fiveBits ); if ( fiveBits ) &#123; serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; &#125; // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) sixBits = difference &lt;= 125; serialize_bool( stream, sixBits ); if ( sixBits ) &#123; serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; &#125; // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true;&#125; template &lt;typename Stream&gt;bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene )&#123; int previous_index = -1; if ( Stream::IsWriting ) &#123; for ( int i = 0; i &lt; MaxObjects; ++i ) &#123; if ( !scene.objects[i].send ) continue; write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); &#125; write_object_index( stream, previous_index, MaxObjects ); &#125; else &#123; while ( true ) &#123; int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); &#125; &#125; return true;&#125; 通常情况下，这将节省大量的带宽，因为要发送的物体的序号往往倾向于聚在一起。在这种情况下，如果下一个物体也要被发送，那么它的的序号就是+1只需要一个比特大小。如果是+2到+5的情况每个序号需要5个比特。平均下来序号所占的数据大小方面可以降低2-3倍。但是要注意的是如果是间隔序号比较大的序号的消耗将比不相关序号编码方案（每个序号都是占12比特空间）要大。这看上去非常糟糕，但是实际上并不会这么差，试想一下，即使你遇到了“最差情况”（要发送的物体的序号间隔均匀都是相差128），那么在一个4000物体的大数组里面你实际才发送几个物体？只有32个而已，所以不用担心这个问题! 协议ID和CRC32和序列化检测阅读到这里，你可能会有一个疑惑。“哦哦哦，整个体系看上去非常脆弱啊，只有一个完全不带任何属性信息的二进制流。流里面只有一个个数据协议。你该怎么对这些信息进行反序列化读取和写入？如果某些人发送一些包含随机信息的数据包给你的服务器。你会不会在解析的时候把服务器弄崩溃掉？” 确实大部分游戏服务器就是这样工作的，但是我有个好消息告诉你和其他之前是这么做服务器的人，存在这样的技术可以减少或者几乎杜绝由于序列化层传过来的数据导致的崩溃可能性。 第一种技术是在你的数据包里面包含协议ID。一般典型的做法是，头4个字节你可以设定一些比较罕见而且独特的值，比如0x12345678，反正是这种其他人不会想着去使用的值就好了。但是说真的，把你的序列ID和协议版本的数字用散列得到一个散列值放到每个数据包的前面32比特的位置，这种方法真的工作的很好。至少如果是其他应用程序的数据包发送到了你的端口（要记住，UDP的数据包可以从任何IP任何端口在任何时间发送过来），你可以通过这３２比特的数据判断出来根本就不是你的应用程序的包，然后就可以直接丢弃了。 12[protocol id] (32bits)(packet data) 下一个级别的防护是对你的数据包整体做一个CRC32的校验，并把这个校验码放到数据包的包头。这可以让你在接收的时候偶然会放过一些错误的数据包进来处理（这确实是会发生，IP的校验和是16位的，所以一堆东西不会使用16位的校验和。。。其实是通过协议ID来避免这种小概率事件的）。现在你的数据包头文件看起来像下面这样： 12345[protocol id](32bits)[crc32](32bits)(packet data) 如果你按着这个顺序做下来的话，现在你可能会有点畏惧。”请等一下，我需要为每个数据包花费8个额外的字节来实现我自己的校验和以及协议ID么？“事实上，你可以不这么做。你可以学习下看看IPv4是如何进行校验的，并让协议ID变成一个魔术前缀(Magical Prefix)。也就是说你可以不发送这个协议ID，但是发送方和接收方提前确认过这个协议ID是什么，并在计算数据包CRC32值的时候装作这个数据包带上了这个协议ID的前缀来参与计算。这样如果发送方使用的协议ID与接收方不一致的时候，CRC32的校验就会失败，这将为每个数据包节省4个字节： 12345[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data) 当然，CRC32只是防止有些随机的数据包误打误撞的情况，但是对于可以轻易修改或者构建恶意数据包的头4个字节以便修正CRC32值的那些恶意发送者来说它起不到什么防护作用。要防止那些恶意发送者，你需要使用一个保密性更好的密码哈希函数，同时还需要一个密钥，这个密钥最好是在客户端尝试登陆游戏服务器之前就通过HTTPS协议在客户端和服务器之间统一好(而且要确保每个客户端的密钥都不一样，只有服务器和对应的客户端才知道密钥是什么)。 最后一项技术，也可能是最有效的阻止恶意发送者的技术了（虽然会导致数据包的加密和签名有很多冗余信息），这就是序列化检查（serialization check）。这个技术基本上来说是在包的中间，在一段复杂的序列化写入之前或者之后写上一个已知的32比特整数，并在另外一端序列化读取的时候用相同的值进行检测判断。如果序列化检查值是不正确的，那么就中止序列化读取并丢弃这个数据包。 我喜欢在我的数据包每个部分之间写入一些序列化检查值，这样我至少知道我的数据包那部分已经被成功的序列化读取和写入（有些问题无论你如何努力避免都很难完全避免的）。我喜欢使用的另外一个很酷的技巧是在数据包的结尾序列化一个协议检查值，这非常非常的有用，因为它能够帮我判断是否遇到了数据包截断（非常像上一篇文章最后提到的臭名昭著的大端截断和小端截断，在开发的时候也是很让人头疼的地方）。 所以现在网络包看起来应该是像这样： 1234567[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data)[end of packet serialize check] (32 bits) 如果你喜欢的话，你可以把这些协议编译出来然后在你的发布版本中检查这些数据包的内容，特别是在有非常棒的数据包加密和数据包签名支持的情况下，不过不编译也没关系，反正不再需要它们了。 下一篇预告：数据包的分包和重组 请继续阅读这个系列的下一篇文章，在这篇文章里我将向大家介绍如何拓展本章中实现的网络协议来实现数据包的分包和重组以确保你的网络包的大小在MTU限制以下。 MTU：最大_传输_单元，Maximum Transmission Unit，是指一种通信协议的某一层上面所能通过的最大数据包大，以字节为单位。最大传输单元这个参数通常与通信接口有关，比如网络接口卡、串口等。因为协议数据单元的包头和包尾的长度是固定的，MTU越大，则一个协议数据单元的承载的有效数据就越长，通信效率也越高。MTU越大，传送相同的用户数据所需的数据包个数也越低。MTU也不是越大越好，因为MTU越大， 传送一个数据包的延迟也越大；并且MTU越大，数据包中 bit位发生错误的概率也越大。MTU越大，通信效率越高而传输延迟增大，所以要权衡通信效率和传输延迟选择合适的MTU。以以太网传送IPv4报文为例。MTU表示的长度包含IP包头的长度，如果IP层以上的协议层发送的数据报文的长度超过了MTU，则在发送者的IP层将对数据报文进行分片，在接收者的IP层对接收到的分片进行重组。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议一之数据包的读取和写入]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%9E%84%E5%BB%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%B8%80%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%8C%85%E7%9A%84%E8%AF%BB%E5%8F%96%E5%92%8C%E5%86%99%E5%85%A5%2F</url>
    <content type="text"><![CDATA[自我总结这篇文章只是介绍, 之后的文章才是正题. 此篇文章大体介绍了 : 文本格式传输的低效率问题， 为了可读性而产生了太多冗余无用数据 为什么不用目前已经有了的库比如Protocol Buffers：因为我们不需要版本信息，也不需要什么跨语言的支持。所以让我们直接忽略掉这些功能并用我们自己的不带属性的二进制流进行代替，在这个过程中我们可以获得更多的控制性和灵活性 要注意大小端的问题 实现一个位打包器， 工作在32位或者64位的级别， 而不是是工作在字节这个级别。因为现代机器对这个长度进行了专门的优化而不应该像1985年那样在字节的级别对缓冲区进行处理。 要注意防止恶意数据包的问题 ： 我们需要实现一个方法来判断整数值是否超出预期范围，如果超出了就要中止网络包的读取和解析，因为会有一些不怀好意的人给我们发送恶意网络包希望我们的程序和内存崩溃掉。网络包的读取和解析的中止必须是自动化的，而且不能使用异常处理，因为异常处理太慢了会拖累我们的程序。 如果独立的读取和写入函数是手动编解码的，那么维护它们真的是一个噩梦。我们希望能够为包一次性的编写好序列化代码并且没有任何运行时的性能消耗（主要是额外的分支、虚化等等）。 我们为了不想自己手动检查各种可能会被攻击的地方， 需要实现检查自动化， 在下一篇文章 构建游戏网络协议二之序列化策略 里将会说。. . . 原文原文出处 原文标题 : Reading and Writing Packets (Best practices for reading and writing packets) IntroductionHi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In this article we’re going to explore how AAA multiplayer games like first person shooters read and write packets. We’ll start with text based formats then move into binary hand-coded binary formats and bitpacking. At the end of this article and the next, you should understand exactly how to implement your own packet read and write the same way the pros do it. BackgroundConsider a web server. It listens for requests, does some work asynchronously and sends responses back to clients. It’s stateless and generally not real-time, although a fast response time is great. Web servers are most often IO bound. Game server are different. They’re a headless version of the game running in the cloud. As such they are stateful and CPU bound. The traffic patterns are different too. Instead of infrequent request/response from tens of thousands of clients, a game server has far fewer clients, but processes a continuous stream of input packets sent from each client 60 times per-second, and broadcasts out the state of the world to clients 10, 20 or even 60 times per-second. And this state is huge. Thousands of objects with hundreds of properties each. Game network programmers spend a lot of their time optimizing exactly how this state is sent over the network with crazy bit-packing tricks, hand-coded binary formats and delta encoding. What would happen if we just encoded this world state as XML? 1234567891011121314151617181920&lt;world_update world_time=”0.0”&gt; &lt;object id=”1” class=”player”&gt; &lt;property name=”position” value=”(0,0,0)”&lt;/property&gt; &lt;property name=”orientation” value=”(1,0,0,0)”&lt;/property&gt; &lt;property name=”velocity” value=”(10,0,0)”&lt;/property&gt; &lt;property name=”health” value=”100”&gt;&lt;/property&gt; &lt;property name=”weapon” value=”110”&gt;&lt;/property&gt; … 100s more properties per-object … &lt;/object&gt; &lt;object id=”100” class=”grunt”&gt; &lt;property name=”position” value=”(100,100,0)”&lt;/property&gt; &lt;property name=”health” value=”10”&lt;/property&gt; &lt;/object&gt; &lt;object id=”110” class=”weapon”&gt; &lt;property type=”semi-automatic”&gt;&lt;/property&gt; &lt;property ammo_in_clip=”8”&gt;&lt;/property&gt; &lt;property round_in_chamber=”true”&gt;&lt;/property&gt; &lt;/object&gt; … 1000s more objects …&lt;/world_update&gt; Pretty verbose… it’s hard to see how this would be practical for a large world. JSON is a bit more compact: 12345678910111213141516171819202122232425&#123; “world_time”: 0.0, “objects”: &#123; 1: &#123; “class”: “player”, “position”: “(0,0,0)”, “orientation”: “(1,0,0,0)”, “velocity”: “(10,0,0)”, “health”: 100, “weapon”: 110 &#125; 100: &#123; “class”: “grunt”, “position”: “(100,100,0)”, “health”: 10 &#125; 110: &#123; “class”: “weapon”, “type: “semi-automatic” “ammo_in_clip”: 8, “round_in_chamber”: 1 &#125; // etc… &#125;&#125; But it still suffers from the same problem: the description of the data is larger than the data itself. What if instead of fully describing the world state in each packet, we split it up into two parts? A schema that describes the set of object classes and properties per-class, sent only once when a client connects to the server. Data sent rapidly from server to client, which is encoded relative to the schema. The schema could look something like this: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; “classes”: &#123; 0: “player” &#123; “properties”: &#123; 0: &#123; “name”: “position”, “type”: “vec3f” &#125; 1: &#123; “name”: “orientation”, “type”: “quat4f” &#125; 2: &#123; “name”: “velocity”, “type”: “vec3f” &#125; 3: &#123; “name”: “health”, “type”: “float” &#125; 4: &#123; “name”: “weapon”, “type”: “object”, &#125; &#125; &#125; 1: “grunt”: &#123; “properties”: &#123; 0: &#123; “name”: “position”, “type”: “vec3f” &#125; 1: &#123; “name”: “health”, “type”: “float” &#125; &#125; &#125; 2: “weapon”: &#123; “properties”: &#123; 0: &#123; “name”: “type”, “type”: “enum”, “enum_values”: [ “revolver”, “semi-automatic” ] &#125; 1: &#123; “name”: “ammo_in_clip”, “type”: “integer”, “range”: “0..9”, &#125; 2: &#123; “name”: “round_in_chamber”, “type”: “integer”, “range”: “0..1” &#125; &#125; &#125; &#125;&#125; The schema is quite big, but that’s beside the point. It’s sent only once, and now the client knows the set of classes in the game world and the number, name, type and range of properties per-class. With this knowledge we can make the rapidly sent portion of the world state much more compact: 12345678&#123; “world_time”: 0.0, “objects”: &#123; 1: [0,”(0,0,0)”,”(1,0,0,0)”,”(10,0,0)”,100,110], 100: [1,”(100,100,0)”,10], 110: [2,1,8,1] &#125;&#125; And we can compress it even further by switching to a custom text format: 12340.0 1:0,0,0,0,1,0,0,0,10,0,0,100,110 100:1,100,100,0,10 110:2,1,8,1 As you can see, it’s much more about what you don’t send than what you do. The Inefficiencies of TextWe’ve made good progress on our text format so far, moving from a highly attributed stream that fully describes the data (more description than actual data) to an unattributed text format that’s an order of magnitude more efficient. But there are inherent inefficiencies when using text format for packets: We are most often sending data in the range A-Z, a-z and 0-1, plus a few other symbols. This wastes the remainder of the 0-255 range for each character sent. From an information theory standpoint, this is an inefficient encoding. The text representation of integer values are in the general case much less efficient than the binary format. For example, in text format the worst case unsigned 32 bit integer 4294967295 takes 10 bytes, but in binary format it takes just four. In text, even the smallest numbers in 0-9 range require at least one byte, but in binary, smaller values like 0, 11, 31, 100 can be sent with fewer than 8 bits if we know their range ahead of time. If an integer value is negative, you have to spend a whole byte on ’-’ to indicate that. Floating point numbers waste one byte specifying the decimal point. The text representation of numerical values are variable length: “5”, “12345”, “3.141593”. Because of this we need to spend one byte on a separator after each value so we know when it ends. Newlines ‘\n’ or some other separator are required to distinguish between the set of variables belonging to one object and the next. When you have thousands of objects, this really adds up. In short, if we wish to optimize any further, it’s necessary to switch to a binary format. Switching to a Binary FormatIn the web world there are some really great libraries that read and write binary formats like BJSON, Protocol Buffers, Flatbuffers, Thrift, Cap’n Proto and MsgPack. In manay cases, these libraries are great fit for building your game network protocol. But in the fast-paced world of first person shooters where efficiency is paramount, a hand-tuned binary protocol is still the gold standard. There are a few reasons for this. Web binary formats are designed for situations where versioning of data is _extremely_important. If you upgrade your backend, older clients should be able to keep talking to it with the old format. Data formats are also expected to be language agnostic. A backend written in Golang should be able to talk with a web client written in JavaScript and other server-side components written in Python or Java. Game servers are completely different beasts. The client and server are almost always written in the same language (C++), and versioning is much simpler. If a client with an incompatible version tries to connect, that connection is simply rejected. There’s simply no need for compatibility across different versions. So if you don’t need versioning and you don’t need cross-language support what are the benefits for these libraries? Convenience. Ease of use. Not needing to worry about creating, testing and debugging your own binary format. But this convenience is offset by the fact that these libraries are less efficient and less flexible than a binary protocol we can roll ourselves. So while I encourage you to evaluate these libraries and see if they suit your needs, for the rest of this article, we’re going to move forward with a custom binary protocol. Getting Started with a Binary FormatOne option for creating a custom binary protocol is to use the in-memory format of your data structures in C/C++ as the over-the-wire format. People often start here, so although I don’t recommend this approach, lets explore it for a while before we poke holes in it. First define the set of packets, typically as a union of structs: 123456789101112131415161718192021222324252627struct Packet&#123; enum PacketTypeEnum &#123; PACKET_A, PACKET_B, PACKET_C &#125;; uint8_t packetType; union &#123; struct PacketA &#123; int x,y,z; &#125; a; struct PacketB &#123; int numElements; int elements[MaxElements]; &#125; b; struct PacketC &#123; bool x; short y; int z; &#125; c; &#125;;&#125;; When writing the packet, set the first byte in the packet to the packet type number (0, 1 or 2). Then depending on the packet type, memcpy the appropriate union struct into the packet. On read do the reverse: read in the first byte, then according to the packet type, copy the packet data to the corresponding struct. It couldn’t get simpler. So why do most games avoid this approach? The first reason is that different compilers and platforms provide different packing of structs. If you go this route you’ll spend a lot of time with #pragma pack trying to make sure that different compilers and different platforms lay out the structures in memory exactly the same way. The next one is endianness. Most computers are mostly little endian these days but historically some architectures like PowerPC were big endian. If you need to support communication between little endian and big endian machines, the memcpy the struct in and out of the packet approach simply won’t work. At minimum you need to write a function to swap bytes between host and network byte order on read and write for each variable in your struct. There are other issues as well. If a struct contains pointers you can’t just serialize that value over the network and expect a valid pointer on the other side. Also, if you have variable sized structures, such as an array of 32 elements, but most of the time it’s empty or only has a few elements, it’s wasteful to always send the array at worst case size. A better approach would support a variable length encoding that only sends the actual number of elements in the array. But ultimately, what really drives a stake into the heart of this approach is security. It’s a massive security risk to take data coming in over the network and trust it, and that’s exactly what you do if you just copy a block of memory sent over the network into your struct. Wheee! What if somebody constructs a malicious PacketB and sends it to you with numElements = 0xFFFFFFFF? You should, no you must, at minimum do some sort of per-field checking that values are in range vs. blindly accepting what is sent to you. This is why the memcpy struct approach is rarely used in professional games. Read and Write FunctionsThe next level of sophistication is read and write functions per-packet. Start with the following simple operations: 1234567void WriteInteger( Buffer &amp; buffer, uint32_t value );void WriteShort( Buffer &amp; buffer, uint16_t value );void WriteChar( Buffer &amp; buffer, uint8_t value );uint32_t ReadInteger( Buffer &amp; buffer );uint16_t ReadShort( Buffer &amp; buffer );uint8_t ReadByte( Buffer &amp; buffer ); These operate on a structure which keeps track of the current position: 123456struct Buffer&#123; uint8_t data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written&#125;; The write integer function looks something like this: 12345678910void WriteInteger( Buffer &amp; buffer, uint32_t value )&#123; assert( buffer.index + 4 &lt;= size );#ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = bswap( value );#else // #ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = value;#endif // #ifdef BIG_ENDIAN buffer.index += 4;&#125; And the read integer function looks like this: 123456789101112uint32_t ReadInteger( Buffer &amp; buffer )&#123; assert( buffer.index + 4 &lt;= size ); uint32_t value;#ifdef BIG_ENDIAN value = bswap( ((uint32_t)(buffer.data+buffer.index)) );#else // #ifdef BIG_ENDIAN value = ((uint32_t)(buffer.data+buffer.index));#endif // #ifdef BIG_ENDIAN buffer.index += 4; return value;&#125; Now, instead of copying across packet data in and out of structs, we implement read and write functions for each packet type: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859struct PacketA&#123; int x,y,z; void Write( Buffer &amp; buffer ) &#123; WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); &#125; void Read( Buffer &amp; buffer ) &#123; ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); &#125;&#125;;struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( Buffer &amp; buffer ) &#123; WriteInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); &#125; void Read( Buffer &amp; buffer ) &#123; ReadInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); &#125;&#125;;struct PacketC&#123; bool x; short y; int z; void Write( Buffer &amp; buffer ) &#123; WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); &#125; void Read( Buffer &amp; buffer ) &#123; ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); &#125;&#125;; When reading and writing packets, start the packet with a byte specifying the packet type via ReadByte/WriteByte, then according to the packet type, call the read/write on the corresponding packet struct in the union. Now we have a system that allows machines with different endianness to communicate and supports variable length encoding of elements. BitpackingWhat if we have a value in the range [0,1000] we really only need 10 bits to represent all possible values. Wouldn’t it be nice if we could write just 10 bits, instead of rounding up to 16? What about boolean values? It would be nice to send these as one bit instead of 8! One way to implement this is to manually organize your C++ structures into packed integers with bitfields and union tricks, such as grouping all bools together into one integer type via bitfield and serializing them as a group. But this is tedious and error prone and there’s no guarantee that different C++ compilers pack bitfields in memory exactly the same way. A much more flexible way that trades a small amount of CPU on packet read and write for convenience is a bitpacker. This is code that reads and writes non-multiples of 8 bits to a buffer. Writing BitsMany people write bitpackers that work at the byte level. This means they flush bytes to memory as they are filled. This is simpler to code, but the ideal is to read and write words at a time, because modern machines are optimized to work this way instead of farting across a buffer at byte level like it’s 1985. If you want to write 32 bits at a time, you’ll need a scratch word twice that size, eg. uint64_t. The reason is that you need the top half for overflow. For example, if you have just written a value 30 bits long into the scratch buffer, then write another value that is 10 bits long you need somewhere to store 30 + 10 = 40 bits. 1234uint64_t scratch;int scratch_bits;int word_index;uint32_t buffer; When we start writing with the bitpacker, all these variables are cleared to zero except buffer which points to the start of the packet we are writing to. Because we’re accessing this packet data at a word level, not byte level, make sure packet buffers lengths are a multiple of 4 bytes. Let’s say we want to write 3 bits followed by 10 bits, then 24. Our goal is to pack this tightly in the scratch buffer and flush that out to memory, 32 bits at a time. Note that 3 + 10 + 24 = 37. We have to handle this case where the total number of bits don’t evenly divide into 32. This is actually the common case. At the first step, write the 3 bits to scratch like this: 1xxx scratch_bits is now 3. Next, write 10 bits: 1yyyyyyyyyyxxx scratch_bits is now 13 (3+10). Next write 24 bits: 1zzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx scratch_bits is now 37 (3+10+24). We’re straddling the 32 bit word boundary in our 64 bit scratch variable and have 5 bits in the upper 32 bits (overflow). Flush the lower 32 bits of scratch to memory, advance word_index by one, shift scratch right by 32 and subtract 32 from scratch_bits. scratch now looks like this: 1zzzzz We’ve finished writing bits but we still have data in scratch that’s not flushed to memory. For this data to be included in the packet we need to make sure to flush any remaining bits in scratch to memory at the end of writing. When we flush a word to memory it is converted to little endian byte order. To see why this is important consider what happens if we flush bytes to memory in big endian order: 1DCBA000E Since we fill bits in the word from right to left, the last byte in the packet E is actually on the right. If we try to send this buffer in a packet of 5 bytes (the actual amount of data we have to send) the packet catches 0 for the last byte instead of E. Ouch! But when we write to memory in little endian order, bytes are reversed back out in memory like this: 1ABCDE000 And we can write 5 bytes to the network and catch E at the end. Et voilà! Reading BitsTo read the bitpacked data, start with the buffer sent over the network: 1ABCDE The bit reader has the following state: 123456uint64_t scratch;int scratch_bits;int total_bits;int num_bits_read;int word_index;uint32_t buffer; To start all variables are cleared to zero except total_bits which is set to the size of the packet as bytes 8, and bufferwhich points to the start of the packet. The user requests a read of 3 bits. Since scratch_bits is zero, it’s time to read in the first word. Read in the word to scratch, shifted left by scratch_bits (0). Add 32 to scratch_bits. The value of scratch is now: 1zzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx Read off the low 3 bits, giving the expected value of: 1xxx Shift scratch to the right 3 bits and subtract 3 from scratch_bits: 1zzzzzzzzzzzzzzzzzzzyyyyyyyyyy Read off another 10 bits in the same way, giving the expected value of: 1yyyyyyyyyy Scratch now looks like: 1zzzzzzzzzzzzzzzzzzz The next read asks for 24 bits but scratch_bits is only 19 (=32-10-3). It’s time to read in the next word. Shifting the word in memory left by scratch_bits (19) and or it on top of scratch. Now we have all the bits necessary for z in scratch: 1zzzzzzzzzzzzzzzzzzzzzzzz Read off 24 bits and shift scratch right by 24. scratch is now all zeros. We’re done! Beyond BitpackingReading and writing integer values into a packet by specifying the number of bits to read/write is not the most user friendly option. Consider this example: 123456789101112131415161718192021const int MaxElements = 32;struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) &#123; WriteBits( writer, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i] ); &#125; void Read( BitReader &amp; reader ) &#123; ReadBits( reader, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( reader, elements[i] ); &#125;&#125;; This code looks fine at first glance, but let’s assume that some time later you, or somebody else on your team, increases MaxElements from 32 to 200 but forget to update the number of bits required to 7（注意看WriteBits( writer, numElements, 6 )的6， 现在需要7了）. Now the high bit of numElements are being silently truncated on send. It’s pretty hard to track something like this down after the fact. The simplest option is to just turn it around and define the maximum number of elements in terms of the number of bits sent: 12const int MaxElementBits = 7;const int MaxElements = ( 1 &lt;&lt; MaxElementBits ) - 1; Another option is to get fancy and work out the number of bits required at compile time: 12345678910111213141516171819202122232425262728template &lt;uint32_t x&gt; struct PopCount&#123; enum &#123; a = x - ( ( x &gt;&gt; 1 ) &amp; 0x55555555 ), b = ( ( ( a &gt;&gt; 2 ) &amp; 0x33333333 ) + ( a &amp; 0x33333333 ) ), c = ( ( ( b &gt;&gt; 4 ) + b ) &amp; 0x0f0f0f0f ), d = c + ( c &gt;&gt; 8 ), e = d + ( d &gt;&gt; 16 ), result = e &amp; 0x0000003f &#125;;&#125;;template &lt;uint32_t x&gt; struct Log2&#123; enum &#123; a = x | ( x &gt;&gt; 1 ), b = a | ( a &gt;&gt; 2 ), c = b | ( b &gt;&gt; 4 ), d = c | ( c &gt;&gt; 8 ), e = d | ( d &gt;&gt; 16 ), f = e &gt;&gt; 1, result = PopCount&lt;f&gt;::result &#125;;&#125;;template &lt;int64_t min, int64_t max&gt; struct BitsRequired&#123; static const uint32_t result = ( min == max ) ? 0 : ( Log2&lt;uint32_t(max-min)&gt;::result + 1 );&#125;;#define BITS_REQUIRED( min, max ) BitsRequired&lt;min,max&gt;::result Now you can’t mess up the number of bits, and you can specify non-power of two maximum values and it everything works out. 12const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); But be careful when array sizes aren’t a power of two! In the example above MaxElements is 32, so MaxElementBits is 6. This seems fine because all values in [0,32] fit in 6 bits. The problem is that there are additional values within 6 bits that are outside our array bounds: [33,63]. An attacker can use this to construct a malicious packet that corrupts memory! This leads to the inescapable conclusion that it’s not enough to just specify the number of bits required when reading and writing a value, we must also check that it is within the valid range: [min,max]. This way if a value is outside of the expected range we can detect that and abort read. I used to implement this using C++ exceptions, but when I profiled, I found it to be incredibly slow. In my experience, it’s much faster to take one of two approaches: set a flag on the bit reader that it should abort, or return false from read functions on failure. But now, in order to be completely safe on read you must to check for error on every read operation. 123456789101112131415161718192021222324252627282930313233343536const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements );struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) &#123; WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) &#123; WriteBits( writer, elements[i], 32 ); &#125; &#125; void Read( BitReader &amp; reader ) &#123; ReadBits( reader, numElements, MaxElementBits ); if ( numElements &gt; MaxElements ) &#123; reader.Abort(); return; &#125; for ( int i = 0; i &lt; numElements; ++i ) &#123; if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); &#125; &#125;&#125;; If you miss any of these checks, you expose yourself to buffer overflows and infinite loops when reading packets. Clearly you don’t want this to be a manual step when writing a packet read function. You want it to be automatic, just read the NEXT ARTICLE. 译文译文出处 译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 导论大家好，我是格伦·菲德勒。欢迎大家阅读我新开的这个系列教程：《构建游戏网络协议》。 在这个系列文章中，我将完全从头开始为动作游戏（比如说第一人称射击游戏、近身格斗和实时多人在线战斗竞技场游戏都是动作游戏）基于UDP协议构建一个专业级别的游戏网络协议。我所使用的工具只包括我的Macbook Pro、 Sublime Text 3（这是一个很好用的编辑器，非常值得一试）、我信赖的C++编译器和一组UDP套接字。 我写这个系列文章的目的是分享在过去十年里我在这个领域学到的有关游戏网络方面的专业知识，因为似乎没有人写过这些方面的东西。所以其他的网络程序员到底是如何想如何做的，我想通过这个系列文章来进行一定的分享。如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我可以有机会来写更多的文章。如果你对我的工作进行支持和捐赠的话，你还可以访问到这个系列文章的示例源代码(这些源代码都是开源的,所以您可以使用任何你想用的地方甚至是商业内容上！)以及一个密码以便访问这个网站上还没有发表的一些文章。 关于我的情况，已经说得足够多了，现在让我们开始这个系列文章把！ 对数据包的读取和写入你是否有想过多人在线游戏是如何读取和写入数据包的？ 如果你有web开发的背景话，你可能会使用XML、JSON或YAML以文本格式来通过网络发送数据。但大多数游戏网络程序员会嘲笑这种对游戏数据进行编码的建议。他们可能会说：”哈哈哈，这真的很有趣。你不是认真的对吧？“哦。你…是认真的。你被解雇了。你可以收拾东西回家了。 我只是开了一个玩笑，但是玩笑归玩笑，为什么这种方法不好？ 一个网页服务器位于网络上的某个位置，监听请求并发送响应。这些请求和响应都是无状态的并且对实时性要求非常非常低（当然有个快速的响应是非常重要的，但是如果不是这种情况也没什么关系）。因为无状态和请求/响应的频率非常低，所以具有非常良好的扩展性。但是多人在线游戏的服务器与这种网页服务器完全是不同的。它是以每秒60次的速度来对游戏世界的状态进行模拟。它是实时的并且有状态的，并且需要把所有的状态以每秒20次或者30次的频率下发给客户端。因为整个游戏世界有几千个物体，每个物体可能会有几百个状态，所以下发给客户端的状态可能是海量的。 如果你使用文本格式(如XML或JSON)对这个游戏状态进行编码的话，有可能这种方法是非常低效的。 让我们举个简单的例子，看下下面这个XML文档： 1234567891011121314151617181920&lt;world_update world_time=“0.0”&gt; &lt;object id=“1” class=“player”&gt; &lt;property name=“position” value=“(0,0,0)” &lt;=”” property=“”&gt; &lt;property name=“orientation” value=“(1,0,0,0)” &lt;=”” property=“”&gt; &lt;property name=“velocity” value=“(10,0,0)” &lt;=”” property=“”&gt; &lt;property name=“health” value=“100”&gt;&lt;/property&gt; &lt;property name=“weapon” value=“110”&gt;&lt;/property&gt; … 100s more properties per-object … &lt;/property&gt;&lt;/property&gt;&lt;/property&gt;&lt;/object&gt; &lt;object id=“100” class=“grunt”&gt; &lt;property name=“position” value=“(100,100,0)” &lt;=”” property=“”&gt; &lt;property name=“health” value=“10” &lt;=”” property=“”&gt; &lt;/property&gt;&lt;/property&gt;&lt;/object&gt; &lt;object id=“110” class=“weapon”&gt; &lt;property type=“semi-automatic”&gt;&lt;/property&gt; &lt;property ammo_in_clip=“8”&gt;&lt;/property&gt; &lt;property round_in_chamber=“true”&gt;&lt;/property&gt; &lt;/object&gt; … 1000s more objects …&lt;/world_update&gt; 这看上去真的很让人讨厌。我们可以做得更好吗?当然。。。使用JSON来编码的话文本量可以少一些： 123456789101112131415161718192021222324&#123; “world_time”: 0.0, “objects”: &#123; 1: &#123; “class”: “player”, “position”: “(0,0,0)”, “orientation”: “(1,0,0,0)”, “velocity”: “(10,0,0)”, “health”: 100, “weapon”: 110 &#125; 100: &#123; “class”: “grunt”, “position”: “(100,100,0)”, “health”: 10 &#125; 110: &#123; “class”: “weapon”, “type: “semi-automatic“ “ammo_in_clip“: 8, “round_in_chamber”: 1 &#125; &#125;&#125; 但要注意的是描述数据的属性比实际要发送的数据还大。这糟透了。 但是如果我们把数据分成两个部分呢? 一个模式来描述物体类的几何以及每个类的属性。 数据相对于该模式进行编码来快速下发给各个客户端。 下面是JSON的一个模式。它只会被下发一次： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; “classes”: &#123; 0: “player” &#123; “properties”: &#123; 0: &#123; “name”: “position”, “type”: “vec3f” &#125; 1: &#123; “name”: “orientation”, “type”: “quat4f” &#125; 2: &#123; “name”: “velocity”, “type”: “vec3f” &#125; 3: &#123; “name”: “health”, “type”: “float” &#125; 4: &#123; “name”: “weapon”, “type”: “object”, &#125; &#125; &#125; 1: “grunt”: &#123; “properties”: &#123; 0: &#123; “name”: “position”, “type”: “vec3f” &#125; 1: &#123; “name”: “health”, “type”: “float” &#125; &#125; &#125; 2: “weapon”: &#123; “properties”: &#123; 0: &#123; “name”: “type”, “type”: “enum”, “enum_values”: [ “revolver”, “semi-automatic” ] &#125; 1: &#123; “name”: “ammo_in_clip”, “type”: “integer”, “range”: “0..9”, &#125; 2: &#123; “name”: “round_in_chamber”, “type”: “integer”, “range”: “0..1” &#125; &#125; &#125; &#125;&#125; 现在我们可以更加紧凑的进行状态更新(每秒进行20次或30次状态更新…)： 12345678&#123; “world_time”: 0.0, “objects”: &#123; 1: [0,“(0,0,0)”,“(1,0,0,0)”,“(10,0,0)”,100,110], 100: [1,“(100,100,0)”,10], 110: [2,1,8,1] &#125;&#125; 这确实更好一点了。但是我们为什么不能直接下发一个简单的文本格式而没有任何废话吗? 12340.01:0,0,0,0,1,0,0,0,10,0,0,100,110100:1,100,100,0,10110:2,1,8,1 这比原来紧凑多了。当然它在没有模式的情况下是完全不可读的，但是这也是整个问题的所在。 文本格式存在的问题我们确实取得了一些进展，但是我们仍然在使用文本格式，所以就继承了文本格式固有的低效。 为什么？ 让我们以一个浮点数为例来分析这个问题。在二进制格式中一个浮点数需要占据4个字节，但是在文本格式下一个浮点数需要多的多的空间。让我们举个简单的例子：“3.141592653589793”一共用了17个字节（并没有包括终止符‘\0’）。当然你可以说让我们把这个浮点数缩短到6位有效数字的情况。但是就算我们这样处理了，所得到的“3.141593”所占据的空间仍然是二进制表示法的2倍大小。更糟糕的是如果你有一个很大的表示位置的数值，比如”12134.534112”。 你仍然需要6位精度的准确性所以现在你需要使用12个字节来表示这个浮点数。而这样所占据的空间是二进制表示法的3倍大小。你还会为每一个‘,’分隔符浪费一个字节。 我希望你能看到，在多人在线游戏中使用文本格式进行通信是完全不可行的。“但是格伦，整个互联网都是构建于文本格式之上的，为什么在多人在线游戏中使用文本格式进行通信完全不可行？”。恩，我认为这是那些创造互联网的人在这个过程中犯下的一些错误。(译注：原作者的这个结论下的太武断了，我觉得主要是因为互联网和游戏通信协议的面向对象不同，游戏通信协议面向的目标是非常特定而且固定的，这使得他们可以提前沟通到底以什么格式来传输协议，或者说不需要传输任何解读的标签就能够互相理解，而这种互相理解的基础是通过其他方式已经进行了沟通，但是互联网的通信则是完全不同的，它要非常的通用，兼顾更多的需求，而且根本就不知道是谁来解读这些信息，所以信息的可读性就非常非常的重要，因为我们没有其他的管道进行通信，只能凭借传输过去的信息本身来解读，所以互联网的协议才是设计成这样。)。可读性可能在这些早期协议很重要，但我可以向你保证，让人们很容易来阅读这些协议和修改你的游戏的网络协议正好是硬币的两面，根本没有办法兼得。 而且因为带宽是非常关键的要素，并且你希望在每秒用尽可能少的比特数来传递尽可能多的游戏内容，所以你完全没有办法使用文本格式，必须切换成二进制格式。 切换成二进制格式在网络世界中存在一些用来读取和写入二进制格式的库，比如BJSON,、Protocol Buffers、 Flatbuffers、 Thrift、 Cap’n Proto 和 MsgPack。这几乎都是常识了。 这些库其实都还不错。借助接口描述语言和一些代码生成工具的帮助能够读取和写入（这通常被称为序列化）你的游戏数据结构成为二进制格式，而这些二进制数据会在网络上进行传递。在网络的另外一侧，这些数据会被反序列化并且转换回原始的数据结构。 使用二进制格式进行传说网络数据的优点有：1) 你不必自己动手写一个序列化层。2）这些库往往是语言无关的，所以你可以在程序的前端（也就是客户端）使用一种语言而在程序的后端（也就是服务端）使用另外一种语言，而在这种情况下手，程序的前端和程序的后端仍然是可以进行通信的。3)提供了版本信息，这样的话，如果你的客户端使用的是一个比较旧版本的协议，而服务器使用的是一个比较新版本的协议的话，它们仍然是可以进行通信的。反过来也是一样。 这些库看上去很棒，但是它们是实现你的游戏的网络协议的一种很棒的方法么？ 其实情况并不总是如此。如果你有一个协议，这个协议是负责向网页服务器进行请求和接受响应，但是需要支持多种语言并且版本信息对你来说非常的重要，那么在这种情况下，因为有了可以支持这些功能的库的存在，你再自己去实现自己的带版本信息的序列化层和多语言支持就是一件特别傻的事情。如果你的游戏需要从一台机器上远程调用另外一台机器的函数。那么可能只用这些已有的库就是完全没问题的。。。 但是在性能至关重要的网络通信的情况下，比如我们在这篇文章中讨论的这种情况（对于第一人称射击游戏游戏、动作游戏等等），游戏网络的基本单位是状态而不是远程函数调用。同时,跨语言支持二进制格式所能提供的好处很小，这是因为客户端和服务器通常情况下都是使用一种语言进行开发的（比如C++）。这些游戏也不需要复杂的版本信息和版本验证机制，因为如何客户端试图以一个不同的协议版本与服务器进行连接的话，那么服务器可以直接拒绝这个连接就好。有一种特别简单但是有效的办法可以解决版本的问题，就是绕过客户端永远用和服务器相同的协议版本进行连接。 所以如果你的协议不是大部分使用远程函数调用的话，你其实根本就不需要版本信息，也不需要什么跨语言的支持，到底这么做有什么好处？从我的观点来看好处其实并不多。所以让我们直接忽略掉这些功能并用我们自己的不带属性的二进制流进行代替，在这个过程中我们可以获得更多的控制性和灵活性。 如何开始使用二进制格式如果你在用C或C++对你的游戏进行编码，你可能想知道为什么不能直接使用memcpy（这是一个函数，可以直接进行内存拷贝, 将n字节长的内容从一个内存地址复制到另一个地址）把我的结构拷贝到数据包里面？很多人会经常从这里开始，因此尽管我不推荐这种方法，还是让我们看下这个方法，看看如果用这个方法来进行网络包传递的话会有哪些问题。 首先要定义你的数据包的集合，通常情况下这是结构的”联合“（C语言的一种语法名字）： 123456789101112131415161718192021222324252627struct Packet&#123; enum PacketTypeEnum &#123; PACKET_A, PACKET_B, PACKET_C &#125;; uint8_t packetType; union &#123; struct PacketA &#123; int x,y,z; &#125; a; struct PacketB &#123; int numElements; int elements[MaxElements]; &#125; b; struct PacketC &#123; bool x; short y; int z; &#125; c; &#125;; &#125;; 当对数据包进行写入的时候，设置数据包的第一个字节为数据包的类型（0或者1或者2）。然后依据数据包的类型，使用memcpy函数将合适的联合结构拷贝到数据包里面。在读取数据包的时候进行完全相反的操作：读取的数据包的第一个字节，然后根据数据包的类型就能判断出这个数据包还有多少字节没有读取。然后使用memcpy函数将数据包的数据拷贝到合适的联合结构里面。 这种方法没有办法更加简化了，所以这就完了么？不完全是这样的！我不推荐这种方法，因为它有一些很讨厌的问题。 首先，不同的编译器和平台对于数据结构的打包方式是完全不同的。如果你走这条路的话，你会花很多时间来使用#pragma来努力确保在不同平台的不同的编译器上布局结构在内存中使用完全相同的方式。让这种方式可以在32位和64位架构上都能顺利工作是一个很有“意思”的过程。这并不是说这是不可能的，但它肯定不是一件容易的事情。 下一个比较大的问题是字节顺序。现代计算机大多是使用小端这种字节顺序（英特尔的中央处理器都是如此），但PowerPC的内核使用大端这种字节顺序。历史上的网络数据通过用大端这种字节顺序（网络字节顺序）来进行发送，但没有理由让你遵循这一传统。在我的代码里面，我使用的是小端这种字节顺序，这么做的原因是使用这个字节顺序在最常见的平台（英特尔）上在打包和解包中所要做的工作量最少。 如果你需要支持使用小端字节顺序的机器和使用大端字节顺序的机器之间进行通信的话，只是使用memcpy函数来将结构体拷贝到数据包的方法根本行不通。你至少需要编写一个函数来在结构被读取以后对它的每个属性的字节顺序进行调整，然后才能顺利的读取和写入。 还有一些其他的小问题。很显然，如果你的结构中包含指针，你不能直接把指针进行序列化然后在网络上进行传递然后期望在网络的另外一端反序列化这个指针以后，这个指针在那边还能够正常的使用。另外，如果你有可变大小的结构，比如说一个可以多达32个元素的数组，但是它在大多数时间里面都是空的或者只有很少一些元素，但是为了防止最差的情况你总是需要假设它有32个元素并且进行序列化和反序列化，这非常非常的浪费。一个更好的办法是让你可以对你的可变长度的结构进行编码，能够把长度信息编码到数据结构本身，这样在序列化和反序列化的时候都能妥善处理这个问题。 我觉得真正影响这个方法的可用性的最后一个问题是安全性。如果使用这种方法，你相当于直接把整个C++结构中的数据包直接拷贝，然后在网络上进行发送。你到底在想什么？!!如果有人构造了一个恶意的数据包，并把这个包的长度标记为0xFFFFFFFF发送给你的话，这样你在处理这个数据包的时候，将导致你耗费尽所有的内存空间。 这是一个巨大的安全风险，让你的原始数据直接在网络上进行传输并且选择相信这些在网络上接收到的数据。。你应该，至少，对这些值进行一个取值范围的检查，让这些值确保落在你期望的范围之内，而不是盲目的相信所有发送给你的数据。 读取和写入函数这个复杂度导致的下一个问题就是每个数据包的读取和写入函数。让我们先从以下几个简单的操作开始： 1234567void WriteInteger( Buffer &amp; buffer, uint32_t value );void WriteShort( Buffer &amp; buffer, uint16_t value );void WriteChar( Buffer &amp; buffer, uint8_t value ); uint32_t ReadInteger( Buffer &amp; buffer );uint16_t ReadShort( Buffer &amp; buffer );uint8_t ReadByte( Buffer &amp; buffer ); 这几个函数是对一个缓冲区结构进行操作，有点类似这样： 123456struct Buffer&#123; uint8_t data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written&#125;; 举个简单的例子来说，写入整数的函数会像是如下这样： 12345678910void WriteInteger( Buffer &amp; buffer, uint32_t value )&#123; assert( buffer.index + 4 &lt;= size );#ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = bswap( value ); #else // #ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = value; #endif // #ifdef BIG_ENDIAN buffer.index += 4;&#125; 而读取整数的函数会像是如下这样： 12345678910111213uint32_t ReadInteger( Buffer &amp; buffer )&#123; assert( buffer.index + 4 &lt;= size ); uint32_t value;#ifdef BIG_ENDIAN value = bswap( ((uint32_t)(buffer.data+buffer.index)) );#else // #ifdef BIG_ENDIAN value = ((uint32_t)(buffer.data+buffer.index));#endif // #ifdef BIG_ENDIAN buffer.index += 4; return value;&#125; 现在就不仅仅是直接使用memcpy函数把内存中的数据结构直接拷贝到数据包里面，而是为每个数据包类型使用了单独的读取和写入函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859struct PacketA&#123; int x,y,z; void Write( Buffer &amp; buffer ) &#123; WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); &#125; void Read( Buffer &amp; buffer ) &#123; ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); &#125;&#125;; struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( Buffer &amp; buffer ) &#123; WriteInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); &#125; void Read( Buffer &amp; buffer ) &#123; ReadInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); &#125;&#125;; struct PacketC&#123; bool x; short y; int z; void Write( Buffer &amp; buffer ) &#123; WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); &#125; void Read( Buffer &amp; buffer ) &#123; ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); &#125;&#125;; 当对数据包进行读取和写入的时候，通过ReadByte / WriteByte函数来在数据包的数据之前加上一个字节，用来表明数据包的类型，然后根据数据包的类型，调用联合体里面对应数据包结构里面读取或者写入函数。所以，现在我们有了一个简单的系统，允许使用不同的字节顺序的机器可以进行通信，并支持可变长度的编码。举个简单的例子来说，现在可以对数组的长度进行序列化并且只对存在的数据进行遍历和序列化，而不用像以前那样，总是要按照最坏情况来发送整个队列。 读取和写入函数存在的问题有了读取和写入函数，是相比较之前的memcpy方法进行数据包的打包和解包前进了一大步，但是这种方法也存在一些问题。让我们一起看下具体都有些什么问题。 第一个存在的问题：如果有一个值，它的取值范围是【0，100】，那么它只需要10个比特就能表示所有可能的值，但是因为我们只能序列化一个字节的值【0，255】，一个short类型的整数【0,655325】或者是一个32比特的整数【0，2^32-1】，所以我们必须凑够16位。这样的话就浪费了6位！ 同样的，如果是一个布尔值（只有真和假两种情况）只需要一位就能表示，但是它们必须取整为一个字节，这就浪费了7位！现在你可以通过用C++结构来实现你的数据包以及对C++的位域进行序列化并使用一些联合方面的技巧来对这个值进行取整，但是你真的能保证两个完全不同的C ++编译器用完全相同的方式来在内存对位域进行打包？据我所知，应该是不可能的。 第二个存在的问题：怀有恶意的人仍然可以构造一个恶意的数据包。举个简单的例子来说，让我们假设MaxElements是32，所以数据包PacketB的元素的数目肯定是在范围【0，32】之间。由于我们是在字节级别对数据包进行读取和吸入，所以元素的数目是存在一个字节里面，可能的取值范围是【0，255】。这导致字节里面【33,255】这个范围的值是完全没有定义的。 如果有人构造了一个elementCount = 255的恶意的数据包，会发生什么？会发生内存崩溃！当然你可以手动设置一个阈值，对读入的数据的大小进行限制，或者手动对它们进行检查，如果出现问题就中止读取，但是你真正想要的一个知道每个要读取的域的最小/最大值到底是多少的系统，并且在任何数据包中对应的值如果超出预期的范围就会自动中止读取，这样在你的代码看到这些不合法的值之前，这些值就已经被丢弃了。在我看来，最后一个存在的问题是，维护这些单独的读取和写入函数真的很让人讨厌。随着这些函数变得越来越复杂，它很容易对其中一个函数进行改变而忘记相应的改变另外一个函数（读取和写入函数是成对出现的，如果要改变一个的话，需要同时改变另外一个，要么就会出现问题，发送方和写入方根本就没法正确的得到数据包里面的信息）。导致之间的读写不同步。而这些不同步可能非常难以追查。 我们要解决所有这些问题，但是首先我们要通过实现一个位打包器来朝这个方向努力，这样我们才不会继续浪费位来存储一些没必要的数据。 实现一个位打包器如果我们要在数据包写入一个布尔值的话，它应该只需要在数据包里面占据一个位的大小。如果我们要在数据包写入一个取值范围在【0，31】的值，它应该在数据包里面占据五个位的大小，而不是八个位的大小。如果我们要在数据包写入一个取值范围在【0，100000】的值，它应该在数据包里面占据十七位的大小，而不是二十四位的大小或者三十二位的大小。要做到这一点，我们需要写一个位打包器。 很多人所写的位打包器是工作在字节这个级别上的，举个例子来说明的话就是他们会把生成的字节刷新到流里面，但是我不喜欢这种方法，因为如果它们是工作在word这个级别的话会快的多。我的目标是每次是写下很多word的时候（32位或者64位），然后每次读取32位或者64位，因为现代机器对这个长度进行了专门的优化而不应该像1985年那样在字节的级别对缓冲区进行处理。 我的位打包技术的原理是类似这样的：如果你想在一次在数据包写入32位的话，你需要一个两倍大小的临时word，比如说uint64_t。如果你想在一次在数据包写入64位的话，你需要128位。这样做的原因是你需要整个区域的上半部分进行溢出，因为你可以只写一个30位大小的值到临时缓冲区，然后需要写入一个10位大小的值到临时缓冲区中。如果这个临时字的上半部分没有额外的空间的话，你需要额外的分支和逻辑来处理溢出情况。既然你想在位打包里面的循环里面产生尽可能少的分支，那么这种方法将有很大的意义。 以位来写入数据包对于位打包器的数据吸入，你需要一些缓冲区以及一个变量来记录目前在缓冲区里面的位的数目。在这个例子之中，让我们把字长选为32位，这样，位打包器的变量看上去就会像是这样： 1234uint64_t scratch;int scratch_bits;int word_index;uint32_t buffer; 当你开始启动你的位打包器进行写入的时候，所有这些变量都将被清零，缓冲区的指针会指向缓冲区的起始位置，这个指针用于数据包实际开始写入的位置指示。这个缓冲区的长度是以字节为单位的，必须要是4的整数倍，因为我们工作的字长是32位。 比方说，我们要写入3位数据，然后是10位数据，再然后是24位数据。你的目标是在临时缓冲区对这块数据使用一个比较紧密的打包方式并把整理好的数据刷新到内存中去，一次是32位（一个字长）。需要注意的是3 + 10 + 24 = 37，这是故意设计的。你必须处理这种情况。 在第一步中，向临时缓冲区写入3位数据就像下面这样： 1xxx 临时缓冲区的长度现在就是3位。 接下来，向临时缓冲区写入10位数据就像下面这样： 1yyyyyyyyyyxxx 临时缓冲区的长度现在就是13位（10+3）。为什么要以这种方式进行打包而不是使用xxxyyyyyyyyyy这种方式？我用这种字节顺序写入的原因是因为我用小端字节顺序来存储网络数据。如果我没有按照这个方向对位数据进行存储的话，那么在发送数据包的时候我将不得不将数据包的大小对齐到下一个双字的长度那里，或者我只能对我的数据包的尾端进行截断。如果你想以大端字节顺序来发送刷新的数据的话，你应该按照另外一个方向对数据包的数据进行打包。 接下来，向临时缓冲区写入24位数据就像下面这样： 1zzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx 临时缓冲区的长度现在就是37位（10+3+24）。我们正在跨越32位字的边界，并有5位会写入到uint64_t结构的上半32位中（所以这实际是一个溢出）。现在bit_index &gt;= 32，刷新低32位的数据到内存中去，并对word_index加1，然后从临时缓冲区的长度减去32并向对临时缓冲区向右偏移32位。 临时缓冲区现在看上去就像是下面这样： 1zzzzz 我们可以继续下去，但是我们决定在停下来对这一点进行解释。我们必须在整个数据包的最后放置的是一个32位的字。对字这个级别进行处理的位打包器的一个微妙的一点是你需要在整体写入的最后进行一个刷新处理，这样才能保证最后的这些位会写入到内存中去。当我们把一个字刷新到内存中去的时候，我们要确保这个字会被正确的转换成小端字节顺序。举个简单的例子来说：ＡＢＣＤ当被写入内存中的时候需要被有效的转换成ＤＣＢＡ这个顺序。如果要想明白为什么这种做法非常重要，需要考虑下如果我们用大端字节顺序来把字刷新到内存中去会发生什么事情？最后一个字会截断，因为这个字会以在整数相同的字节顺序写入到内存中去。因为我们会把开始的那些位写到字的低的那个字节中去，我们的内存中的顺序看上去是这样的：对于刚才那串比特值，写入内存的时候会是ＡＢＣＤＥ这个顺序。 1DCBA000E 现在，如果我们尝试以５个字节的大小来用一个数据包来发送缓冲区（我们要发送的数据的实际大小）它捕获的最后一个字节会是０而不是Ｅ。（作者在这里打了一个笑脸）。 但是，当我们以小端这种字节顺序把上面的内容写入内存的时候，字节在内存中的布局是这样的： 1ABCDE000 我们可以只在网络上写5个字节，这样就节省了3个字节，并且仍然是以Ｅ来作为数据包的结尾。 实际上，我们所做的是开关字周围的字节，因为我们通过这种方法进行构造来避免小端字节顺序的重新排序，所以我们希望是以它们在内存中的顺序直接写入到网络的数据包中，字节顺序是很难处理的。 其代码类似于 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144class BitWriter&#123;public: BitWriter( void* data, int bytes ) : m_data( (uint32_t*)data ), m_numWords( bytes / 4 ) &#123; assert( data ); assert( ( bytes % 4 ) == 0 ); // buffer size must be a multiple of four m_numBits = m_numWords * 32; m_bitsWritten = 0; m_wordIndex = 0; m_scratch = 0; m_scratchBits = 0; &#125; void WriteBits( uint32_t value, int bits ) &#123; assert( bits &gt; 0 ); assert( bits &lt;= 32 ); assert( m_bitsWritten + bits &lt;= m_numBits ); value &amp;= ( uint64_t(1) &lt;&lt; bits ) - 1; m_scratch |= uint64_t( value ) &lt;&lt; m_scratchBits; m_scratchBits += bits; if ( m_scratchBits &gt;= 32 ) &#123; assert( m_wordIndex &lt; m_numWords ); m_data[m_wordIndex] = host_to_network( uint32_t( m_scratch &amp; 0xFFFFFFFF ) ); m_scratch &gt;&gt;= 32; m_scratchBits -= 32; m_wordIndex++; &#125; m_bitsWritten += bits; &#125; void WriteAlign() &#123; const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) &#123; uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); &#125; &#125; void WriteBytes( const uint8_t* data, int bytes ) &#123; assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8 || ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; FlushBits(); assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) &#123; assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data + headBytes, numWords * 4 ); m_bitsWritten += numWords * 32; m_wordIndex += numWords; m_scratch = 0; &#125; assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes ); &#125; void FlushBits() &#123; if ( m_scratchBits != 0 ) &#123; assert( m_wordIndex &lt; m_numWords ); m_data[m_wordIndex] = host_to_network( uint32_t( m_scratch &amp; 0xFFFFFFFF ) ); m_scratch &gt;&gt;= 32; m_scratchBits -= 32; m_wordIndex++; &#125; &#125; int GetAlignBits() const &#123; return ( 8 - ( m_bitsWritten % 8 ) ) % 8; &#125; int GetBitsWritten() const &#123; return m_bitsWritten; &#125; int GetBitsAvailable() const &#123; return m_numBits - m_bitsWritten; &#125; const uint8_t* GetData() const &#123; return (uint8_t*) m_data; &#125; int GetBytesWritten() const &#123; return ( m_bitsWritten + 7 ) / 8; &#125; int GetTotalBytes() const &#123; return m_numWords * 4; &#125;private: uint32_t* m_data; uint64_t m_scratch; int m_numBits; int m_numWords; int m_bitsWritten; int m_wordIndex; int m_scratchBits;&#125;; 以位来读取数据包我们该如何在网络的另外一侧读取已经通过位打包器打包好的数据？ 要我们从要在网络上进行发送的缓冲区开始，假设我们刚刚从recvfrom.函数返回。它里面的内容有５位这么长。 1ABCDE 因为我们是按照字这个等级进行读取，我们必须要把数据的长度截断到双字这个长度，比如说８个字节： 1ABCDE000 现在，这里有一点很微妙的地方。当我在网络上发送一个数据包的时候，我真的不知道它到底包含了多少位的数据（否则的话我将不得不将这个数据包的大小直接记录在数据包的包头里面），而且这是一个带宽的浪费。但是通过在网络的另外一侧使用recvfrom函数我确实知道到底这个数据包的内容的长度是多少，因此当５个字节大小的数据包从网络上到达的时候，我可以直接认为缓冲区中的位的大小是数据包字节大小乘以８。因此，实际上，位读取器认为这个分组中要被读取的比特数为5 8 = 40，而不是37。 你真的想在这里做的事情是确保如果位读取器读取的位置已经超过缓冲区实际位数的结尾，在这个例子中，就是发送的３７位数据，那么它将读取的是零而不是未定义数据。这会自动发生在数据包最后一个字节的最后3位上，因为它们是由位写入器写入的零，但是对于在缓冲区中的最后3个比特你必须确保它们被读为零，以及未来的任何可能位的读取也是返回零。这是一个非常重要的安全步骤以防止你读取的时候超过缓冲区或者数据流的结尾。 现在让我们开始吧。你将在位读取器里面有如下这些变量： 123456uint64_t scratch;int scratch_bits;int total_bits;int num_bits_read;int word_index;uint32_t buffer; 这比位写入器要复杂一些，因为你需要做更多的检测。但是请记住，永远不要相信来自客户端的数据。 要开始进行读取的时候，字的序号是０，临时缓冲区的内容和临时缓冲区的位数都是０。 然后用户请求３位的空间。因为临时缓冲区的位数现在是０，现在是时候来读取第一个字了。读取第一个字然后把它放到临时缓冲区，并对临时缓冲区的位数（目前是0）向右进行偏移。把临时缓冲区的位数添加32。 现在通过把临时缓冲区的内容拷贝到另外一个变量里面来读取前面的3位并通过&amp; ( (1&lt;&lt;3 ) – 1 )进行遮罩处理，来给出最后输出的结果： 1xxx 现在将临时缓冲区的内容向右偏移3位，并从临时缓冲区的位数中减去3： 1zzzzzzzzzzzzzzzzzzzyyyyyyyyyy 现在用完全相同的办法读取后面的10位。临时缓冲区看上去就是下面这样的： 1zzzzzzzzzzzzzzzzzzz 恩。接下来的读取调用请求24位数据的内容，但是临时缓冲区的位数只有19了（19 = 32-10-3 ）。。。现在是时候来读取下一个字了。这个字多半是零因为我们已经对它们进行了清除，但是它还有多余的5位我们需要在接下来进行读取。现在我们准备读取临时缓冲区里面有关z的比特位了： 1zzzzzzzzzzzzzzzzzzzzzzzz 接下来读取24位并向右位移24位。临时缓冲区现在全部都是0了。 理论上位读取器认为还有27位数据留在缓冲区没有进行读取，如果我们继续进行读取的话，这些位会被作为零弹出来，因为最后一个字节的前3位是零，并且我们把临时缓冲区的最后3个字节全部清位0，因为我们为了按照字进行对齐。（为了对齐，所以填充了3个全部是0的字节。所以位读取器这时候看的话还有3个完整的字节没有读。） 但是让我们假设，由于某种原因，越过了这个点以后用户还是一直尝试进行读取。为了处理这种情况，检查缓冲区中你需要读取的字的数目，以及每次你需要从缓冲区实际读取的字的情况。如果你已经读完了要读的字以后，不要继续增加word_index并继续读取数据这会让你的内存崩溃的，给你的缓冲区填充0来对齐并在缓冲区的位数添加32在每次给缓冲区的末尾添加一个新的字的情况下。通过这种方法，在读取位数的每次内部循环调用的时候能确保分支最后总是返回零，并把它放在你每次需要读取一个新字的地方，但是你读取的位置已经超过了缓冲区结尾的地方你仍然是安全的并且返回0个比特。 这似乎有点过于谨慎了，但是在对网络数据进行读取的时候这种谨慎是特别重要的。这是通过网络传输过来的数据。不要相信它们！如果你的读取和写入是不同步的，或者有人给你发送了一个恶意的缓冲区数据，你可能会被困在一个循环里面并不断尝试读取数据。确保内部循环里面所有的遍历里面的读取都会检测缓冲区是否溢出或者有损坏，如果用户读取的位置已经超出了缓冲区的结尾这种策略总是返回定义的值（0），通过这种行为你可以确保大多数情况都是符合预期的。 其代码类似于： 123456789101112131415161718192021222324252627282930313233343536class BitReader&#123;public: // ... uint32_t ReadBits( int bits ) &#123; assert( bits &gt; 0 ); assert( bits &lt;= 32 ); assert( m_bitsRead + bits &lt;= m_numBits ); m_bitsRead += bits; assert( m_scratchBits &gt;= 0 &amp;&amp; m_scratchBits &lt;= 64 ); if ( m_scratchBits &lt; bits ) &#123; assert( m_wordIndex &lt; m_numWords ); m_scratch |= uint64_t( network_to_host( m_data[m_wordIndex] ) ) &lt;&lt; m_scratchBits; m_scratchBits += 32; m_wordIndex++; &#125; assert( m_scratchBits &gt;= bits ); const uint32_t output = m_scratch &amp; ( (uint64_t(1)&lt;&lt;bits) - 1 ); m_scratch &gt;&gt;= bits; m_scratchBits -= bits; return output; &#125; //...&#125; 如何使位打包器更棒位打包器这种方法非常的棒，但是直接用于读取和写入数据包时，这并不是最有用的方法。我见过有团队直接使用位打包器进行读取和写入数据包，但是这并不是最佳的方法。你会拥有很多复杂的代码，并且非常容易出错。 让我们一起看下这个例子： 123456789101112131415161718192021const int MaxElements = 32; struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) &#123; WriteBits( writer, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i] ); &#125; void Read( BitReader &amp; reader ) &#123; ReadBits( reader, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( reader, elements[i] ); &#125;&#125;; 第一种方法很容易出错，让我们假设在一段时间以后，你把MaxElements的值从32提高到100，但是你没有修改需要序列化的比特的数目，也就是需要序列化的比特的数目还是6（注意看WriteBits( writer, numElements, 6 )的6， 现在需要7了）。哎呦。因为你忘记了在读取和写入函数里面更新比特的数目，那么现在当你在发送数据的时候你会把高位进行截断。事后追查这样的事情是相当困难的。让我们通过增加一些编译时候的检测来计算出所需要的比特的位数： 12345678910111213141516171819202122232425262728template &lt;uint32_t x&gt; struct PopCount&#123; enum &#123; a = x - ( ( x &gt;&gt; 1 ) &amp; 0x55555555 ), b = ( ( ( a &gt;&gt; 2 ) &amp; 0x33333333 ) + ( a &amp; 0x33333333 ) ), c = ( ( ( b &gt;&gt; 4 ) + b ) &amp; 0x0f0f0f0f ), d = c + ( c &gt;&gt; 8 ), e = d + ( d &gt;&gt; 16 ), result = e &amp; 0x0000003f &#125;; &#125;;template &lt;uint32_t x&gt; struct Log2&#123; enum &#123; a = x | ( x &gt;&gt; 1 ), b = a | ( a &gt;&gt; 2 ), c = b | ( b &gt;&gt; 4 ), d = c | ( c &gt;&gt; 8 ), e = d | ( d &gt;&gt; 16 ), f = e &gt;&gt; 1, result = PopCount&lt;f&gt;::result &#125;;&#125;;template &lt;int64_t min, int64_t max&gt; struct BitsRequired&#123; static const uint32_t result = ( min == max ) ? 0 : ( Log2&lt;uint32_t(max-min)&gt;::result + 1 );&#125;;#define BITS_REQUIRED( min, max ) BitsRequired&lt;min,max&gt;::result 哦，太好了。模板元编程和宏。感谢格伦！ 但是这么做真的真棒，相信我！因为你现在没有办法弄乱需要的比特的数目了： 12345678910111213141516171819202122232425const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) &#123; WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i], 32 ); &#125; void Read( BitReader &amp; reader ) &#123; ReadBits( reader, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( buffer, elements[i], 32 ); &#125;&#125;; 当然现在也有机会犯错。MaxElements的值是32所以BITS_REQUIRED(0,32) 返回的是6，因为5比特所能得到的取值范围只有【0，31】。这没什么问题，但是现在我们有概率把未定义的值进行插入，如果有一个恶意发送者发送了一个取值范围在【33,63】之间的值的话。当你在长度为32的数组里面读入63个整数的时候会发生什么？当然，你可以对取值范围进行限制来修正这个问题，但是仔细想想。。。如果你从位读取器得到了一个值但是它超出了取值范围，你要么就是让读取和写入操作完全不同步（这显然是你自己的错误）或者有人试图坑你。所以，不要对取值范围进行限制。如果遇到这种情况，直接停止对数据包的读取并且丢弃这个数据包。我还想在这里提到一个陷阱，因为这个陷阱看上去很方便，但是其实它会让代码运行的很慢。我有一次使用异常实现了数据包的读取中断。它看上去很棒，因为在一个递归的位打包器读取函数里面你可能会有28层调用堆栈，而你想要做的不过是展开堆栈然后回到数据包读取函数调用的地方，但是异常真的太慢太慢了。为了取代异常，有两种方法可以运行的快得多：1)在位读取器那里设置一个值表明这个数据包应该被丢弃，2）升级数据包的读取函数让它在读取失败的时候返回false。但是现在，你可以使用这里面的任意一种方法，为了实现读取时候的安全性，你需要检测上面说的标记或者在每一次读取的时候返回一个值，否则如果遇到读取失败的情况，你还是会一直继续读取直到把内存全部耗光。 123456789101112131415161718192021222324252627282930313233343536const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB&#123; int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) &#123; WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i], 32 ); &#125; void Read( BitReader &amp; reader ) &#123; ReadBits( reader, numElements, MaxElementBits ); if ( numElements &gt; MaxElements ) &#123; reader.Abort(); return; &#125; for ( int i = 0; i &lt; numElements; ++i ) &#123; if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); &#125; &#125;&#125;; 但是这么做了以后，整个读取函数就开始变得非常的复杂，而且如果有什么地方你漏过了这些检查的话，你就把自己置于缓冲区溢出和无线循环这种危险的境地。你不会希望在写数据包读取函数的时候全部变成一个手动的过程，你肯定是希望这个过程是自动化的。因为这给了程序员太多犯错误的机会。 请继续关注下一篇文，在那篇文章里面我将向你展现如何使用C++来用非常简洁的方式实现。 这个系列的下一篇文章是《序列化策略》。 在这个系列的下一篇文章，我将向你展示如何将读取和写入函数统一到一个单独的序列化函数里面，它可以在不增加任何运行时损耗的情况同时处理读取和写入。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟六之状态同步]]></title>
    <url>%2F2019%2F05%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E5%85%AD%E4%B9%8B%E7%8A%B6%E6%80%81%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[自我总结状态同步的要点为 : input+state : 既通过网络发送输入信息又会发送状态信息来进行同步 发送端 优先级累加器 : 只发送一些重要的实体状态更新, 而不是所有都发. 如果遇到一个物体的状态更新信息不合适放到这个数据包里面，那么就跳过这个物体并尝试下一个。当你序列化完这个数据包以后，将那些已经在这帧更新过的物体在优先级累加器里面的值重置为0，但是那些没有在这帧更新过的物体在优先级累加器里面的值则保持不变。 接收端 抗网络抖动 : 做一个jitter buffer来缓冲数据, 然后以相同时间的间隔均匀取出 应用状态更新 : 一旦你的数据包从抖动缓冲器里面出来，你该在状态更新直接应用这些信息进行仿真。 对两边都量化(这里的量化指的是&lt;&lt;网络物理模拟五之快照压缩>&gt;说的量化压缩技术) : 如果只有接收端用了量化的数据, 那接收端模拟的结果很可能与发送端不同, 所以要对两边都量化来避免发送端和接收端模拟的差异 长时间丢包的平滑处理 : 对于不同的网络断开时间用不同的平滑因子, 来自适应误差 增量压缩 : 相对编码 : 在数据包的包头里面发送最近确认的数据包的序列号（这个数据是从可靠的确认系统里面得到的）然后对每个物体编码相对这个基准帧的偏移量 绝对编码 原文原文出处 原文标题 : State Synchronization (Keeping simulations in sync by sending state) IntroductionHi, I’m Glenn Fiedler and welcome to Networked Physics. In the previous article we discussed techniques for compressing snapshots. In this article we round out our discussion of networked physics strategies with state synchronization, the third and final strategy in this article series. State SynchronizationWhat is state synchronization? The basic idea is that, somewhat like deterministic lockstep, we run the simulation on both sides but, unlike deterministic lockstep, we don’t just send input, we send both input and state. This gives state synchronization interesting properties. Because we send state, we don’t need perfect determinism to stay in sync, and because the simulation runs on both sides, objects continue moving forward between updates. This lets us approach state synchronization differently to snapshot interpolation. Instead of sending state updates for every object in each packet, we can now send updates for only a few, and if we’re smart about how we select the objects for each packet, we can save bandwidth by concentrating updates on the most important objects. So what’s the catch? State synchronization is an approximate and lossy synchronization strategy. In practice, this means you’ll spend a lot of time tracking down sources of extrapolation divergence and pops. But other than that, it’s a quick and easy strategy to get started with. ImplementationHere’s the state sent over the network per-object: 12345678struct StateUpdate&#123; int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity;&#125;; Unlike snapshot interpolation, we’re not just sending visual quantities like position and orientation, we’re also sending non-visual state such as linear and angular velocity. Why is this? The reason is that state synchronization runs the simulation on both sides, so it’s always extrapolating from the last state update applied to each object. If linear and angular velocity aren’t synchronized, this extrapolation is done with incorrect velocities, leading to pops when objects are updated. While we must send the velocities, there’s no point wasting bandwidth sending (0,0,0) over and over while an object is at rest. We can fix this with a trivial optimization, like so: 1234567891011121314151617181920void serialize_state_update( Stream &amp; stream, int &amp; index, StateUpdate &amp; state_update )&#123; serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) &#123; serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); &#125; else if ( stream.IsReading() ) &#123; state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); &#125;&#125; What you see above is a serialize function. It’s a trick I like to use to unify packet read and write. I like it because it’s expressive while at the same time it’s difficult to desync read and write. You can read more about them here. Packet StructureNow let’s look at the overall structure of packets being sent: 12345678910const int MaxInputsPerPacket = 32;const int MaxStateUpdatesPerPacket = 64;struct Packet&#123; uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket];&#125;; First we include a sequence number in each packet so we can determine out of order, lost or duplicate packets. I recommend you run the simulation at the same framerate on both sides (for example 60HZ) and in this case the sequence number can work double duty as the frame number. Input is included in each packet because it’s needed for extrapolation. Like deterministic lockstep we send multiple redundant inputs so in the case of packet loss it’s very unlikely that an input gets dropped. Unlike deterministic lockstep, if don’t have the next input we don’t stop the simulation and wait for it, we continue extrapolating forward with the last input received. Next you can see that we only send a maximum of 64 state updates per-packet. Since we have a total of 901 cubes in the simulation so we need some way to select the n most important state updates to include in each packet. We need some sort of prioritization scheme. To get started each frame walk over all objects in your simulation and calculate their current priority. For example, in the cube simulation I calculate priority for the player cube as 1000000 because I always want it to be included in every packet, and for interacting (red cubes) I give them a higher priority of 100 while at rest objects have priority of 1. Unfortunately if you just picked objects according to their current priority each frame you’d only ever send red objects while in a katamari ball and white objects on the ground would never get updated. We need to take a slightly different approach, one that prioritizes sending important objects while also distributing updates across all objects in the simulation. Priority AccumulatorYou can do this with a priority accumulator. This is an array of float values, one value per-object, that is remembered from frame to frame. Instead of taking the immediate priority value for the object and sorting on that, each frame we add the current priority for each object to its priority accumulator value then sort objects in order from largest to smallest priority accumulator value. The first n objects in this sorted list are the objects you should send that frame. You could just send state updates for all n objects but typically you have some maximum bandwidth you want to support like 256kbit/sec. Respecting this bandwidth limit is easy. Just calculate how large your packet header is and how many bytes of preamble in the packet (sequence, # of objects in packet and so on) and work out conservatively the number of bytes remaining in your packet while staying under your bandwidth target. Then take the n most important objects according to their priority accumulator values and as you construct the packet, walk these objects in order and measure if their state updates will fit in the packet. If you encounter a state update that doesn’t fit, skip over it and try the next one. After you serialize the packet, reset the priority accumulator to zero for objects that fit but leave the priority accumulator value alone for objects that didn’t. This way objects that don’t fit are first in line to be included in the next packet. The desired bandwidth can even be adjusted on the fly. This makes it really easy to adapt state synchronization to changing network conditions, for example if you detect the connection is having difficulty you can reduce the amount of bandwidth sent (congestion avoidance) and the quality of state synchronization scales back automatically. If the network connection seems like it should be able to handle more bandwidth later on then you can raise the bandwidth limit. Jitter BufferThe priority accumulator covers the sending side, but on the receiver side there is much you need to do when applying these state updates to ensure that you don’t see divergence and pops in the extrapolation between object updates. The very first thing you need to consider is that network jitter exists. You don’t have any guarantee that packets you sent nicely spaced out 60 times per-second arrive that way on the other side. What happens in the real world is you’ll typically receive two packets one frame, 0 packets the next, 1, 2, 0 and so on because packets tend to clump up across frames. To handle this situation you need to implement a jitter buffer for your state update packets. If you fail to do this you’ll have a poor quality extrapolation and pops in stacks of objects because objects in different state update packets are slightly out of phase with each other with respect to time. All you do in a jitter buffer is hold packets before delivering them to the application at the correct time as indicated by the sequence number (frame number) in the packet. The delay you need to hold packets for in this buffer is a much smaller amount of time relative to interpolation delay for snapshot interpolation but it’s the same basic idea. You just need to delay packets just enough (say 4-5 frames @ 60HZ) so that they come out of the buffer properly spaced apart. Applying State UpdatesOnce the packet comes out of the jitter how do you apply state updates? My recommendation is that you should snap the physics state hard. This means you apply the values in the state update directly to the simulation. I recommend against trying to apply some smoothing between the state update and the current state at the simulation level. This may sound counterintuitive but the reason for this is that the simulation extrapolates from the state update so you want to make sure it extrapolates from a valid physics state for that object rather than some smoothed, total bullshit made-up one. This is especially important when you are networking large stacks of objects. Surprisingly, without any smoothing the result is already pretty good: As you can see it’s already looking quite good and barely any bandwidth optimization has been performed. Contrast this with the first video for snapshot interpolation which was at 18mbit/sec and you can see that using the simulation to extrapolate between state updates is a great way to use less bandwidth. Of course we can do a lot better than this and each optimization we do lets us squeeze more state updates in the same amount of bandwidth. The next obvious thing we can do is to apply all the standard quantization compression techniques such as bounding and quantizing position, linear and angular velocity value and using the smallest three compression as described in snapshot compression. But here it gets a bit more complex. We are extrapolating from those state updates so if we quantize these values over the network then the state that arrives on the right side is slightly different from the left side, leading to a slightly different extrapolation and a pop when the next state update arrives for that object. Quantize Both SidesThe solution is to quantize the state on both sides. This means that on both sides before each simulation step you quantize the entire simulation state as if it had been transmitted over the network. Once this is done the left and right side are both extrapolating from quantized state and their extrapolations are very similar. Because these quantized values are being fed back into the simulation, you’ll find that much more precision is required than snapshot interpolation where they were just visual quantities used for interpolation. In the cube simulation I found it necessary to have 4096 position values per-meter, up from 512 with snapshot interpolation, and a whopping 15 bits per-quaternion component in smallest three (up from 9). Without this extra precision significant popping occurs because the quantization forces physics objects into penetration with each other, fighting against the simulation which tries to keep the objects out of penetration. I also found that softening the constraints and reducing the maximum velocity which the simulation used to push apart penetrating objects also helped reduce the amount of popping. With quantization applied to both sides you can see the result is perfect once again. It may look visually about the same as the uncompressed version but in fact we’re able to fit many more state updates per-packet into the 256kbit/sec bandwidth limit. This means we are better able to handle packet loss because state updates for each object are sent more rapidly. If a packet is lost, it’s less of a problem because state updates for those objects are being continually included in future packets. Be aware that when a burst of packet loss occurs like 1⁄4 a second with no packets getting through, and this is inevitable that eventually something like this will happen, you will probably get a different result on the left and the right sides. We have to plan for this. In spite of all effort that we have made to ensure that the extrapolation is as close as possible (quantizing both sides and so on) pops can and will occur if the network stops delivering packets. Visual SmoothingWe can cover up these pops with smoothing. Remember how I said earlier that you should not apply smoothing at the simulation level because it ruins the extrapolation? What we’re going to do for smoothing instead is calculating and maintaining position and orientation error offsets that we reduce over time. Then when we render the cubes in the right side we don’t render them at the simulation position and orientation, we render them at the simulation position + error offset, and orientation * orientation error. Over time we work to reduce these error offsets back to zero for position error and identity for orientation error. For error reduction I use an exponentially smoothed moving average tending towards zero. So in effect, I multiply the position error offset by some factor each frame (eg. 0.9) until it gets close enough to zero for it to be cleared (thus avoiding denormals). For orientation, I slerp a certain amount (0.1) towards identity each frame, which has the same effect for the orientation error. The trick to making this all work is that when a state update comes in you take the current simulation position and add the position error to that, and subtract that from the new position, giving the new position error offset which gives an identical result to the current (smoothed) visual position. The same process is then applied to the error quaternion (using multiplication by the conjugate instead of subtraction) and this way you effectively calculate on each state update the new position error and orientation error relative to the new state such that the object appears to have not moved at all. Thus state updates are smooth and have no immediate visual effect, and the error reduction smoothes out any error in the extrapolation over time without the player noticing in the common case. I find that using a single smoothing factor gives unacceptable results. A factor of 0.95 is perfect for small jitters because it smooths out high frequency jitter really well, but at the same time it is too slow for large position errors, like those that happen after multiple seconds of packet loss: The solution I use is two different scale factors at different error distances, and to make sure the transition is smooth I blend between those two factors linearly according to the amount of positional error that needs to be reduced. In this simulation, having 0.95 for small position errors (25cms or less) while having a tighter blend factor of 0.85 for larger distances (1m error or above) gives a good result. The same strategy works well for orientation using the dot product between the orientation error and the identity matrix. I found that in this case a blend of the same factors between dot 0.1 and 0.5 works well. The end result is smooth error reduction for small position and orientation errors combined with a tight error reduction for large pops. As you can see above you don’t want to drag out correction of these large pops, they need to be fast and so they’re over quickly otherwise they’re really disorienting for players, but at the same time you want to have really smooth error reduction when the error is small hence the adaptive error reduction approach works really well. Delta CompressionEven though I would argue the result above is probably good enough already it is possible to improve the synchronization considerably from this point. For example to support a world with larger objects or more objects being interacted with. So lets work through some of those techniques and push this technique as far as it can go. There is an easy compression that can be performed. Instead of encoding absolute position, if it is within a range of the player cube center, encode position as a relative offset to the player center position. In the common cases where bandwidth is high and state updates need to be more frequent (katamari ball) this provides a large win. Next, what if we do want to perform some sort of delta encoding for state synchronization? We can but it’s quite different in this case than it is with snapshots because we’re not including every cube in every packet, so we can’t just track the most recent packet received and say, OK all these state updates in this packet are relative to packet X. What you actually have to do is per-object update keep track of the packet that includes the base for that update. You also need to keep track of exactly the set of packets received so that the sender knows which packets are valid bases to encode relative to. This is reasonably complicated and requires a bidirectional ack system over UDP. Such a system is designed for exactly this sort of situation where you need to know exactly which packets definitely got through. You can find a tutorial on how to implement this in this article. So assuming that you have an ack system you know with packet sequence numbers get through. What you do then is per-state update write one bit if the update is relative or absolute, if absolute then encode with no base as before, otherwise if relative send the 16 bit sequence number per-state update of the base and then encode relative to the state update data sent in that packet. This adds 1 bit overhead per-update as well as 16 bits to identify the sequence number of the base per-object update. Can we do better? Yes. In turns out that of course you’re going to have to buffer on the send and receive side to implement this relative encoding and you can’t buffer forever. In fact, if you think about it you can only buffer up a couple of seconds before it becomes impractical and in the common case of moving objects you’re going to be sending the updates for same object frequently (katamari ball) so practically speaking the base sequence will only be from a short time ago. So instead of sending the 16 bit sequence base per-object, send in the header of the packet the most recent acked packet (from the reliability ack system) and per-object encode the offset of the base sequence relative to that value using 5 bits. This way at 60 packets per-second you can identify an state update with a base half a second ago. Any base older than this is unlikely to provide a good delta encoding anyway because it’s old, so in that case just drop back to absolute encoding for that update. Now lets look at the type of objects that are going to have these absolute encodings rather than relative. They’re the objects at rest. What can we do to make them as efficient as possible? In the case of the cube simulation one bad result that can occur is that a cube comes to rest (turns grey) and then has its priority lowered significantly. If that very last update with the position of that object is missed due to packet loss, it can take a long time for that object to have its at rest position updated. We can fix this by tracking objects which have recently come to rest and bumping their priority until an ack comes back for a packet they were sent in. Thus they are sent at an elevated priority compared with normal grey cubes (which are at rest and have not moved) and keep resending at that elevated rate until we know that update has been received, thus “committing” that grey cube to be at rest at the correct position. ConclusionAnd that’s really about it for this technique. Without anything fancy it’s already pretty good, and on top of that another order of magnitude improvement is available with delta compression, at the cost of significant complexity! 译文译文出处 译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 介绍大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。 在这篇文章，我们将讨论第三种也是最后一种同步的策略：状态同步。 状态同步概念在我看来，这是最简单的同步策略也是最容易理解的同步策略。事实上，这是我开始实现《雇佣兵2：战火纷飞》的网络物理部分的时候我首先尝试的同步策略。这个同步策略的基本想法是我们在网络的两侧同时运行仿真，但是与具有确定性的帧同步不同的是帧同步会通过网络发送输入信息并且依赖完美的确定性来保持同步，这种同步策略是既通过网络发送输入信息又会发送状态信息来进行同步。 这就赋予了状态同步与之前的同步策略完全不同的属性。与具有确定性的帧同步不同，这种同步策略不需要要求确定性来保持同步，因为我们可以迅速的通过网络发送状态来纠正任何的偏差。这种同步策略也跟快照信息的插值不同，如果一个对象不在数据包里面的话，这个物体还是会继续移动，因为网络两侧的仿真都在持续的运行。 正是由于这一特性，状态同步的实现方法才会与快照信息的插值有差别。不再是在每个数据包里面发送每个物体的状态更新信息，我们可以只对几个对象进行更新。如果我们在每个数据包选择要同步的物体的时候方案比较聪明的话，我们可以更有效地利用带宽，把注意力主要集中在最重要的物体的更新上，而那些不那么重要的物体，他们的更新信息可以以一个较低的速率进行发送。这样的话，相比较快照信息插值这种要在一个快照里面包括所有物体的方法，状态同步这种方法使用的带宽可以减少一个数量级。此外，状态同步这种方法不会在网络延迟之上还要附加插值带来的延迟，因为它相比较于快照信息插值这种方法，延迟也更低。 这样做的代价是状态同步是一个近似和有损的同步策略。如果推送信息的时候出现了一些问题导致大量的数据包丢失的话，远程的模拟仿真使用的是过期的数据进行预测。根据我的经验，如果使用这个同步策略的话，你会花很多时间追踪由于进行预测所带来的差异。如果使用这个同步策略的话，在大量物体堆叠的时候，会看到很多物体的移动不正常，并且很难精确地追查。在这篇文章中，我会告诉你如何追踪并通过网络发送量化和压缩的物理状态来减少分歧的根源。 实现让我们从实际的实现来看下这个同步策略。这里是每个发送的对象的网络状态： 12345678struct StateUpdate &#123; int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity;&#125;; 需要注意的是，我们发送的不仅仅是一些像位置、方向这样的视觉信息，这个地方与快照信息插值那种方法相同，我们还发送了很多非视觉的物体状态信息，比如线性速度和角速度，这是与快照信息插值那种方法不同的地方。这么做是必要的是因为物理仿真需要对每个物体最近的状态进行外推。因此，状态更新需要提供所有进行外推所需的信息，以便能够正确的进行推测。如果一个物体的速度信息没有发送的话，在预测物体前进的时候，就会使用一个不正确的速度信息，这将导致下一次物体信息进行更新的时候有一个拉扯。 当我们在网络上对状态更新进行序列化的时候，没有必要对不动的物体浪费网络带宽，为这些不动的物体发送什么(0,0,0)来表示线性速度和角速度。我们可以做一个简单的优化，通过把物体的静止状态包含在内来给每个静止的物体节省24字节的带宽： 123456789101112131415161718192021void serialize_state_update( Stream &amp; stream, int &amp; index, StateUpdate &amp; state_update )&#123; serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) &#123; serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); &#125; else if ( stream.IsReading() ) &#123; state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); &#125;&#125; 上面的代码就是我所谓的序列化功能。这里面有一个我喜欢的小技巧来统一位打包器的读取和写入函数，它们通常是分开实现的。这个函数会在两种不同的上下文中进行调用：写入的时候和读取的时候。你可以通过IsReading/IsWriting函数来知道自己目前处在哪个上下文。我喜欢这个技巧的原因是如何读取和写入功能统一在一个函数的时候，读取和写入的不同步就会很少发生。如果你希望读取和写入功能统一在一起并且像我这样进行数据包的数据，请参考这里。 数据包结构体当把状态更新写入的时候，如果这个物体是静止不动的话，这个函数其实只序列化了一比特的信息而不会更新线性速度和角速度的信息。如果这个物体不是静止不动的话，会把线性速度和角速度的信息写入之前先写入一比特的信息。在从数据包进行读取的时候，代码会读取这个比特位，如果这个比特位是0的话，会从这个比特流里面读取线性速度和角速度的信息，否则的话，会把物体的线性速度和角速度全部清为（0,0,0）。这是一个非常简单而有效的无损带宽压缩策略能够针对静止不动的物体进行数据的压缩，能够节省将近一半的带宽。 接下来让我们看一下被发送的数据包的结构： 1234567891011const int MaxInputsPerPacket = 32;const int MaxStateUpdatesPerPacket = 64; struct Packet&#123; uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket];&#125;; 从上面的数据包结构中，你可以看到，首先登场的是我们在每个数据包包含的序列号，通过这个数据信息我们可以判断数据包是否出故障、丢失或者重复。我强烈建议你在网络两侧的运行都按照相同的帧速率（比如说60fps）进行仿真。在这种情况下，你还可以给序列号赋予另外一重任务：作为状态更新的帧号。 输入信息被包含在每个数据包里面，这是因为仿真需要输入信息才能进行外推。当仿真在网络的另外一侧运行的时候，我们希望通过状态更新的信息以及运行玩家相同的输入信息来往前预测后续状态的信息，并且希望预测出来的状态能够和真实的状态尽可能的接近。就跟具有确定性的帧同步一样，我们发送了多个冗余输入信息，这样即使在有包丢失的情况下，输入信息也不太可能完全被丢弃而不能到达网络的另外一端，但是跟具有确定性的帧同步不一样的是，就算是最坏的情况下（也就是我们没有收到后续的输入信息的情况），我们本地的仿真也不会停止并且等待后续的输入信息的到来，我们还是会根据最后收到的输入信息继续往前模拟。 接下来，你可以看到，在一个数据包里面我们最多发送64个状态更新。我们在仿真的场景中一共有901个立方体，所以我们需要一些方法来从这901个立方体里面选出一些最重要的立方体，在每一个数据包进行数据更新。我们需要某种优先级方案，这样我们才能找到最重要的物体，允许我们只会很频繁的发送最重要的物体的状态信息，而那些不怎么重要的物体的更新就会不那么的频繁，零零散散的更新，这样保证所有对象都有机会进行更新和发送，但是又能让最重要的那些物体始终得到更新。 这样就要求在仿真的每一帧开始的时候遍历所有的物体并且计算它们当前的优先级。让我们举个简单的例子来说，在立方体模拟这个场景中，我把玩家立方体的优先级设为100000，因为我希望玩家立方体的更新信息能够包含在每个数据包里面，而对于发生交互的立方体（那些红色的立方体），我赋予它们的优先级为100，而所有静止不动的立方体，优先级为1。 非常遗憾的是如果只有这一个机制的话，是不足以公平分配对象的更新的，这是因为如果你刚刚仅仅是在每一帧对物体的当前优先级进行了排序，这样的话，如果人物立方体和红色立方体有交互的话，那么就永远只有红色立方体的信息会被发送，而地面上的白色立方体则永远不会更新。我们需要一个稍微不同的方法，优先发送重要的对象，同时也会在仿真的过程让那些不重要的物体也有更新的机会。 优先级累加器你可以通过优先级累加器做到这一点。这是一组浮点数值，每个对象都会有一个对应的浮点数值，在帧与帧之间会一直保留。有了这个值以后，不再是根据当前帧中这个物体的重要性进行排序，而是在每一帧中将每个物体的重要性加到这个优先级累加器中，然后对优先级累加器的值进行从大到小进行排序。这个排序的顺序中前面的物体就是这一帧中你应该发送的物体。 你可以为所有的N个物体发送状态更新信息，但是通常情况下你的带宽会有一些限制，比如说你需要控制在256k比特每秒。尊重这个带宽限制是很容易的。只要计算出你的数据包Header有多大，并且计算下数据包的preamble部分有多大（这主要是指序号、标记哪些物体在这个数据包等信息），这样就能计算出来你的数据包还剩下多少字节，可以通过计划传递多少个物体的更新信息来确保带宽小于约定的最大带宽。 然后根据它们的优先级累加器里面的值选取数目和上面计算相符合的n个最重要的物体，然后用这n个最重要的物体的更新信息来构建你的数据包。对这n个最重要的物体进行依次遍历并测试它们的数据更新信息是否应该放在这个数据包里面。如果遇到一个物体的状态更新信息不合适放到这个数据包里面，那么就跳过这个物体并尝试下一个。当你序列化完这个数据包以后，将那些已经在这帧更新过的物体在优先级累加器里面的值重置为0，但是那些没有在这帧更新过的物体在优先级累加器里面的值则保持不变。 通过使用这个办法，刚才检测不合适放到数据包的物体的更新信息会被首先包含在下一帧的数据包里面。使用这种同步策略，所需的带宽甚至可以动态调整。这使得这种同步策略可以很容易的根据不断变化的网络条件来调整状态同步的信息量，让我们举个简单的例子来说，如果你发现连接有困难，就可以减少发送所占的带宽（拥塞避免），这样状态同步的规模会逐步的自动回复回来。如果网络连接似乎可以处理更多的带宽，那么就可以把带宽限制提高。我们这里所做的处理主要是在网络发送这一端。 抖动缓冲区对物体做了优先级的排序，并且在每帧里面更新物体在优先级累加器里面的值，并在每次发送数据包的时候只发送n个最重要的物体。但是在网络接收这一端，还有很多需要做的事情，比如当应用接收过来的状态更新信息的时候，如何避免与之前预测的物体状态信息之间的差异会被玩家感觉到。 你需要考虑的第一件事情是，网络抖动的存在。你没有任何办法来确保你发送的数据包就是完美的是每秒60次的频率抵达网络的另外一侧。在现实世界中会发生的事情是你可能在一帧中收到两个数据包，然后在下一帧一个数据包也收不到，然后下面几帧可能是1个，2个或者0个。为了处理这种情况，你需要实现一个抖动缓冲器来保存你的状态更新数据包。 如果你不这样做，所做的推测质量就很难保证而且会出现对象堆叠的情况，这是因为在不同的状态更新数据包里面，每一个物体的信息都会有些轻微的变化。 在抖动缓冲器你所要做的事情就是保存这些数据包，然后根据数据包里面的序号（其实也就是帧号了）来在正确的时间将数据包发给应用程序处理。你需要在这个缓冲区来保存数据包所导致的延迟相比较快照信息插值所带来的延迟是一段非常微小的时间，但是这两种同步策略的基本思想是一致的。你只要稍微延迟一下数据包让时间刚刚好就好（比如说每秒60次更新的情况延迟个4-5帧），这样数据包就能够以合适的间隔从缓冲区里面出来到达应用程序。 应用状态更新一旦你的数据包从抖动缓冲器里面出来，你该如何使用数据包的信息进行状态更新呢？我的建议是，你要努力对齐这种状态。在状态更新直接应用这些信息进行仿真。我反对在状态更新和当前仿真的目前状态之间做一些平滑的更替。这听起来可能有悖常理，但这样做的原因是，当前有些物体的状态可能是根据之前状态推测出来的，所以你要保证这个物体的预测信息是从物体有效的物理状态出发，而不是一些平滑出来的完全是虚假的数据出发。这在你有大量的物体对象的时候会格外的重要。到目前为止，我们已迅速建立了一个实用的同步策略而没做有太多的工作。事实上，这种同步策略已经足够好了，已经完全可以在互联网上进行游戏对战了，并且它对丢包、抖动和带宽限制都处理的相当不错。 【视频1：cube_state_sync_uncompressed】 正如你可以看到的那样，这种同步策略已经看上去相当不错了，并且几乎没有做任何的带宽优化。与快照信息插值那种策略的第一次18m每秒的信息量相比，你可以看到在状态更新之间进行状态的推测是使用更少的带宽的好方法。当然，我们可以做得比现在的状态好的多，每次我们做优化的时候我们都可以使用相同的带宽来传递更多的物体状态更新信息。我们可以做的下一个明显有效果的事情是应用所有的标准量化压缩技术，比如压缩边界和量化位置、线性速度和角速度的值以及如同《网络物理模拟五之快照压缩》里面的描述的“最小的三个变量”方法。但在这里它变得更复杂一点。我们从这些状态更新向前进行推测，所以如果我们量化这些通过网络传递的值的话，那么到达右侧的状态将与左侧的状态稍有不同，这会让推测变得更加不准，并且当下一个状态更新到达的时候会出现一些拉扯现象。 【视频2： cube_state_sync_compressed】 对两边都量化解决的办法是量化两侧的状态。这意味着，在每一个仿真进行一次更新之前，你要对网络的两侧同时量化整个模拟状态，就好像它们都已经在网络上传输了一样。一旦这样做了的话，左侧和右侧都是从量化的状态进行推测，这样它们的推测结果就会非常的接近。由于这些量化以后的值会被反馈到仿真中去，你会发现这种方法对精确度的要求比快照信息插值的方法所要求的精确度要高的多，因为在快照信息插值的方法里面，使用的数据只是用来插值的视觉信息。在立方体模拟这个情况下，我发现有必要对于每米要有4096个位置的精度，而在快照信息插值的方法里每米只要有512个位置的精度就可以了，所以四元数最小的三个分量每个要15比特位（在快照信息插值的方法里四元数最小的三个分量每个只要9个比特位的信息就行）。如果没有这种额外的精度出现，就非常容易出现物体的拉扯的情况，这是因为量化以后的数据会迫使物理对象相互渗透，这与模拟要所求的尽量保持物理对象不相互渗透的努力是背道而驰的。我还发现，软化约束以及减少模拟用于推开相互渗透的物理对象的最大速度也有助于减少出现物体拉扯的情况。 【视频3： cube_state_sync_quantize_both_sides】 在量化应用于网络两侧的模拟以后，就可以再次看到结果是比较完美的。这种处理以后，看起来视觉效果与未压缩版本差不多，但事实上通过这种方案我们能够适应每个包进行更多的状态更新，同时还能满足每秒256比特的带宽限制。这意味着我们能够更好地处理数据包的丢失，因为每一个对象的状态更新可以更迅速的发送。如果出现数据包丢失的情况，对整个模拟来说也会引发更少的问题，这是因为通过未来到来的书包正在持续不断地对这些物体进行状态更新。 请注意如果出现数据包的集中丢弃的情况，比如说在四分之一秒的时间没有数据包通过，这种情况是不可避免的，总是会发生一些这样的事情，你可能会在网络的两侧得到完全不同的结果。我们必须为这种情况进行规划。我们会尽一切努力来确保外推是尽可能与实际结果相接近的（采用在网络的两侧进行量化以及其他一些方案），但是由于网络停止传输数据包，还是会发生各种拉扯和不准确的情况。 视觉平滑还记得我之前说过的那个事情么？你不应该对模拟这一侧使用平滑算法，因为它会对外推有不好的影响吗? 我们要做的不是平滑而是计算和维护位置和方向的误差补偿，这个量会随着时间而减少。然后当我们在网络的右侧渲染立方体的时候，我们并不是用模拟的位置和方向对这些立方体进行渲染，我们是用模拟的位置和方向再加上误差补偿来对这些立方体进行渲染。位置信息是模拟位置信息加上误差补偿，方向信息是模拟的方向信息再乘以方向的误差补偿。 随着时间的推移，我们努力减少这些误差补偿，让位置的误差补偿尽量趋近于0，而方向的误差补偿尽量趋近于一致。为了减少误差，我使用了一个指数平滑的移动，平均线趋近零。所以实际上，我用每一帧的位置误差乘以某个系数（比如说是0.9），直到它接近于零而被清除(这样就避免了突变)。对于方向而言，我用某一个固定的量（比如说是0.1）来对每一帧的标准向量进行球面插值，这个可以达到方向误差相同的效果。 让所有事情都能够正常运行的诀窍在于当一个状态更新数据包到达的时候，你获取当前的模拟位置信息，并把位置误差添加上去，然后再从新的位置里面减去这个值，这样就可以让新位置的位置误差和当前的视觉位置比较一致（平滑）。然后把相同的过程应用于四元数误差（使用乘法的共轭而不是减法来与基准方向信息进行叠加），通过这种方法你就可以有效的计算在每个状态更新数据包到达的时候，相对于新的状态下新的位置误差和方向误差，这样处理的话物体看上去就根本没有进行任何的移动。因此状态更新的非常平滑，没有任何突然移动的视觉效果，而且可以随着时间慢慢减少由于推断带来的误差而通常情况下这么处理不会让玩家注意到。 我发现只使用一个单独的平滑因子会产生不可接受的结果。平滑因子0.95对于那些小的抖动来说是非常完美的，因为它对那些高频抖动的平滑是非常完美的，但是它对于大的位置误差来说平滑的太慢了，比如说发生了好几秒数据包丢失以后，物体的位置和实际位置差的比较大，这时候用这个因子来平滑就太慢了： 【视频4：cube_state_sync_basic_smoothing】 我使用的解决方案是针对不同的误差距离使用两个不同的平滑因子，并且我会根据需要减少的位置误差的大小来对这两个平滑因子进行线性的混合来让过渡非常的平滑。在这个模拟中，我使用的是0.95来平滑小的位置误差(针对25厘米或者误差更小的情况)，而对于大一点的距离而言会使用一个更严格的混合系数0.85(针对1米或者误差更大的情况)，这给出了一个非常好的结果。对于方向而言，相同的策略适用于对方向误差和单位矩阵使用点积的情况。我发现在这种情况下，混合系数分别采用0.1和0.5的效果就非常的好。 最终的结果是对小的位置误差和方向误差的平滑操作与对大的位置误差和方向误差的快速收敛很好的结合在了一起。正如你在上面看到的那样，你不想拖着一直不处理这些大的位置误差和方向误差，这些大的位置误差和方向误差需要被快速的解决否则它们会给玩家造成非常大的困扰，但是同时当位置误差和方向误差很小的时候你希望这个误差减少的过程能够非常的平滑，因此自适应误差减少方法效果很好。 【视频5：cube_state_sync_adaptive_smoothing.mp4】 增量压缩尽管我认为上述结果可能已经足够好了，从这一点上来看可以大大提高同步的质量。让我们举些简单的例子来说明，比如支持一个有大量对象的世界或者有更多的对象与之交互。所以让我们通过一些技术上的改进，来推动这项技术尽可能的完美。 有一种简单的压缩，可以立刻执行。不再是编码绝对位置，如果位置是在玩家立方体中心的某个范围之内的话，就会以玩家的中心位置的偏移量来进行编码。如果是常见情况下，带宽很高而且状态更新需要非常的频繁（katamari球），通过这种方法就能节省下很多带宽。 接下来，如果我们想对状态同步执行某种增量编码怎么办? 我们可以做到但是具体的方法会和快照里面的增量编码方法差别很大，这是因为在这种情况下我们的每个数据包不会包含每一个立方体的信息，所以我们不能跟踪最新收到的数据包，并且自以为地觉得这个数据包的所有这些状态更新都是相对于X这个数据包的。 你实际要做的就是逐对象的进行更新，对数据包进行跟踪包括更新的基础值。你还需要跟踪收到的数据包的准备的数量，这样发送方才能知道哪些数据包可以作为增量编码有效的基础值。这是相当复杂的，并且是需要通过UDP协议进行双向确认的系统。这样一个系统是专为这种情况设计的，因为你肯定需要知道哪些数据包确定是到达了另外一侧。你可以在这个教程里面找到具体如何实现这个功能的指南。 所以假设你有一个确认系统，这样你就知道已经发送到网络另外一侧的数据包的序列号。你所要做的就是在每个状态更新的时候，用一位数据来记录下这个更新到底是相对更新还是绝对更新，如果是绝对更新就没有针对基础编码这回事，否则就是一个相对更新，所以要发送16位序列号来标记每个状态相对应的基础状态，然后相对于基础状态对更新数据进行编码并通过数据包进行发送。这为每次更新增加了1比特开销，以及需要增加16位序列号的开销来标记每个物体更新的基准帧。我们可以做得更好吗? 是的。确实可以做的更好。你要在发送和接收端进行缓冲来实现这个相对编码机制，但是你不可能永远缓冲。事实上，如果你仔细想想，你只能缓冲几秒钟然后整个缓冲就变得不切实际，对于物体在移动这个常见的情况，你会经常发送相同对象的更新信息（比如说katamari球），所以实际上基准帧只能是很短时间之前的一帧状态。 所以对每个物体发送16位的序列号来表明基准帧，在数据包的包头里面发送最近确认的数据包的序列号（这个数据是从可靠的确认系统里面得到的）然后对每个物体编码相对这个基准帧的偏移量，这个偏移量使用5位信息。通过这种方式在每秒60个数据包的情况下，你可以识别相对于基准帧半秒前的状态更新。任何比这个值更老的基准帧不太可能提供一个良好的增量编码的基准，主要是因为它们太老了，所以在这种情况下就要切回到绝对编码进行状态更新。 现在让我们看看会使用这些绝对编码而不是相对编码的对象的类型。他们是静止的对象。我们能做什么来让他们的更新尽可能的高效?在这种立方体模拟的情况，一个可能发生的很糟糕的结果是一个立方体进行停止状态（变成灰色）然后它的优先级显著降低。如果由于数据包的丢失，导致最后对象的位置更新信息被错过的话，可能需要很长时间才会轮到这个物体来更新它的停止位置信息。 我们可以通过跟踪哪些最近变成停止状态的对象来解决这个问题，并且会提高这些对象的优先级直到一个确认包返回来标记这些对象的位置更新信息已经被成功的发送了。因此他们的发送优先级会相对于正常的灰色立方体（那些处于静止状态没有移动的立方体）的发送优先级有一定的提高，并且会在这个提高后的优先级上一直发送，直到我们知道对这些立方体的更新信息已经收到，也就是网络的另外一侧会“承诺”把这些灰色的立方体放在正确的位置上停止。 最后这就是有关于这种技术的全部内容。它非常的有趣，不需要任何花哨的内容就已经足够好了，然后在此基础上可以做一个数量级的带宽节省（通过增量编码），但是这个方案的复杂性非常的高。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟五之快照压缩]]></title>
    <url>%2F2019%2F05%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%BA%94%E4%B9%8B%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[自我总结快照压缩技术的要点为 : 压缩Orientation数据 : 利用四元数的”最小的三个分量”性质:x^2+y^2+z^2+w^2 = 1 来在传输的时候丢弃一个分量并在网络的另外一端对整个四元数进行重建 压缩线性速度和Postion数据 : 把他们限制在某个范围内, 就可以用这个范围的最大值所占用的比特位数来保存这两种数据了, 而不用一个超大的数来保证可以保存他们的最大值了(占用超多bit位) 增量压缩 . . . 原文原文出处 原文标题 : Snapshot Compression (Advanced techniques for optimizing bandwidth) IntroductionHi, I’m Glenn Fiedler and welcome to Networked Physics. In the previous article we sent snapshots of the entire simulation 10 times per-second over the network and interpolated between them to reconstruct a view of the simulation on the other side. The problem with a low snapshot rate like 10HZ is that interpolation between snapshots adds interpolation delay on top of network latency. At 10 snapshots per-second, the minimum interpolation delay is 100ms, and a more practical minimum considering network jitter is 150ms. If protection against one or two lost packets in a row is desired, this blows out to 250ms or 350ms delay. This is not an acceptable amount of delay for most games, but when the physics simulation is as unpredictable as ours, the only way to reduce it is to increase the packet send rate. Unfortunately, increasing the send rate also increases bandwidth. So what we’re going to do in this article is work through every possible bandwidth optimization (that I can think of at least) until we get bandwidth under control. Our target bandwidth is 256 kilobits per-second. Starting Point @ 60HZLife is rarely easy, and the life of a network programmer, even less so. As network programmers we’re often tasked with the impossible, so in that spirit, let’s increase the snapshot send rate from 10 to 60 snapshots per-second and see exactly how far away we are from our target bandwidth. That’s a LOT of bandwidth: 17.37 megabits per-second! Let’s break it down and see where all the bandwidth is going. Here’s the per-cube state sent in the snapshot: 1234567struct CubeState &#123; bool interacting; vec3f position; vec3f linear_velocity; quat4f orientation; &#125;; And here’s the size of each field: quat orientation: 128 bits vec3 linear_velocity: 96 bits vec3 position: 96 bits bool interacting: 1 bit This gives a total of 321 bits bits per-cube (or 40.125 bytes per-cube). Let’s do a quick calculation to see if the bandwidth checks out. The scene has 901 cubes so 901x40.125 = 36152.625bytes of cube data per-snapshot. 60 snapshots per-second so 36152.625 x 60 = 2169157.5 bytes per-second. Add in packet header estimate: 2169157.5 + 32x60 = 2170957.5. Convert bytes per-second to megabits per-second: 2170957.5 x 8 / ( 1000 x 1000 ) = 17.38mbps. Everything checks out. There’s no easy way around this, we’re sending a hell of a lot of bandwidth, and we have to reduce that to something around 1-2% of it’s current bandwidth to hit our target of 256 kilobits per-second. Is this even possible? Of course it is! Let’s get started :) Optimizing OrientationWe’ll start by optimizing orientation because it’s the largest field. (When optimizing bandwidth it’s good to work in the order of greatest to least potential gain where possible…) Many people when compressing a quaternion think: “I know. I’ll just pack it into 8.8.8.8 with one 8 bit signed integer per-component!”. Sure, that works, but with a bit of math you can get much better accuracy with fewer bits using a trick called the “smallest three”. How does the smallest three work? Since we know the quaternion represents a rotation its length must be 1, so x^2+y^2+z^2+w^2 = 1. We can use this identity to drop one component and reconstruct it on the other side. For example, if you send x,y,z you can reconstruct w = sqrt( 1 - x^2 - y^2 - z^2 ). You might think you need to send a sign bit for w in case it is negative, but you don’t, because you can make w always positive by negating the entire quaternion if w is negative (in quaternion space (x,y,z,w) and (-x,-y,-z,-w) represent the same rotation.) Don’t always drop the same component due to numerical precision issues. Instead, find the component with the largest absolute value and encode its index using two bits [0,3] (0=x, 1=y, 2=z, 3=w), then send the index of the largest component and the smallest three components over the network (hence the name). On the other side use the index of the largest bit to know which component you have to reconstruct from the other three. One final improvement. If v is the absolute value of the largest quaternion component, the next largest possible component value occurs when two components have the same absolute value and the other two components are zero. The length of that quaternion (v,v,0,0) is 1, therefore v^2 + v^2 = 1, 2v^2 = 1, v = 1/sqrt(2). This means you can encode the smallest three components in [-0.707107,+0.707107] instead of [-1,+1] giving you more precision with the same number of bits. With this technique I’ve found that minimum sufficient precision for my simulation is 9 bits per-smallest component. This gives a result of 2 + 9 + 9 + 9 = 29 bits per-orientation (down from 128 bits). This optimization reduces bandwidth by over 5 megabits per-second, and I think if you look at the right side, you’d be hard pressed to spot any artifacts from the compression. Optimizing Linear VelocityWhat should we optimize next? It’s a tie between linear velocity and position. Both are 96 bits. In my experience position is the harder quantity to compress so let’s start here. To compress linear velocity we need to bound its x,y,z components in some range so we don’t need to send full float values. I found that a maximum speed of 32 meters per-second is a nice power of two and doesn’t negatively affect the player experience in the cube simulation. Since we’re really only using the linear velocity as a hint to improve interpolation between position sample points we can be pretty rough with compression. 32 distinct values per-meter per-second provides acceptable precision. Linear velocity has been bounded and quantized and is now three integers in the range [-1024,1023]. That breaks down as follows: [-32,+31] (6 bits) for integer component and multiply 5 bits fraction precision. I hate messing around with sign bits so I just add 1024 to get the value in range [0,2047] and send that instead. To decode on receive just subtract 1024 to get back to signed integer range before converting to float. 11 bits per-component gives 33 bits total per-linear velocity. Just over 1⁄3 the original uncompressed size! We can do even better than this because most cubes are stationary. To take advantage of this we just write a single bit “at rest”. If this bit is 1, then velocity is implicitly zero and is not sent. Otherwise, the compressed velocity follows after the bit (33 bits). Cubes at rest now cost just 127 bits, while cubes that are moving cost one bit more than they previously did: 159 + 1 = 160 bits. But why are we sending linear velocity at all? In the previous article we decided to send it because it improved the quality of interpolation at 10 snapshots per-second, but now that we’re sending 60 snapshots per-second is this still necessary? As you can see below the answer is no. Linear interpolation is good enough at 60HZ. This means we can avoid sending linear velocity entirely. Sometimes the best bandwidth optimizations aren’t about optimizing what you send, they’re about what you don’t send. Optimizing PositionNow we have only position to compress. We’ll use the same trick we used for linear velocity: bound and quantize. I chose a position bound of [-256,255] meters in the horizontal plane (xy) and since in the cube simulation the floor is at z=0, I chose a range of [0,32] meters for z. Now we need to work out how much precision is required. With experimentation I found that 512 values per-meter (roughly 2mm precision) provides enough precision. This gives position x and y components in [-131072,+131071] and z components in range [0,16383]. That’s 18 bits for x, 18 bits for y and 14 bits for z giving a total of 50 bits per-position (originally 96). This reduces our cube state to 80 bits, or just 10 bytes per-cube. This is approximately 1⁄4 of the original cost. Definite progress! Now that we’ve compressed position and orientation we’ve run out of simple optimizations. Any further reduction in precision results in unacceptable artifacts. Delta CompressionCan we optimize further? The answer is yes, but only if we embrace a completely new technique: delta compression. Delta compression sounds mysterious. Magical. Hard. Actually, it’s not hard at all. Here’s how it works: the left side sends packets to the right like this: “This is snapshot 110 encoded relative to snapshot 100”. The snapshot being encoded relative to is called the baseline. How you do this encoding is up to you, there are many fancy tricks, but the basic, big order of magnitude win comes when you say: “Cube n in snapshot 110 is the same as the baseline. One bit: Not changed!” To implement delta encoding it is of course essential that the sender only encodes snapshots relative to baselines that the other side has received, otherwise they cannot decode the snapshot. Therefore, to handle packet loss the receiver has to continually send “ack” packets back to the sender saying: “the most recent snapshot I have received is snapshot n”. The sender takes this most recent ack and if it is more recent than the previous ack updates the baseline snapshot to this value. The next time a packet is sent out the snapshot is encoded relative to this more recent baseline. This process happens continuously such that the steady state becomes the sender encoding snapshots relative to a baseline that is roughly RTT (round trip time) in the past. There is one slight wrinkle: for one round trip time past initial connection the sender doesn’t have any baseline to encode against because it hasn’t received an ack from the receiver yet. I handle this by adding a single flag to the packet that says: “this snapshot is encoded relative to the initial state of the simulation” which is known on both sides. Another option if the receiver doesn’t know the initial state is to send down the initial state using a non-delta encoded path, eg. as one large data block, and once that data block has been received delta encoded snapshots are sent first relative to the initial baseline in the data block, then eventually converge to the steady state of baselines at RTT. As you can see above this is a big win. We can refine this approach and lock in more gains but we’re not going to get another order of magnitude improvement past this point. From now on we’re going to have to work pretty hard to get a number of small, cumulative gains to reach our goal of 256 kilobits per-second. Incremental ImprovementsFirst small improvement. Each cube that isn’t sent costs 1 bit (not changed). There are 901 cubes so we send 901 bits in each packet even if no cubes have changed. At 60 packets per-second this adds up to 54kbps of bandwidth. Seeing as there are usually significantly less than 901 changed cubes per-snapshot in the common case, we can reduce bandwidth by sending only changed cubes with a cube index [0,900] identifying which cube it is. To do this we need to add a 10 bit index per-cube to identify it. There is a cross-over point where it is actually more expensive to send indices than not-changed bits. With 10 bit indices, the cost of indexing is 10xn bits. Therefore it’s more efficient to use indices if we are sending 90 cubes or less (900 bits). We can evaluate this per-snapshot and send a single bit in the header indicating which encoding we are using: 0 = indexing, 1 = changed bits. This way we can use the most efficient encoding for the number of changed cubes in the snapshot. This reduces the steady state bandwidth when all objects are stationary to around 15 kilobits per-second. This bandwidth is composed entirely of our own packet header (uint16 sequence, uint16 base, bool initial) plus IP and UDP headers (28 bytes). Next small gain. What if we encoded the cube index relative to the previous cube index? Since we are iterating across and sending changed cube indices in-order: cube 0, cube 10, cube 11, 50, 52, 55 and so on we could easily encode the 2nd and remaining cube indices relative to the previous changed index, e.g.: +10, +1, +39, +2, +3. If we are smart about how we encode this index offset we should be able to, on average, represent a cube index with less than 10 bits. The best encoding depends on the set of objects you interact with. If you spend a lot of time moving horizontally while blowing cubes from the initial cube grid then you hit lots of +1s. If you move vertically from initial state you hit lots of +30s (sqrt(900)). What we need then is a general purpose encoding capable of representing statistically common index offsets with less bits. After a small amount of experimentation I came up with this simple encoding: [1,8] =&gt; 1 + 3 (4 bits) [9,40] =&gt; 1 + 1 + 5 (7 bits) [41,900] =&gt; 1 + 1 + 10 (12 bits) Notice how large relative offsets are actually more expensive than 10 bits. It’s a statistical game. The bet is that we’re going to get a much larger number of small offsets so that the win there cancels out the increased cost of large offsets. It works. With this encoding I was able to get an average of 5.5 bits per-relative index. Now we have a slight problem. We can no longer easily determine whether changed bits or relative indices are the best encoding. The solution I used is to run through a mock encoding of all changed cubes on packet write and count the number of bits required to encode relative indices. If the number of bits required is larger than 901, fallback to changed bits. Here is where we are so far, which is a significant improvement: Next small improvement. Encoding position relative to (offset from) the baseline position. Here there are a lot of different options. You can just do the obvious thing, eg. 1 bit relative position, and then say 8-10 bits per-component if all components have deltas within the range provided by those bits, otherwise send the absolute position (50 bits). This gives a decent encoding but we can do better. If you think about it then there will be situations where one position component is large but the others are small. It would be nice if we could take advantage of this and send these small components using less bits. It’s a statistical game and the best selection of small and large ranges per-component depend on the data set. I couldn’t really tell looking at a noisy bandwidth meter if I was making any gains so I captured the position vs. position base data set and wrote it to a text file for analysis. I wrote a short ruby script to find the best encoding with a greedy search. The best bit-packed encoding I found for the data set works like this: 1 bit small per delta component followed by 5 bits if small [-16,+15] range, otherwise the delta component is in [-256,+255] range and is sent with 9 bits. If any component delta values are outside the large range, fallback to absolute position. Using this encoding I was able to obtain on average 26.1 bits for changed positions values. Delta Encoding Smallest ThreeNext I figured that relative orientation would be a similar easy big win. Problem is that unlike position where the range of the position offset is quite small relative to the total position space, the change in orientation in 100ms is a much larger percentage of total quaternion space. I tried a bunch of stuff without good results. I tried encoding the 4D vector of the delta orientation directly and recomposing the largest component post delta using the same trick as smallest 3. I tried calculating the relative quaternion between orientation and base orientation, and since I knew that w would be large for this (rotation relative to identity) I could avoid sending 2 bits to identify the largest component, but in turn would need to send one bit for the sign of w because I don’t want to negate the quaternion. The best compression I could find using this scheme was only 90% of the smallest three. Not very good. I was about to give up but I run some analysis over the smallest three representation. I found that 90% of orientations in the smallest three format had the same largest component index as their base orientation 100ms ago. This meant that it could be profitable to delta encode the smallest three format directly. What’s more I found that there would be no additional precision loss with this method when reconstructing the orientation from its base. I exported the quaternion values from a typical run as a data set in smallest three format and got to work trying the same multi-level small/large range per-component greedy search that I used for position. The best encoding found was: 5-8, meaning [-16,+15] small and [-128,+127] large. One final thing: as with position the large range can be extended a bit further by knowing that if the component value is not small the value cannot be in the [-16,+15] range. I leave the calculation of how to do this as an exercise for the reader. Be careful not to collapse two values onto zero. The end result is an average of 23.3 bits per-relative quaternion. That’s 80.3% of the absolute smallest three. That’s just about it but there is one small win left. Doing one final analysis pass over the position and orientation data sets I noticed that 5% of positions are unchanged from the base position after being quantized to 0.5mm resolution, and 5% of orientations in smallest three format are also unchanged from base. These two probabilities are mutually exclusive, because if both are the same then the cube would be unchanged and therefore not sent, meaning a small statistical win exists for 10% of cube state if we send one bit for position changing, and one bit for orientation changing. Yes, 90% of cubes have 2 bits overhead added, but the 10% of cubes that save 20+ bits by sending 2 bits instead of 23.3 bit orientation or 26.1 bits position make up for that providing a small overall win of roughly 2 bits per-cube. As you can see the end result is pretty good. ConclusionAnd that’s about as far as I can take it using traditional hand-rolled bit-packing techniques. You can find source code for my implementation of all compression techniques mentioned in this article here. It’s possible to get even better compression using a different approach. Bit-packing is inefficient because not all bit values have equal probability of 0 vs 1. No matter how hard you tune your bit-packer a context aware arithmetic encoding can beat your result by more accurately modeling the probability of values that occur in your data set. This implementation by Fabian Giesen beat my best bit-packed result by 25%. It’s also possible to get a much better result for delta encoded orientations using the previous baseline orientation values to estimate angular velocity and predict future orientations rather than delta encoding the smallest three representation directly. Chris Doran from Geomerics wrote also wrote an excellent article exploring the mathematics of relative quaternion compression that is worth reading. 译文译文出处 译者：张大伟（卡卡是我）/ 许春(conan) 审校：崔国军（飞扬971） 介绍大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。 在前面的文章中，我们会通过网络以每秒10次的速度发送整个模拟状态的快照，并在网络通信的另外一侧对这些快照进行插值来重建整个模拟的世界状态。 因为我们发送的快照的频率比较低，这样会带来的一个问题就是对快照进行插值的话会在网络延迟的基础上还要增加插值带来的延迟。如果是每秒10次快照这个情况，最小的插值延迟是100毫秒，考虑到网络抖动的话一个比较实际的最小延迟是150毫秒。如果需要在连续丢失一个到两个包的情况进行保护处理的话，可能延迟就会高达250毫秒甚至350毫秒。 这种程度的延迟对于大多数游戏来说都是不能接受的量。减少这种延迟的唯一方法是增加快照发送的频率。由于许多游戏是以60fps的频率进行更新，可以尝试以每秒60次的频率发送快照而不是我们正在使用的每秒10次。但是很不幸的是，这样做的话会带来网络带宽的损耗，不仅是因为我们更加频繁的发送相同大小的数据包，而且还因为发送每个数据包都会有包头数据的负担。 这个的原因听起来很明显，如果以每秒60次的频率来发送数据包，那么相比以每秒10次的频率来发送数据包，我们发送的UDP/IP数据包头的数据量很明显将是6倍。在计算数据包头的带宽消耗的时候我使用了一个经验法则，大概每个数据包头带来的带宽损耗大概是32字节。这个估计并不十分准确但是能让我们对一个典型的数据包头到底是多大有个粗略的概念。把这个大小乘以60就是每秒的带宽损耗，你会发现这样损耗的带宽其实不是一个小数目。而且这样带来带宽损耗是一个基础大小，你根本就没有办法来减少。如果是采用IPv6的话，数据包头的大小可能会更大，每秒的带宽损耗也会跟着变大。 对于数据包的包头带来开销，我们基本是没有办法进行优化的，但是数据包的其他所有部分我们都可以进行优化。所以我们将在本文中要做的事情就是遍历一切可能的带宽优化方法（至少是我能想到的一切带宽优化方法）直到我们把带宽的消耗控制在我们能接受的范围内。对于这个应用程序，我们把带宽控制的目标设置为每秒256kb。 从60HZ为起点开始优化吧这看起来似乎是比较小的一个流量，可能你的网络连接能够支持更大的流量，但是我们要明白的是当你对视频游戏或者物理模拟进行网络通信的时候，你的目标是减少延迟和确保对玩家来说最好的网络连接情况。为了实现这一目标，最好不要让连接一直饱和工作，做到这一点的办法是使用一个比较保守的带宽量，这样就比较不容易给你的玩家造成困扰和麻烦。 让我们看下当我们以每秒60次的频率发送未压缩的快照的时候，我们到底使用了多大的带宽。 这一切的带宽都是从哪里来的呢？这个数据包包含了一个有901个立方体的数组，其它的东西就没有什么了。很显然，立方体的数据是造成高带宽的原因，但是为什么发送每个立方体的数据会这么昂贵呢？ 每个立方体具有以下这些属性： 用quat表示的立方体朝向：128比特。 用vec3表示的立方体速度：96比特。 用vec3表示的立方体位置：96比特。 用bool表示的是否相互作用：1比特。 所以一个立方体一共要占据321比特的大小（40.125字节） 让我们做下数学计算并确保一切的东西都包括在内了。这个场景有901个立方体所以每次快照的立方体数据大概有901x40.125 = 36152.625字节。每秒60张快照就一共是，每秒6152.625 x 60 = 2169157.5字节。再加入报文头部大小的估计：2169157.5 + 32x60 = 2170957.5字节。将单位从每秒比特转换成每秒Mb：2170957.5 x 8 / ( 1000 x 1000 ) =17.38mbps。这与刚才的得到的结果就足够接近了！ 优化Orientation数据正如你看到的那样，所有的东西我们都考虑进来了。让我们先开始从立方体的方向数据进行优化，因为它是最大的一块数据。（当对带宽进行优化的时候，最有效的办法是从大数据到小数进行优化，这样才能收益最大）。 在压缩四元数的时候很多人会这么想：“我知道了，我可以把四元数的每一位用8个比特的有符号整数来表示，这样大小就用一个32位的整数来表示四元数了“。当然，这是一个可行的方法，但是如果使用一些数学方面的技巧，你可以用更少的位数得到一个更加准确的表示方法，这个数学技巧被称为”最小的三个分量“。 最小的三个分量这种方法是如何起作用的？因为我们知道代表旋转的四元数的长度一定是1，因此代表旋转的四元数会有这么一个性质：x^2+y^2+z^2+w^2 = 1。我们可以利用四元数的这个性质来在传输的时候丢弃一个分量并在网络的另外一端对整个四元数进行重建。举个简单的例子来说，如果你通过网络发送的是x、y、z，你就可以利用这样一个公式来重建w分量：w = sqrt( 1 – x^2 – y^2 – z^2 )。你可能觉得需要发送一个符号位来标识w的正负，以防止w是负的情况，但是事实上，你根本不需要这么做，因为总是可以保证w是正的，如果w是负的话，可以把整个四元数的四个分量取反就好了（在四元数空间中，(x,y,z,w)和(-x,-y,-z,-w)代表的是相同的旋转）。 不要总是丢弃同一个分量，因为不总是丢弃同一个分量会得到更高的精度。，相反，要找到四个分量中最大的那一个（以绝对值的大小来衡量）并把这个分量的序号编码进一个2比特[0,3]的信息中（0=x,1=y, 2=z, 3=w)），然后通过网络发送最小的三个分量以及最大分量的序号（这样的话，通过最大的分量的序号，我们就知道了发送过来的三个分量的名字）。在网络的另外一端，我们会从2比特的最大分量的序号信息中解码出来我们需要重建的分量是哪一个，然后就可以利用传过来的三个分量对其进行重建。 最后一个改进。如果v是四元数四个分量中最大的那个分量，可能会出现两个分量是0而另外两个分量的绝对值一样大的情况，这个四元数（v，v，0，0）的长度是1，因此v^2 + v^2= 1，2v^2 = 1，v = 1/sqrt(2)。这意味着你将会在[-0.707107，+0.707107]的大小区间里面对最小的三个分量进行编码而不是在[-1，+1]这个完整的可用空间，这将使你得到更高的精度。 通过这种方法，我发现对于我的模拟情况来说保证最小的精度要求也需要用9个比特来表示一个分量。这样的话，结果就是要对每个方向需要用2 + 9 + 9 + 9= 29比特。（而原来是需要128个比特位！）。 优化线性速度数据接下来我们应该优化什么？线性速度和位置数据均可（需要96个比特位）。 根据我的经验来看，位置信息是非常难以压缩的，所以让我们从线性速度开始。 要压缩线性速度的话，我们首先需要把线性速度的分量限制在某个范围内，这样的话我们就不需要发送完整的浮点值。我发现最大速度定为32米每秒的话就会非常好，正好是2的平方数，并且在立方体模拟的情况下不会影响玩家的体验。由于我们实际上只是使用线性速度作为一个辅助信息来提高位置采样点之间的插值，所以我们可以极大地压缩线性速度值。我发现，其实是只使用32个离散的值（0到31）也是一个可以接受的精度。 线性速度已经被限定在某个范围内并进行了量化，现在三个整数会分布在【-1024，1023】内。这种分解具体如下：【-32，+ 31】（6位）用来表示分量的整数部分，然后会乘以5位的小数部分。我不喜欢用符号位乱搞，所以我只是整体都加了1024来让值的范围在【0，2047】，并把新得到的值发送出去。在接收的时候如果想要解码的话，先要减去1024，这样才会得到原始的有符号数，然后才是转换成浮点数。 每个分量占据11比特，加起来就是每个线性速度要一共占据33比特的大小，只比未压缩前的数据量的1/3稍微多一点点！ 因为大多数立方体是固定的，我们可以处理的更好。为了利用这一点，我们可以在立方体“处于休息状态”的时候只写一个单独的比特位。如果这个比特的值是1，那么我们就知道了速度其实是0并且并不发送。否则，压缩好的速度会跟着这个比特值（压缩好的速度值一共有33比特）。“处于休息状态”的立方体现在一共占据了127比特，而正处于移动状态的立方体消耗的带宽大小比之前的方法要多一个比特：159 +1 = 160 比特。 但是我们为什么要发送线性速度呢？在前一篇文章中，我们决定发送线性速度是因为它对于每秒10次快照的情况能显著的提高插值的质量。但是，现在我们每秒发送60次快照，那么是否还需要发送线性速度呢？你可以在下面看到，答案是不需要。在高发送频率的时候线性插值的效果是足够好的。 优化Position数据那么现在我们只有一个位置信息需要压缩了。我们将使用用于线性速度一样的小技巧：限定在某个范围内并进行量化。大多数的游戏世界其实是相当大的所以我选择的位置限制是在水平平面上的【-256,256】米之内，因为在我的立方体模拟情形中，地板的高度是z=0，所以我选择的z的范围是【0,32】米。 现在我们需要确定我们对精度的要求到底是多少。通过一些实验，我发现每米有512个值（大概精度是2mm）的情况就能够提供足够的精度。这样做的话会让x和y分量的值可以分布在区间【131072,+131071】，让z分量的值分布在区间【0, 16383】。所以这么处理的话，x分量占据18比特的大小，y分量占据18比特的大小，而z分量占据14比特的大小，加起来每个位置一共占据50比特的大小（原先是96比特的大小）。 这可以把我们的每个立方体的信息减至80比特，也就是10字节。（4倍的提升，原先每个立方体的状态大概需要40字节的大小）。 现在我们对位置信息和方向信息进行了压缩，我们已经通过减少我们发送的数据的精度来完成了简单的压缩。并且压缩率已经到了如果任何方向上进一步压缩都会导致精度进一步受损到不可接受的程度。 我们还可以进行进一步的优化么？ 增量压缩答案是肯定的，但是我们需要使用一种全新的技术：增量压缩。 增量压缩听起来让人感觉神秘、神奇、很艰深。实际上，这个技术根本就不难。下面是它具体如何工作的：网络连接的一侧给另外一侧发送数据包，像这样：“这是快照110相对于快照100的编码”。快照是基于某个被称为基线的东西进行编码的。具体你如何实现这种编码方式完全取决于你，这里面有很多花哨的技巧，但是基础是一样的，当你说出“在快照110里面的第n个立方体相对于基线是没有任何改变，所以它只用1个比特位表示：没有变化！”的时候，大量的数据传输就被节省下来了。 为了实现增量压缩的编码，当然有一点是非常关键的就是发送方必须只编码那些相对基线发生了变化的东西，这样就要求它要知道网络连接的另外一侧已经接收了什么，否则发送方根本就没法对快照进行编码。因此，为了处理数据包丢失的情况，接收方必须持续发送“ack”（接收确认）的数据包给发送方，这个数据包是说“我已经收到的最新的快照是快照n”。发送方解析最近收到最近的接收确认包，如果这个接收确认包比之前的接收确认包记录的快照更新的话，就会把基线的值调整成最近的接收确认包里面记载的快照。下一次发送数据包的时候，快照就会根据最新更新的基线进行编码。这个过程会持续的进行，这样的话稳定状态下发送者编码快照时候与基线的差距基本就是由过去这段时间的往返时间决定的。 这里面会有一个小问题：在刚开始连接的时候，因为发送方没有一次通信所需要的往返时间以及并没有从接收方收到任何的确认包，所以发送方根本就没有任何基线进行相对编码。我是通过在数据包里面添加了一个单独的标志来解决这个问题的，这个标记的意思是“这个快照是相对于模拟的初始状态进行编码的”，而这个标志位是双方都明白意思的。另外一个解决方案是如果发送方不知道发送的初始状态的话，就使用一个非增量的路径进行发送初始状态。所以有可能最初发送的是一个非常大的数据块，一旦这个数据块被接收确认的话，后续发送就会以这个大的数据块作为机箱来发送增量编码的快照，然后最终收敛到以往返时延作为基准的稳定状态。 正如你可以在上面看到的那样，这是一个巨大的胜利。我们可以完善这一做法，并来获得更多的收益，但是这个收益是有限的，不会是像刚才这个做法这样带来这么大幅度的提升。从现在开始，我们将需要努力工作来获得一些比较小幅度能累积的收益来达到我们设定的256kb每秒的目标。 增量的一些优化第一个小的提升。每个未发送的立方体需要花费1比特的带宽（如果立方体没有变化的话）。因为场景中一共有901个立方体，所以即使所有的数据包都没有变化的话，我们还是要在每个数据包要发送901个比特。如果是每秒发送60个数据包的频率的话，这就将增加54kb的带宽。可以看到在通常情况下会在每次快照的时候发生变化的立方体数目明显小于901个，所以我们可以通过只发送变化过的立方体来减小消耗的带宽。我们创建了一个立方体索引【0，900】来标记哪一个立方体是什么。为了做到这一点，我们需要给每个立方体增加10位的索引来标识它。 这里面其实是有一个权衡点的，就是发送索引比发送一个位来表示立方体未发生变化要浪费更多的带宽。因为每个立方体的索引是10位，所以索引的消耗是10xn位。因此如果我们发送的立方体数目小于90个的话（也就是小于900位的话），使用索引是更加有效率的。我们可以依照这个数值对每个快照进行评估，并在数据包的头部发送一个单独的位来进行标示我们该使用哪种方法，我们使用如下的定义：0=索引，1=用单独的1位进行标示是否发生变化。通过这种方法我们根据快照中要发送的发生改变的立方体数目来进行最有效的编码。 这种方法会在所有物体都是固定不发生变化的情况下可以减少稳定状态下的带宽到大约15kb每秒。这种情况下带宽是完全由我们自己的数据包包头（16位无符号的顺序号、16位无符号的基准线编号、用了标记是否是初始状态的布尔值）外加IP 和UDP的包头（28位）来占据的。 下一个小的提升。如果我们相对于之前的立方体索引进行当前立方体索引编码怎么样？因为我们在遍历所有的立方体的时候是按照顺序进行遍历的并会按照顺序发送发生改变的立方体：比如说像立方体0、立方体10,、立方体11、立方体50、立方体52、立方体55这样，所以我们可以很容易根据前一个立方体的索引对当前立方体的编号进行相对索引，这样的话，前面的例子就会变成：+10、+1、 +39、 +2、+3。如果我们可以很聪明的利用相对index编码的话，从平均情况来说，我们可以用少于10位的数据来表示一个立方体的索引。 最好的编码方法取决于和你进行交互的物体集合。如果你花了很多时间进行水平移动的同时还将很多立方体从最初的立方体位置上推开，那么就会在相对index编码的方法得到很多的+1。如果你从最初的状态开始垂直移动，那么就会在相对index编码的方法得到很多的+30(sqrt(900))。我们需要的是一种通用的编码方法能够用很少的位来表示统计学下通用的index偏移。 在进行少量的实验之后，我想出了这个简单的编码方式： [1,8] =&gt; 1 + 3 (4位) [9,40] =&gt; 1 + 1 + 5 (7位) [41,900] =&gt; 1 + 1 + 10 (12位) 需要注意下相对偏移具体是有多大，这个大小可能超过10位的大小。这是一个统计意义的游戏。赌注是我们可能得到一个大得多的偏差，这样如果赢的话会消除大偏移带来的增加的消耗。这确实起作用了。有了这个编码方法，我的每个相对序号的大小平均下来是5.5位这么大。 现在我们会有一个小问题。我们再也不能很容易地确定到底是用一位来表示是否发生了更改还是使用相对序号才是最好的编码方法。我使用的解决方案就是通过将所有发生改变的立方体通过一个模拟编码的方式写入一个数据包里面，然后计算相对序号这种编码方式所需要的位数。如果所需的位数比901大，那么我们就切换回用一位来表示是否发生了更改的方法。 接下来我们要做这样的一个小的提升。利用基线时候立方体所在的位置对位置信息进行编码。这里有很多不同的选择。你可以做那些有明显效果的事情。举个简单的例子来说，用1位来表示这是相对位置的信息，然后用8-10位来表示每个分量的相对位置信息，如果所有分量的位置正好在这些位数提供的数据范围之内，否则的话就该发送绝对位置（需要使用50位）。 这给出了一个还不错的编码方法，但是我们可以做的更好。如果你仔细想想的话，就会发现有如下一个情况，一个位置分量很大但是其他位置分量很小。如果我们可以利用这一点的话，就能得到更好的效果，并且用更少的位数来发送这些大小比较小的分量信息。 这是一个统计游戏，到底是给每个分量选择一个比较小的数据范围还是选择一个比较大的数据范围依赖于数据集本身。我没有办法只是看带宽的大小就能告诉一些真正有用的信息，所以我捕获了位置以及基准位置数据，并把它们写入一个文件进行分析。这个格式是每一行表示一个立方体的数据信息，依次是x、y、z、base_x、base_y、base_z。目标是在每一行用基准线状态下位置的x、y、z分量来对当前状态下的x、y、z分量进行编码。如果你有兴趣的话,你可以在这里（地址是http://gafferongames.com/wp-content/uploads/2015/02/position_values.txt）下载这个数据集。 我写了一个简短的ruby脚本来使用暴力搜索找到最优的编码方案。我发现最好的位打包编码的数据集是这样运作的：先对每个分量使用1位数据标识，然后如果确实是小数据的话(也就是区间在【-16.15】之内的话)，就使用5位的数据，否则的话，分量的增量的取值范围在【-256，256】之内，并用9位数据进行发送。如果任意分量的增量超出了这个范围的话就会换回使用绝对位置。通过使用这种编码方法，我可以对每个变化的位置只用平均下来26.1位来进行表示。 增量编码最小的三个分量接下来，我将指出如同相对方向变化的话能几乎取得相对位置变化相同的效果。这里的问题可能会有些不一样，就是位置变化的取值范围相对于整个位置空间而言是非常非常小的，但是100毫秒内方向上的变化可能占整个四元数空间的话，是一个非常大的比例。 我尝试了很多方法但并没有得到好的结果。我尝试直接对方向的增量这个四维向量进行编码并使用那个“最小的三个分量”这个技巧来隐含的表示最大的分量。我还尝试计算基线状态下的方向和当前方向之间的相对四元数。因为我知道w分量将是最大的分量（因为这个四元数表示的是旋转），我可以不用发送2位数据来确定最大的分量，但反过来将需要发送一位数据来标识w分量的正负，因为我不想对整个四元数取反。通过这种方案我能找到的最好的压缩方法只有“最小的三个分量”方法的数据量的90%。这个结果并不是太令人满意。 我几乎都要放弃这个方向的优化了，但是我对“最小的三个分量”方法跑了一些分析结果。我发现按照“最小的三个分量”的格式90%的情况下方向里面最大的那个分量都和100毫秒之前基线状态下方向里面最大的分量是相同的。这意味着如果直接对最小的三个分量进行增量编码的话有可能是能获得更大收益的。更重要的是，我发现如果使用这种方法的话，在从基线数据重建整个方向信息的话不会有任何额外的精度损失。我从场景中运行一些方向信息并把这些方向信息以最小的三个分量”的格式导出来，并使用我曾经对位置信息使用过的暴力穷举方法来对每个分量的取值范围进行评估。 所找到的最佳编码方法是：5-8，这意味着对于比较小的数值使用的是【-16.15】这个区间，对于比较大的数值，使用的是【-128,+127】这个区间。最后一件要做的事情：就跟位置信息的处理一样，大的取值范围可以通过单独的一位来提前知道分量值不会落在【-16.15】这个区间而进行更一步的拓展。我把如何做到这一点作为一个练习留给读者作为一个练习。要小心，不要让这个区间的首尾值最后都成为0。 最后的结果是每个相对四元数平均下来只需要23.3位的数据来表示。这是“最小的三个分量”方法的数据量的80.3%。 我们所做的优化大概就是这些内容了。但是还有一个小的优化还没有做。通过对传过来的位置和方向数据集进行一个了最终的分析，我注意到如果是在0.5毫米这个精确度的话大概有5%的位置信息是相比于基线状态的位置是没有任何变化的，而且有5%以“最小的三个分量”格式表示的方向信息也是相比于基线状态的方向信息是没有任何变化的。 这两种概率是相互排斥的，因为这两种情况同时满足的话那么对应的立方体根本就不会发生变化进而根本就不会发送这个立方体的信息过来，这意味这在这个小规模的统计里面存在10%的立方体，它们的状态我们可以发送一个单独的位来表明是否有位置变化，再用另外一个单独的位来表明是否有方向变化。是的，如果这么做的话，就会给90%的立方体带来2位的额外负担，但是10%的立方体可以节省20多位的带宽（或者用2位信息换取了23.3位的方向信息，或者用2位信息换取了26.1位的位置信息），这种方法大概能给每个立方体在每次快照时候要发送的数据量减少大概2位。 带宽优化有很多的选择方案，而且可以通过一点点工作一些看上去不可能的事情事实上也会变得可能。通过文中的种种方法，我们大概降低了20M比特下来，最后平均下来只有不到0.25m比特。这相比原来未压缩的带宽，大概只有原来的1.25％！ 总结好了, 这就是我用传统的手动位打包技术能压缩优化到的最大程度了，你可以看到做一些优化之后可以得到多么大的提升。 你可以在这里（地址在这里）找到文中提到的所有压缩技术的代码实现。 可以使用不同的方法得到更好的压缩比例。位打包这种方法并不是非常有效率的，是因为并不是所有的位的值取0或者1的概率都是相同的。无论根据具体的环境来如何努力的调整位打包技术，我们都可以根据数据集里面的取值的概率情况来建立一个更精确的模型来轻松的打败之前的调整结果。Fabian Giesen的实现（地址在这里）可以比我最好的位打包结果还能提升25%。 Geomerics的克里斯·多兰（地址在这里）写了一篇很好的文章来探索数学上如何对四元数进行压缩，非常值得一读。如果不是直接使用”最小的三个分量“这种方法，而是利用之前的基线数据来估计下角速度和预测未来的方向应该能对增量编码的方向信息得到好的多的结果。 下一章要讲的是：《状态同步》。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟四之快照插值]]></title>
    <url>%2F2019%2F05%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E5%9B%9B%E4%B9%8B%E5%BF%AB%E7%85%A7%E6%8F%92%E5%80%BC%2F</url>
    <content type="text"><![CDATA[自我总结快照插值这种游戏同步技术的要点为 : 视觉模拟 : 每帧从网络的发送侧捕获所有相关状态的快照，并将其传输到网络的接收侧，在那里我们将试图重建一个视觉上近似合理的模拟 缓冲区 : 内插值之前会缓冲一段合适的时间来处理网络抖动 内插值Interpolation : 处理快照之间的拉扯 线性插值 Hermite插值 外插值Extrapolation (文中翻译为”预测”或”推测”) : 不可行, 因为外插值无法精准预测刚体运动以及各种物理 降低延迟 : 因为我们发送的快照的频率比较低，这样会带来的一个问题就是对快照进行插值的话会在网络延迟的基础上还要增加插值带来的延迟. 所以我们需要增加发送速率, 为了提高发送速度我们需要压缩快照数据技术的配合, 不然占用太多带宽了 减少宽带占用 : 因为所有需要在快照中包含所有实体信息, 所以数据量相当大, 得用各种方法压缩快照数据(网络物理模拟五之快照压缩) . . . 原文原文出处 原文标题 : Snapshot Interpolation (Interpolating between snapshots of visual state) IntroductionHi, I’m Glenn Fiedler and welcome to Networked Physics. In the previous article we networked a physics simulation using deterministic lockstep. Now, in this article we’re going to network the same simulation with a completely different technique: snapshot interpolation. BackgroundWhile deterministic lockstep is very efficient in terms of bandwidth, it’s not always possible to make your simulation deterministic. Floating point determinism across platforms is hard. Also, as the player counts increase, deterministic lockstep becomes problematic: you can’t simulate frame n until you receive input from all players for that frame, so players end up waiting for the most lagged player. Because of this, I recommend deterministic lockstep for 2-4 players at most. So if your simulation is not deterministic or you want higher player counts then you need a different technique. Snapshot interpolation fits the bill nicely. It is in many ways the polar opposite of deterministic lockstep: instead of running two simulations, one on the left and one on the right, and using perfect determinism and synchronized inputs keep them in sync, snapshot interpolation doesn’t run any simulation on the right side at all! SnapshotsInstead, we capture a snapshot of all relevant state from the simulation on the left and transmit it to the right, then on the right side we use those snapshots to reconstruct a visual approximation of the simulation, all without running the simulation itself. As a first pass, let’s send across the state required to render each cube: 123456struct CubeState &#123; bool interacting; vec3f position; quat4f orientation; &#125;; I’m sure you’ve worked out by now that the cost of this technique is increased bandwidth usage. Greatly increased bandwidth usage. Hold on to your neckbeards, because a snapshot contains the visual state for the entire simulation. With a bit of math we can see that each cube serializes down to 225 bits or 28.1 bytes. Since there are 900 cubes in our simulation that means each snapshot is roughly 25 kilobytes. That’s pretty big! At this point I would like everybody to relax, take a deep breath, and imagine we live in a world where I can actually send a packet this large 60 times per-second over the internet and not have everything explode. Imagine I have FIOS (I do), or I’m sitting over a backbone link to another computer that is also on the backbone. Imagine I live in South Korea. Do whatever you need to do to suspend disbelief, but most of all, don’t worry, because I’m going to spend the entire next article showing you how to optimize snapshot bandwidth. When we send snapshot data in packets, we include at the top a 16 bit sequence number. This sequence number starts at zero and increases with each packet sent. We use this sequence number on receive to determine if the snapshot in a packet is newer or older than the most recent snapshot received. If it’s older then it’s thrown away. Each frame we just render the most recent snapshot received on the right: Look closely though, and even though we’re sending the data as rapidly as possible (one packet per-frame) you can still see hitches on the right side. This is because the internet makes no guarantee that packets sent 60 times per-second arrive nicely spaced 1⁄60 of a second apart. Packets are jittered. Some frames you receive two snapshot packets. Other frames you receive none. Jitter and HitchesThis is actually a really common thing when you first start networking. You start out playing your game over LAN and notice you can just slam out packets really fast (60pps) and most of the time your game looks great because over the LAN those packets actually do tend to arrive at the same rate they were sent… and then you start trying to play your game over wireless or the internet and you start seeing hitches. Don’t worry. There are ways to handle this! First, let’s look at how much bandwidth we’re sending with this naive approach. Each packet is 25312.5 bytes plus 28 bytes for IP + UDP header and 2 bytes for sequence number. That’s 25342.5 bytes per-packet and at 60 packets per-second this gives a total of 1520550 bytes per-second or 11.6 megabit/sec. Now there are certainly internet connections out there that can support that amount of traffic… but since, let’s be honest, we’re not really getting a lot of benefit blasting packets out 60 times per-second with all the jitter, let’s pull it back a bit and send only 10 snapshots per-second: You can see how this looks above. Not so great on the right side but at least we’ve reduced bandwidth by a factor of six to around 2 megabit/sec. We’re definitely headed in the right direction. Linear InterpolationNow for the trick with snapshots. What we do is instead of immediately rendering snapshot data received is that we buffer snapshots for a short amount of time in an interpolation buffer. This interpolation buffer holds on to snapshots for a period of time such that you have not only the snapshot you want to render but also, statistically speaking, you are very likely to have the next snapshot as well. Then as the right side moves forward in time we interpolate between the position and orientation for the two slightly delayed snapshots providing the illusion of smooth movement. In effect, we’ve traded a small amount of added latency for smoothness. You may be surprised at just how good it looks with linear interpolation @ 10pps: Look closely though and you can see some artifacts on the right side. The first is a subtle position jitter when the player cube is hovering in the air. This is your brain detecting 1st order discontinuity at the sample points of position interpolation. The other artifact occurs when a bunch of cubes are in a katamari ball, you can see a sort of “pulsing” as the speed of rotation increases and decreases. This occurs because attached cubes interpolate linearly between two sample points rotating around the player cube, effectively interpolating through the player cube as they take the shortest linear path between two points on a circle. Hermite InterpolationI find these artifacts unacceptable but I don’t want to increase the packet send rate to fix them. Let’s see what we can do to make it look better at the same send rate instead. One thing we can try is upgrading to a more accurate interpolation scheme for position, one that interpolates between position samples while considering the linear velocity at each sample point. This can be done with an hermite spline (pronounced “air-mitt”) Unlike other splines with control points that affect the curve indirectly, the hermite spline is guaranteed to pass through the start and end points while matching the start and end velocities. This means that velocity is smooth across sample points and cubes in the katamari ball tend to rotate around the cube rather than interpolate through it at speed. Above you can see hermite interpolation for position @ 10pps. Bandwidth has increased slightly because we need to include linear velocity with each cube in the snapshot, but we’re able to significantly increase the quality at the same send rate. I can no longer see any artifacts. Go back and compare this with the raw, non-interpolated 10pps version. It really is amazing that we’re able to reconstruct the simulation with this level of quality at such a low send rate. As an aside, I found it was not necessary to perform higher order interpolation for orientation quaternions to get smooth interpolation. This is great because I did a lot of research into exactly interpolating between orientation quaternions with a specified angular velocity at sample points and it seemed difficult. All that was needed to achieve an acceptable result was to switch from linear interpolation + normalize (nlerp) to spherical linear interpolation (slerp) to ensure constant angular speed for orientation interpolation. I believe this is because cubes in the simulation tend to have mostly constant angular velocity while in the air and large angular velocity changes occur only discontinuously when collisions occur. It could also be because orientation tends to change slowly while in the air vs. position which changes rapidly relative to the number of pixels affected on screen. Either way, it seems that slerp is good enough and that’s great because it means we don’t need to send angular velocity in the snapshot. Handling Real World ConditionsNow we have to deal with packet loss. After the discussion of UDP vs. TCP in the previous article I’m sure you can see why we would never consider sending snapshots over TCP. Snapshots are time critical but unlike inputs in deterministic lockstep snapshots don’t need to be reliable. If a snapshot is lost we can just skip past it and interpolate towards a more recent snapshot in the interpolation buffer. We don’t ever want to stop and wait for a lost snapshot packet to be resent. This is why you should always use UDP for sending snapshots. I’ll let you in on a secret. Not only were the linear and hermite interpolation videos above recorded at a send rate of 10 packets per-second, they were also recorded at 5% packet loss with +/- 2 frames of jitter @ 60fps. How I handled packet loss and jitter for those videos is by simply ensuring that snapshots are held in the interpolation buffer for an appropriate amount of time before interpolation. My rule of thumb is that the interpolation buffer should have enough delay so that I can lose two packets in a row and still have something to interpolate towards. Experimentally I’ve found that the amount of delay that works best at 2-5% packet loss is 3X the packet send rate. At 10 packets per-second this is 300ms. I also need some extra delay to handle jitter, which in my experience is typically only one or two frames @ 60fps, so the interpolation videos above were recorded with a delay of 350ms. Adding 350 milliseconds delay seems like a lot. And it is. But, if you try to skimp you end up hitching for 1/10th of a second each time a packet is lost. One technique that people often use to hide the delay added by the interpolation buffer in other areas (such as FPS, flight simulator, racing games and so on) is to use extrapolation. But in my experience, extrapolation doesn’t work very well for rigid bodies because their motion is non-linear and unpredictable. Here you can see an extrapolation of 200ms, reducing overall delay from 350 ms to just 150ms: Problem is it’s just not very good. The reason is that the extrapolation doesn’t know anything about the physics simulation. Extrapolation doesn’t know about collision with the floor so cubes extrapolate down through the floor and then spring back up to correct. Prediction doesn’t know about the spring force holding the player cube up in the air so it the cube moves slower initially upwards than it should and has to snap to catch up. It also doesn’t know anything about collision and how collision response works, so the cube rolling across the floor and other cubes are also mispredicted. Finally, if you watch the katamari ball you’ll see that the extrapolation predicts the attached cubes as continuing to move along their tangent velocity when they should rotate with the player cube. ConclusionYou could conceivably spend a great deal of time to improve the quality of this extrapolation and make it aware of various movement modes for the cubes. You could take each cube and make sure that at minimum the cube doesn’t go through the floor. You could add some approximate collision detection or response using bounding spheres between cubes. You could even take the cubes in the katamari ball and make them predict motion to rotate around with the player cube. But even if you do all this there will still be misprediction because you simply can’t accurately match a physics simulation with an approximation. If your simulation is mostly linear motion, eg. fast moving planes, boats, space ships – you may find that a simple extrapolation works well for short time periods (50-250ms or so), but in my experience as soon as objects start colliding with other non-stationary objects, extrapolation starts to break down. How can we reduce the amount of delay added for interpolation? 350ms still seems unacceptable and we can’t use extrapolation to reduce this delay without adding a lot of inaccuracy. The solution is simple: increase the send rate! If we send 30 snapshots per-second we can get the same amount of packet loss protection with a delay of 150ms. 60 packets per-second needs only 85ms. In order to increase the send rate we’re going to need some pretty good bandwidth optimizations. But don’t worry, there’s a lot we can do to optimize bandwidth. So much so that there was too much stuff to fit in this article and I had to insert an extra unplanned article just to cover all of it! 译文译文出处 翻译：崔国军（飞扬971） 审校：张乾光(星际迷航) 介绍大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。 在之前的文章中，我们通过具有确定性的帧同步将物理模拟通过网络通信进行同步。在这一篇文章中我们将使用一种完全不同的技术来将物理模拟通过网络通信进行同步，这个技术就是：快照信息插值方法。 背景为什么需要一种不同的技术？这是因为虽然具有确定性的帧同步这种同步策略在节省带宽方面非常有效，但是要保证你的仿真具有完美的确定性这个事情并不总是可行的，有些时候是不实际的。此外，具有确定性的帧同步这种同步策略在玩家数目增多的情况下会遇到一些问题，因为你要收到所有玩家对应帧的输入才能对这一帧进行模拟。在实践中，这意味着每个人必须等待最滞后的那个玩家。以我的经验来说，我建议在联网环境下只在2到4个玩家的时候使用具有确定性的帧同步这种同步策略。（译者注：其实国内现在已经有20个玩家在互联网环境下使用具有确定性的帧同步这种同步策略的游戏了，就算dota也是支持5v5对战的，原作者太过于谨慎了）。 如果你想要支持更多数目的玩家或者你的模拟并不具有完美的确定性，那么你就需要一种不同的技术了。快照信息插值技术在许多方面都站在具有确定性的帧同步技术的对立面。它不再需要在网络的两侧同时运行仿真，使用程序的确定性以及同步输入信息来保证网络的两侧的仿真始终保持同步。。。快照信息插值技术根本不需要在接收侧运行任何的模拟！ 快照我们所要做的就是每帧从网络的发送侧捕获所有相关状态的快照，并将其传输到网络的接收侧，在那里我们将试图重建一个视觉上近似合理的模拟。 作为第一步，让我们把所需的状态直接发送给网络的接收侧，让它可以渲染每一个立方体： 123456struct CubeState &#123; bool interacting; vec3f position; quat4f orientation; &#125;; 需要注意的是，我们发送了一个布尔值用来标记这个立方体是否与玩家存在交互。为什么需要这个布尔值？这是因为在网络的接收侧并没有运行一个模拟，因此并不会有碰撞检测来告诉我们什么时候一个立方体应该被标红而什么时候不需要对一个立方体进行标红。如果我们想要一个立方体在它与玩家存在互动的情况下变红的话，我们需要在快照中包含此信息。 我敢肯定这时候你已经明白这项技术的消耗在于增大了带宽的使用。其实是大大增大了对带宽的占用。这是因为快照包含了整个仿真的状态。通过一点数学计算我们可以看到每个立方体序列化下来的话大概占据225比特或者28.1字节。因为在我们的仿真中有大概900个立方体，这意味着每个快照大约需要25k的字节。这个数据量相当大了！ 在这一点上，我想每个人都放松、深呼吸，想象我们生活在一个世界里，在这个世界里面我可以在互联网上以60次每秒的速度来实际发送数据包，而不会有什么意外。想象一下我有光纤服务光纤服务或者我坐在骨干网的后面，与另外一台位于骨干网的电脑相连。。想象一下，我使用IPv6，而最大传输单元的大小是100k。可以想象一下，我住在韩国。做你任意想做的，不要有任何的怀疑，而且最重要的是，不用担心网络有任何的问题，这是因为我将在下一篇文章中向你展示如何快速优化快照信息插值这种方法带来的带宽负担。 当我们用数据包的方式发送快照的时候，我们会在数据包的头部包括16位的序列号。这个序列号会从零开始并且随着每个快照的发送而增大。我们在接收的时候使用这个序列号来决定数据包中的快照到底比我们最近收到的快照更新还是更旧。如果比我们最近收到的快照更旧的话，我们就会丢弃这个快照。然后，在网络的接收侧我们只会渲染我们接收到的最新的快照上的信息。 请注意，即使我们尽一切可能的快速发送数据（一帧一个数据包），我们仍然会在网络的接收侧看到物体发生抖动现象。这是因为在互联网上根本就不会保证数据包会按照六十分之一秒的间隔到达网络的另外一侧。数据包的到达时间会发生抖动。在某些帧你会收到两个帧的快照，而在另外一些帧则会根本收不到。 抖动与拉扯这实际上是当你第一次启动网络一个非常常见的事情。你开始通过局域网来玩你的游戏并注意到你可以以一个非常高的速度来发送数据包并且你的游戏会看上表现的非常非常不错，这是因为你的数据包几乎在发送的同时就会到达网络的接收侧。。。然后你开始尝试通过无线网络或者互联网来玩你的游戏，然后你就看到各种各样的抖动。不用担心，有办法来处理这个问题！ 首先，让我们看看使用这种幼稚的方法进行发送数据包，我们会占据多少的带宽。每个数据包是25312.5字节加上IP 和UDP包头所占的28个字节并且还要有2个字节用来表示网络包的序号。这就是一个数据包的大小：25342.5字节，每秒60个数据包的话就一共要1520550字节，或者换算下就是11.6M每秒。现在当然也有互联网连接可以支持这种规模的流量。。。但是既然我们每秒发送60个数据包也没有什么太大的益处，还是充满了这种抖动，让我们稍微稳妥一点，只要每秒发送10次快照就可以了。 你可以通过上面的效果看到修改以后的表现效果如何。这对网络的接收侧并没有什么太大的影响，起码我们将带宽减少到了六分之一，大概是2M每秒。我们正在朝着正确的方向继续前进。 线性插值现在是关于处理快照的一些技巧。我们所做的不是在我们接收到快照数据立刻开始进行渲染，而是利用插值缓冲区缓冲了快照数据一小段时间。这个插值缓冲区会持有快照数据一段时间，这样你不仅会持有你准备渲染的这一帧的快照数据，而且从统计数字上看，你很有可能会持有下一帧你需要的快照数据。然后随着网络的接收侧在时间上向前移动，我们会对这两帧轻微延迟的快照数据进行物体位置和方向的插值，提供平滑运动的错觉。实际上，我们通过增加了一小段延迟来交换物体的平滑运动。 你可以会很吃惊，通过每秒10帧的快照数据以及一个简单的线性插值，就可以得到表现如此只好的一个表现： 但是如果你仔细观察的话，你还是可以看到在网络的接收侧里面存在大量的瑕疵。首先是当玩家立方体在空中盘旋的时候，玩家立方体的位置有一个轻微的抖动。这是因为你的大脑在位置插值的采样点那里检测到了1秒左右的中断。另外一个瑕疵出现在一大推立方体出于katamari球之中的时候，你可以看到某种“脉冲“的存在，立方体旋转的速度会出现升高或者降低的情况。这种情况的出现是是因为附加的这些立方体会在围绕玩家立方体的周围旋转的时候在两个采样点之间进行线性的插值，通过玩家立方体进行插值是有效的，这是因为这样两点之间就是最短线性距离。 Hermite插值我发现这些瑕疵是不能接受的，但是不希望增加数据包的发送速率来解决这些问题。让我们看看我们能做些什么，让数据包的发送速率不变的情况让整个效果能够看起来更棒。我们可以尝试升级的一个事情是对于位置的一种更加精确的插值方案：这种方案在位置采样点之间进行插值的同时还要考虑每个采样点的线性速度。 可以用于执行这种插值的一种曲线是厄米特曲线。这种曲线和其他需要控制点间接的影响曲线不同，厄米特曲线保证一定会通过起点和终点的同时还能匹配起点和终点的速度。这意味着立方体在通过采样点的速度是平滑的，而且katamari球中的立方体会倾向于围绕立方体旋转，而不是通过它的速度进行插值。 上面你所看到的效果是通过每秒10次的快照数据进行的厄米特曲线插值。我们的带宽略有增加，但我们能够在显著提高质量的同时保证数据包的发送速率不变。我再也看不到任何的瑕疵。让我们重新回到厄米特曲线插值之前的效果进行下对比。我们能够在如此低的数据包发送速率下重建出来如此质量水平的模拟，这真的是令人惊讶的改变。 顺便说一句，我发现没有必要对方向四元数进行更高阶的插值来得到平滑的插值。因为我做了很多的研究关于对方向四元数使用一个特定的线性速度进行准确的插值，然后我发现这其实非常的困难。为了达到一个可以接收的效果我们所有要做的事情就是从线性插值加归一化（nlerp函数）到球面线性插值插值（slerp函数）来确保方向四元数的插值得到一个固定的角速度。我相信这是因为在仿真中空中的立方体试图保持大体固定的角速度，而在发生碰撞的时候，比较高的角速度会突然变得不连续。它也可能是因为在空中的时候方向趋于变化缓慢而位置会会相对与屏幕上受影响的像素的数目而快速的变化。无论哪种方式，似乎球面插值都表现的不错，这就是很棒的方法因为这意味着我们并不需要在快照数据中发送角速度。 处理真实世界的情况现在，我们必须处理的数据包丢失的问题了。在上一篇文章讨论完了UDP和TCP的优劣之后，我敢肯定你可以知道了为什么我们从来不考虑通过TCP来发送我们的快照数据。快照数据是时间敏感的，但是和有确定性的帧同步中的输入信息不一样，这个数据没有必要是可靠的。如果一个快照数据丢失了，我们可以跳过这一帧，并使用一个缓冲区中更新的快照数据进行插值。们再也不想停下来，等待丢失的快照数据包的重发。这就是为什么你总是应该使用UDP发送快照数据的原因。 我要告诉你一个秘密。上面这两个视频不仅仅是在每秒10个数据包的发送频率下用线性插值或厄米特插值的效果，它们也记录了在有2帧抖动、5%丢包率下的效果。我该如何处理丢包和抖动？我的做法是确保快照数据会被保存在插值缓冲区，这样在插值之前会缓冲一段合适的时间。 我的经验法则是，插值缓冲区应该有足够的延迟，这样就算我连续丢失了两个数据包，我仍然有快照数据可以进行插值。在实验中我发现在有2-5%的数据包丢包率的情况下，延迟量大概是数据包发送速度的3倍左右比较合适。如果是每秒发送10个数据包的情况，这个延迟量就是大概300毫秒。我还需要一些额外的延迟来处理抖动的情况，以我的经验来说，如果是在60fps的条件下，大概预留一到两帧就可以了，所以上面的插值效果的视频使用了一个大概350毫秒的延迟。 添加350毫秒的延迟，似乎带来了很大的延迟。但是如果因为觉得不舍得而不在这里使用350毫秒的延迟的话，要么就得到一个充满拉扯的效果，要么就会遇到每秒有十分之一的数据包丢失。人们在其他领域（比如说第一人称设计游戏、飞行模拟游戏、赛车游戏以及其他的游戏）经常使用的用于隐藏由于插值缓冲区带来的延迟的方法是使用预测方法。以我的经验来看，预测方法对于缸体来说效果似乎不太好，这是因为它们的运动不是线性的并且无法预测。在这里，你可以看到使用了200毫秒的预测方法理论上可以将延迟从350毫秒减少到150毫秒： 问题是，它工作起来效果似乎不是太好。当然这个原因在于预测方法根本不知道任何有关物理模拟方面的内容。预测方法不知道立方体需要与地面发生碰撞，立方体在向下遇到地板的时候会反弹回来，当然这是正确的表现。预测方法不知道有关施加在空中的玩家立方体上的反弹力，因此立方体起初会比实际的情况下移动的慢一些，然后开始加速追上自己应该在的位置。预测方法也不知道任何有关碰撞方面的内容以及如果发生碰撞了该发生何种反应，所以如果立方体在地板上或者其他立方体上滚过的时候就会发生预测上的错误。最后，如果你仔细观察katamari球的话，你会看到预测方法会让附着于其上的立方体继续沿着它们的切线速度运动，而这个时候它们应该跟玩家立方体一起转动。 结论现在，你可以持续的花费大量的时间来改善推测的质量，并通过认识立方体的各种运动模式来持续提高。你可以对每个立方体进行推断，并确保最低程度是立方体不会穿越地面。你还可以添加碰撞检测的一些推断并且利用立方体的球型包围盒做出响应。你甚至可以选取katamari球体里面的立方体并预测当它们在玩家立方体周围的时候会如何运动。 但是，即使你做了所有的这一切，仍然会有预测失误，因为你根本无法精确匹配物理模拟与近似估计之间的差距。如果你的仿真中大多数是直线运动，比如说，快速移动的飞机、轮船、太空飞船等等-你会发现一个简单的外推法非常适用于短时间内（50-250ms左右）的这种运动。以我的经验来看，只要物体开始与非静止的物体开始发生碰撞，这种预测就将完全的不成立。 我们怎样才能减少由于插值带来的延迟？350毫秒似乎仍然是不能接受的延迟，我们不能使用预测和外推来减少这种延迟，因为这样增加了大量的不确定性。解决的办法其实很简单：增加发送速率。如果我们每秒发送30次快照的话，我们可以在相同的数据包丢失率情况将延迟降低到150ms。如果我们把发送速率增大到每秒60个数据包的程度，延迟将只有85ms。 为了提高发送速度，我们会需要进行一些不错的带宽优化。不过不用担心，我们可以做很多很多的事情来优化带宽。但是我们如果把这些东西都放到这篇文章的话，这篇文章就会有太多太多的东西，我不得不插入一个计划之外的文章来涵盖带宽优化方面的东西！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟三之具有确定性的帧同步]]></title>
    <url>%2F2019%2F05%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%B8%89%E4%B9%8B%E5%85%B7%E6%9C%89%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E5%B8%A7%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[自我总结帧同步要点如下 : 确定性 : 去除随机数 缓冲 : 因为数据包并不是均匀地到达, 所以要做一个缓冲区, 然后再均匀地取出 不用TCP : 因为我们的数据对时间非常敏感, 不接受到第n个输入包就无法继续模拟第n帧, 而TCP的确认机制以及重传机制当我们丢包时, 我们只能暂停等待它重发造成卡顿 用UDP : 发送冗余数据 : 因为帧同步只发送玩家input数据, 而input包是很小的, 所以发冗余也不会很大 增量包 : 加一个bit来标志跟上一个包的比较结果, 如果这个包跟上个包一致则只发送一个1, 如果不一致则发送0和这个包的完整数据 帧同步的缺点 : 等的人太多 : 因为你要收到所有玩家对应帧的输入才能对这一帧进行模拟.在实践中，这意味着每个人必须等待最滞后的那个玩家.人越多等得越久, 所以帧同步不适合mmo. 比较耗性能 : 因为帧同步技术的话, 在客户端中，每个对象都要执行所有的物理之类的运算; 而状态同步可以只同步当前玩家周围对象的状态, 不需要同步所有对象 . . . 原文原文出处 原文标题 : Deterministic Lockstep (Keeping simulations in sync by sending only inputs) IntroductionHi, I’m Glenn Fiedler and welcome to Networked Physics. In the previous article we explored the physics simulation we’re going to network in this article series. In this article specifically, we’re going to network this physics simulation using deterministic lockstep. Deterministic lockstep is a method of networking a system from one computer to another by sending only the _inputs_that control that system, rather than the state of that system. In the context of networking a physics simulation, this means we send across a small amount of input, while avoiding sending state like position, orientation, linear velocity and angular velocity per-object. The benefit is that bandwidth is proportional to the size of the input, not the number of objects in the simulation. Yes, with deterministic lockstep you can network a physics simulation of one million objects with the same bandwidth as just one. While this sounds great in theory, in practice it’s difficult to implement deterministic lockstep because most physics simulations are not deterministic. Differences in floating point behavior between compilers, OS’s and even instruction sets make it almost impossible to guarantee determinism for floating point calculations. DeterminismDeterminism means that given the same initial condition and the same set of inputs your simulation gives exactly the same result. And I do mean exactly the same result. Not close. Not near enough. Exactly the same. Exact down to the bit-level. So exact, you could take a checksum of your entire physics state at the end of each frame and it would be identical. Above you can see a simulation that is almost deterministic. The simulation on the left is controlled by the player. The simulation on the right has exactly the same inputs applied with a two second delay starting from the same initial condition. Both simulations step forward with the same delta time (a necessary precondition to ensure exactly the same result) and both simulations apply the same inputs. Notice how after the smallest divergence the simulation gets further and further out of sync. This simulation is non-deterministic. What’s going on is that the physics engine I’m using (Open Dynamics Engine) uses a random number generator inside its solver to randomize the order of constraint processing to improve stability. It’s open source. Take a look and see! Unfortunately this breaks determinism because the simulation on the left processes constraints in a different order to the simulation on the right, leading to slightly different results. Luckily all that is required to make ODE deterministic on the same machine, with the same complied binary and on the same OS (is that enough qualifications?) is to set its internal random seed to the current frame number before running the simulation via dSetRandomSeed. Once this is done ODE gives exactly the same result and the left and right simulations stay in sync. And now a word of warning. Even though the simulation above is deterministic on the same machine, that does _not_necessarily mean it would also be deterministic across different compilers, a different OS or different machine architectures (eg. PowerPC vs. Intel). In fact, it’s probably not even deterministic between debug and release builds due to floating point optimizations. Floating point determinism is a complicated subject and there’s no silver bullet. For more information please refer to this article. Networking InputsNow let’s get down to implementation. Our example physics simulation is driven by keyboard input: arrow keys apply forces to make the player cube move, holding space lifts the cube up and blows other cubes around, and holding ‘z’ enables katamari mode. How can we network these inputs? Must we send the entire state of the keyboard? No. It’s not necessary to send the entire keyboard state, only the state of the keys that affect the simulation. What about key press and release events then? No. This is also not a good strategy. We need to ensure that exactly the same input is applied on the right side, at exactly the same time, so we can’t just send ‘key pressed’, and ‘key released’ events over TCP. What we do instead is represent the input with a struct and at the beginning of each simulation frame on the left side, sample this struct from the keyboard: 123456789struct Input &#123; bool left; bool right; bool up; bool down; bool space; bool z; &#125;; Next we send that input from the left simulation to the right simulation in a way that the simulation on the right side knows that the input belongs to frame n. And here’s the key part: the simulation on the right can only simulate frame n when it has the input for that frame. If it doesn’t have the input, it has to wait. For example, if you were sending across using TCP you could simply send the inputs and nothing else, and on the other side you could read the packets coming in, and each input received corresponds to one frame for the simulation to step forward. If no input arrives for a given render frame, the right side can’t advance forward, it has to wait for the next input to arrive. So let’s move forward with TCP, you’ve disabled Nagle’s Algorithm, and you’re sending inputs from the left to the right simulation once per-frame (60 times per-second). Here it gets a little complicated. Since we can’t simulate forward unless we have the input for the next frame, it’s not enough to just take whatever inputs arrive over the network and then run the simulation on inputs as they arrive because the result would be very jittery. Data sent across the network at 60HZ doesn’t typically arrive nicely spaced, 1/60th of a second between each packet. If you want this sort of behavior, you have to implement it yourself. Playout Delay BufferSuch a device is called a playout delay buffer. Unfortunately, the subject of playout delay buffers is a patent minefield. I would not advise searching for “playout delay buffer” or “adaptive playout delay” while at work. But in short, what you want to do is buffer packets for a short amount of time so they appear to be arriving at a steady rate even though in reality they arrive somewhat jittered. What you’re doing here is similar to what Netflix does when you stream a video. You pause a little bit initially so you have a buffer in case some packets arrive late and then once the delay has elapsed video frames are presented spaced the correct time apart. If your buffer isn’t large enough then the video playback will be hitchy. With deterministic lockstep your simulation behaves exactly the same way: showing hitches when the buffer isn’t large enough to smooth out the jitter. Of course, the cost of increasing the buffer size is additional latency, so you can’t just buffer your way out of all problems. At some point the user says enough! That’s too much latency added. No sir, I will not play your game with 1 second of extra delay :) My playout delay buffer implementation is really simple. You add inputs to it indexed by frame, and when the very first input is received, it stores the current local time on the receiver machine and from that point on delivers packets assuming they should play at that time + 100ms. You’ll likely need to something more complex for a real world situation, perhaps something that handles clock drift, and detecting when the simulation should slightly speed up or slow down to maintain a nice amount of buffering safety (being “adaptive”) while minimizing overall latency, but this is reasonably complicated and probably worth an article in itself. The goal is that under average conditions the playout delay buffer provides a steady stream of inputs for frame n, n+1, n+2 and so on, nicely spaced 1/60th of a second apart with no drama. In the worst case the time arrives for frame n and the input hasn’t arrived yet it returns null and the simulation is forced to wait. If packets get bunched up and delivered late, it’s possibly to have multiple inputs ready to dequeue per-frame. In this case I limit to 4 simulated frames per-render frame so the simulation has a chance to catch up, but doesn’t simulate for so long that it falls further behind, aka. the “spiral of death”. Is TCP good enough?Using this playout buffer strategy and sending inputs across TCP we ensure that all inputs arrive reliably and in-order. This is convenient, and after all, TCP is designed for exactly this situation: reliable-ordered data. In fact, It’s a common thing out there on the Internet for pundits to say stuff like: If you need reliable-ordered, you can’t do better than TCP! Your game doesn’t need UDP (yet) But I’m here to tell you this kind of thinking is dead wrong. Above you can see the simulation networked using deterministic lockstep over TCP at 100ms latency and 1% packet loss. If you look closely on the right side you can see hitches every few seconds. What’s happening here is that each time a packet is lost, TCP has to wait RTT*2 while it is resent (actually it can be much worse, but I’m being generous…). The hitches happen because with deterministic lockstep the right simulation can’t simulate frame n without input n, so it has to pause to wait for input n to be resent! That’s not all. It gets significantly worse as latency and packet loss increase. Here is the same simulation networked using deterministic lockstep over TCP at 250ms latency and 5% packet loss: Now I will concede that if you have no packet loss and/or a very small amount of latency then you very well may get acceptable results with TCP. But please be aware that if you use TCP it behaves terribly under bad network conditions. Can we do better than TCP?Can we beat TCP at its own game. Reliable-ordered delivery? The answer is an emphatic YES. But only if we change the rules of the game. Here’s the trick. We need to ensure that all inputs arrive reliably and in order. But if we send inputs in UDP packets, some of those packets will be lost. What if, instead of detecting packet loss after the fact and resending lost packets, we redundantly include all inputs in each UDP packet until we know for sure the other side has received them? Inputs are very small (6 bits). Let’s say we’re sending 60 inputs per-second (60fps simulation) and round trip time we know is going the be somewhere in 30-250ms range. Let’s say just for fun that it could be up to 2 seconds worst case and at this point we’ll time out the connection (screw that guy). This means that on average we only need to include between 2-15 frames of input and worst case we’ll need 120 inputs. Worst case is 120 x 6 = 720 bits. That’s only 90 bytes of input! That’s totally reasonable. We can do even better. It’s not common for inputs to change every frame. What if when we send our packet instead we start with the sequence number of the most recent input, and the 6 bits of the first (oldest) input, and the number of un-acked inputs. Then as we iterate across these inputs to write them to the packet we can write a single bit (1) if the next input is different to the previous, and (0) if the input is the same. So if the input is different from the previous frame we write 7 bits (rare). If the input is identical we write just one (common). Where inputs change infrequently this is a big win and in the worst case this really isn’t that bad. 120 bits of extra data sent. Just 15 bytes overhead worst case. Of course another packet is required from the right simulation to the left so the left side knows which inputs have been received. Each frame the right simulation reads input packets from the network before adding them to the playout delay buffer and keeps track of the most recent input it has received and sends this back to the left as an “ack” or acknowledgment for inputs. When the left side receives this ack it discards any inputs older than the most recent received input. This way we have only a small number of inputs in flight proportional to the round trip time between the two simulations. Flawless VictoryWe have beaten TCP by changing the rules of the game. Instead of “implementing 95% of TCP on top of UDP” we have implemented something totally different and better suited to our requirements. A protocol that redundantly sends inputs because we know they are small, so we never have to wait for retransmission. So exactly how much better is this approach than sending inputs over TCP? Let’s take a look… The video above shows deterministic lockstep synchronized over UDP using this technique with 2 seconds of latency and 25% packet loss. Imagine how awful TCP would look under these conditions. So in conclusion, even where TCP should have the most advantage, in the only networking model that relies on reliable-ordered data, we can still easily whip its ass with a simple protocol built on top of UDP. 译文译文出处 翻译：张乾光（星际迷航） 审校：陈敬凤(nunu) 介绍大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《网络物理仿真》，这个系列教程的目的是将物理仿真的状态通过网络进行广播。 在之前的文章中，我们讨论了物理仿真需要在网络上进行广播的各种属性。在这篇文章中，我们将使用具有确定性的帧同步技术来将物理仿真通过网络进行传递和广播。 具有确定性的帧同步是一种用来在一台电脑和其他电脑之间进行同步的方法，这种方法发送的是控制仿真状态变化的输入，而不是像其他方法那样发送的是仿真过程中物体的状态变化。这种方法的背后思想是给定一个初始状态，不妨设为S(n)，我们通过使用输入信息I(n)来运行仿真就能得到S(n+1)这个状态。然后我们可以通过S(n+1)这个状态和输入信息I(n+1)来运行仿真就能得到S(n+2)这个状态，我们可以一直重复这个过程得到S(n+3)、S(n+4)以及其后的各个状态。这看上去有点像是数学归纳法，我们可以只通过输入信息和之前的仿真状态就能得到后面的仿真状态-而且得到的仿真状态是高度一致，并且也不需要发送任何状态方面的同步。 这个网络模型的主要优点是所需的带宽仅仅用来传递输入信息，而输入信息所占的带宽其实是与仿真中物体的数目是完全无关的。你可以通过网络来对一百万个物体进行物理仿真，它所需的带宽会跟只对一个物体进行物理仿真所需的带宽完全相同。可以很容易的看到物理物体的状态通常是包含位置、方向、线性速度和角速度（如果是未压缩的话，这些状态一共需要52字节，在这里面假设方向使用的是四元数而其他所有的变量都是用vec3来表示），所以当你有大量的物体需要进行物理仿真的时候，这是一个非常具有吸引力的方案。 确定性如果要采用具有确定性的帧同步这个方案来将物理仿真网络化，首先要做的第一件事就是要确保你的仿真具有确定性。在这个上下文中，确定性其实和自由意志之类的没有关系。它只是意味着给定相同的初始条件和相同的一组输入，仿真能够给出完全相同的结果。而且我在这里要着重强调下是完全相同的结果。而不是说的什么在在浮点数容忍度内足够接近。这种精确是精确到比特位的。所以这种精确性使得你可以在每帧的末尾对整个物理状态做一个校验和，不同机器上面同一帧得到的校验和是完全一致的。 从上面的图中可以看到，这里面的仿真几乎是具有确定性的，但是不完全具有确定性。左边的仿真由玩家进行控制，而右边的仿真有完全一致的初始状态，输入信息也和左边完全相同，但是要有2秒钟的延迟。这两个仿真使用相同的间隔时间进行更新（使用相同的间隔时间进行更新也是确保得到完全一致结果的一个必要前提条件），并且在每一帧前对相同的输入信息进行相应。你可以注意到随着仿真的进行，那些一开始很微小的差异是如何一点点被扩大，最后导致两个仿真完全不同步。所以说这个仿真其实不具有确定性。 上面到底发生了什么?最后会导致两个仿真的结果差的这么大？这是因为我使用的物理引擎（ODE）在它的内部使用了一个随机数生成器来对约束处理的顺序进行随机化来提高稳定性。这个物理引擎是完全开源的，所以可以看看它的内部实现！不幸的是，由于左边的仿真处理约束的顺序和右边的仿真处理约束的顺序不同，这导致有一些轻微不同的结果。 幸运的是我们还是能找到让ODE这个物理引擎具有确定性的条件：要在同一台机器上、使用同一个编译好的二进制文件、并且在完全相同的操作系统上运行（这是必要的限制条件么？），还有就是在运行仿真之前通过dSetRandomSeed把随机数的种子设为当前帧的帧数。一旦满足这些条件的话，ODE这个物理引擎能够给出完全相同的结果，并且左边和右边的仿真能够保持高度一致的同步。 现在让我们针对上面这个情况给出一个警告。即使ODE这个物理引擎能够在相同的机器上得到确定性的结果，但是这并不一定意味着在不同编译器、不同的操作系统甚至不同的机器架构上（比如说在PowerPC架构上和在Intel架构上）它能够得到确定性的结果。事实上，由于浮点数的优化，在程序的debug版本和release版本之间可能都没有办法得到确定性的结果。浮点数的确定性是一个非常复杂的问题，而且这个问题没有银弹（意味着这个问题没有什么简单可行的解决办法）。要了解更多这方面的信息，请参考这篇文章。 网络输入 Inputs让我们讨论下具有确定性的帧同步的具体实现方法。 你可能想知道在我们这个示例仿真中输入信息到底是啥，以及我们该如何吧这些输入信息进行网络化。我们这个示例仿真是由键盘输入进行驱动的：方向键会给代表玩家的立方体施加一个力让他进行移动、按下空格键会把代表玩家的立方体提起来并把碰到的立方体四处滚落、按下‘z’键会启动katamari模式。 但是我们该如何对这些输入信息进行网络化呢？我们需要把整个键盘的状态在网络上进行传输么？在这些键被按下和释放的时候我们要发送这些事件么？不，整个键盘的状态不需要在网络上进行传输，我们只需要传输那些会影响仿真的按键。那么被按下和释放的键的事件需要在网络上进行传输么？不，这也不是一个好的策略。我们需要确保的是在仿真第n帧的时候右边的仿真能够应用完全相同的输入信息，所以我们不能仅仅是通过TCP来发送“按键按下”和“按键释放”的事件，因为这些事件到达网络的另外一侧的时间如果早于或者晚于第n帧的时候都会给仿真造成偏差。 相反我们做的事情是用一个结构来表示整个输入信息，并且在左边一侧仿真开始的时候，通过键盘的访问来填充这个结构并把填充好的结构放到一个滑动窗口中，我们在后面可以根据帧号来对这个输入进行访问。 123456789struct Input &#123; bool left; bool right; bool up; bool down; bool space; bool z; &#125;; 现在我们就可以通过上面的方法来把左边仿真的输入信息发送到右边仿真中去，这样右边的仿真就知道属于第n帧的输入信息到底是怎么样的。举个简单的例子来说，如果你在通过TCP进行发送的话，你可以简单的只发送输入信息而不发送其他的内容，而发送的输入信息的顺序隐含着帧号N。而在网络的另外一侧，你可以读取传送过来的数据包，并且对输入信息进行处理并把输入信息应用到仿真中去。我不推荐这种方法，但我们可以从这里开始，然后再向你展示如何把这种方法变得更好。 在进一步对这个方法进行优化之前，让我们先统一下使用的网络环境，让我们假设下我们是通过TCP进行数据传输，已经禁止了Nagle算法并且每帧都会从左边的仿真向右边的仿真发送一次输入信息（频率是每秒60次）。 这里面有一个问题会变得比较复杂。把左边仿真发生的输入信息通过网络进行传输，然后右边仿真并没有足够的时间来从网络上收到输入信息并利用这些到达的输入信息来模拟仿真，因为这个过程需要一定的时间。你不能按照某个频率在网络上发送信息并且期望它们能够按照完全相同一致的频率到达网络的另外一侧(比如说，每六十分之一秒到达一个数据包)。互联网并不是按照这个方式工作的。根本就没有这样的保证。 播放延迟缓冲区如果你想要做到这一点的话，你必须实现一个叫做播放延迟缓冲区的东西。不幸的是，播放延迟缓冲区收到了专利保护，也就是一个专利雷区。我不建议读者在实际使用确定性的帧同步模型的时候搜索“播放延迟缓冲区”或者是“自适应性延迟缓冲区”。但简而言之，你所需要做的事情是缓存收到的数据包一小段时间以便让这些数据包表现的像是以一个稳定的速度到达那样，即使实际上它们的到达时间是充满抖动的。 你现在所做的事情就跟你在看一个视频流的时候，Netflix所做的事情是很类似的。你在最初开始的时候停顿了一下以便你可以拥有一个缓冲区，这样即使一些数据包的到达时间有点晚，但是这种延迟不会对视频帧按正确时间间距的表现有什么影响，视频帧仍然会按照正确的时间间隔一帧帧的播放。当然如果你的缓冲区没有足够大的话，那么这些视频帧的播放可能还是会充满一些抖动。有了确定性的帧同步机制，你的模拟仿真将会以完全相同的方式执行。我建议在播放的时候最好在一开始有100毫秒-250毫秒的延迟。在下面的例子中，我使用的是100毫秒的延迟，这是因为我让延迟最小化来增强响应性。 我的播放延迟缓冲区的实现非常的简单。是将输入信息按照帧序号进行添加，当收到第一个输入信息的时候，它保存了接收方机器上的当前本地时间，并且从那一个时刻起假设所有到达的数据包都会带上100毫秒的延迟。你可能需要一些更加复杂的机制来适应真实世界的情况，比如说可能需要处理时钟漂移、检测在什么时候应该适当的加速或者减慢模拟的速度来让缓冲区的大小在能够保证整体延迟最小的情况下保持在一个适度的情形（这就是所谓的“自适应”），但是这些内容可能会相当的复杂并且可能需要一整篇文章来专门对这些情况进行专门论述。而且如前所述，这些内容还涉及到了专利保护方面的内容，所以这些内容我就不详细展开了，把如何处理这些东西全部托付给你自己实现。 在平均情况下，播放延迟缓冲区给帧n、n+1、n+2以及后续的帧提供了一个稳定的输入信息流，非常完美的以六十分之一秒的间隔依次到达。在最坏的情况下，就是已经该执行第N帧的模拟仿真了，但是这一帧的输入信息还没有到达，那么它就会返回一个空指针，这样整个模拟仿真就必须在那里进行等待了。如果数据包被集中起来发送并且到达接收方的时候已经比预期时间延迟了，这可能会导致多个帧的输入信息同时准备好等待出列进行计算。如果是这种情况的话，我会限制在一个渲染帧的时间最多只能进行4次模拟仿真，这样给模拟仿真一个追上来的机会。如果你把这个值设置的更高的话，那么可能会引起更多其他的问题，比如卡顿，因为你可能需要超过六十分之一秒的时间来运行这些帧（这可能会造成一个非常不好的反馈体验）。总而言之，重要的是确保你的模拟仿真在使用确定性的帧同步这个方案的时候性能不是在中央处理器这一端受限的，否则的话，你在运行更多的模拟帧来追上正常的模拟速度的时候会遇到很多麻烦。 TCP足够好了吗通过使用这种延迟缓冲区的策略以及通过TCP协议来发送输入信息，我们可以很轻松的确保所有的输入信息会有序的到达并且传输是可信赖的。这就是一开始TCP协议在设计的时候希望达到的目标。实际上，下面这些东西就是互联网的专家常说的一些东西： 如果你需要一个可以信赖的有序的发送信息的方法，你不可能找到一种比通过TCP协议进行传输更好的方法！ 你的游戏根本就不会需要UDP协议。 我在这里将告诉你上面这些想法都是大错特错的。 在上面的视频中，你可以看到如果网络同步模型使用基于TCP协议的确定性的帧同步模型的话，模拟仿真的网络延迟大概是100毫秒，并且有百分之一的丢包率。如果你仔细看右边的话，你可以每隔几秒就会出现一些抖动。如果你在两边都出现这种的情况，那么很抱歉这意味着你的电脑的性能对于播放这些视频而言可能有些艰难。如果是这种情况的话，我建议下载这个视频然后离线观看。无论如何，这里所发生的事情是当一个数据包丢失的时候，TCP协议需要等待至少２个往返时延才会重新发送这个数据包（实际上这里面的等待时间可能会更糟，但是我很慷慨的设定了一个非常理想的情况。。。）。所以上面发生的抖动原因是确定性的帧同步模型要求右边的模拟仿真在没有第n帧的输入信息的时候不能执行第N帧的模拟仿真计算，所以整个模拟仿真就停下来等待对应帧的输入信息的到达! 这还不是全部！随着延迟时间的增大和丢包率的增加，整个情况会变得更加的糟糕。这是在250毫秒延迟和百分之五丢包率的情况下，使用基于TCP协议的确定性的帧同步模型进行相同的仿真模拟运算导致的结果： 现在我要承认一个事情，如果延迟时间设置的非常低的话同时不存在丢包的情况下，那么使用TCP协议进行输入信息的传输会是一个非常可以接受的结果。但是请注意，如果你使用TCP协议来发送时间敏感的数据的话，随着延迟时间的增大和丢包率的增加，整个结果会急剧恶化。 我们能比TCP做得更好吗我们可以做得更好吗?我们能在自己的游戏里面找到一种比使用TCP协议更好的办法。同时还能实现可信赖的有序传递？ 答案是肯定的。但前提是我们需要改变游戏的规则。 下面将具体描述下我们将使用的技巧。我们需要确保所有的输入信息能够可靠地按顺序到达。但是如果我们只发送UDP数据包输入。但是如果我们只使用UDP数据包来发送输入信息的话，这里面的一些数据包会丢失。那么如果我们不采用事后检测的方法来判断哪些数据包丢失并发送这些丢失的数据包的话，我们采用另外一种方法，只是把我们有的所有输入信息都冗余的发送直到我们知道这些输入信息成功的到达另外一侧怎么样？ 输入信息都非常非常的小（只有6比特这么大）。让我们假设下我们在每秒需要发送60个输入信息（因为模拟仿真的频率是60fps ），而且我们知道一个往返的时间大概是在30毫秒到250毫秒之间。纯粹为了好玩，让我们假设下载最糟糕的情况下，一个往返的时间可以高达2秒，如果出现这种情况的话，那么整个连接就会超时。这意味着在平均情况我们只需要包括大概2到15帧的输入信息，而在最坏情况下，我们大概需要120帧的输入信息。那么最坏情况下，输入信息的大小是120 x 6 = 720比特。这只是90字节的输入信息!这是安全合理的。 我们还能做的更好。在每一帧中都出现输入信息的变化是非常不常见的。我们可以用最近的那个input的序列号和第一个input的6比特还有所有未被确认的输入信息的数目来做一些事。然后，当我们对这些输入信息进行遍历将它们写入数据包的时候，如果发现这一帧的输入信息如果和之前帧的输入信息不同的话，我们可以写入一个单独的比特位（1），如果发现这一帧的输入信息如果和之前帧的输入信息相同的话，我们可以写入一个单独的比特位（0）。所以这一帧的输入信息如果和之前帧的输入信息不同的话（这种情况比较少），我们需要写入7个比特位，这一帧的输入信息如果和之前帧的输入信息相同的话（这种情况其实非常常见），我们只需要写入1个比特位。在输入信息很少发生变化的情况，这是一个重大的胜利，而在最坏的情况下出现的情况也不会非常糟糕。只需要发送额外120个比特的数据，也就是说在最坏情况下，也只有15字节的额外开销. 当然在这种情况下，需要从右边的模拟仿真中发送一个数据包到左边的模拟仿真中去，这样左边的模拟仿真才知道哪些输入信息被成功收到了。在每一帧，右边的模拟仿真都会从网络中读取输入的数据包，然后才会把这些数据包添加到延迟播放缓冲区，并且通过帧号记录它已经收到的最近那一帧的输入信息，或者如果你想容易一点处理这个问题的话，那么使用一个16比特的序列号就能很好的包装这个信息。在所有的输入数据包都被处理以后，如果右边的模拟仿真收到任何帧的输入信息以后都会回复一个数据包给左边的模拟仿真，告诉它最近收到的最新序列号是多少，这基本就是一个“ack”包，也就是确认包。 当左边的模拟仿真收到这个“ack”包，也就是确认包以后，它会滑动输入信息窗口并且丢弃比已经确认的序列号还老的输入信息包。已经没有必要再发送这些输入信息包给右边的模拟仿真了，因为已经知道右边的模拟仿真成功的接受到了这些输入信息包。通过这种方式，我们通常只有少量的输入信息正在传输过程中，而且这个数量还是与数据包的往返时间成正比的。 完美胜利我们通过改变游戏的规则成功了找到了一种比TCP协议更好的办法。我们并不是通过在UDP协议纸上构建了实现TCP协议百分之九十五的功能的新协议，而是实现了一种完全不同的方法，而且更加适合我们的要求：数据对时间非常敏感。我们开发了一个自定义的协议，可以冗余的发送input，因为我们知道这些input非常小, 所以我们不必去等待重传它们。 所以这种方法到底比通过TCP协议来发送数据好多少呢？ 让我们通过一个例子来看一下。 上面的视频是基于UDP协议来使用具有确定性的帧同步模型，延迟时间是２秒，并且有百分之二十五的丢包率。想象下如果我们是使用基于TCP协议的具有确定性的帧同步模型，我们该看到多么可怕的场景！ 所以最后我们能得到这么一个结论：即使这是一个TCP协议最具有优势的情况下，这是唯一一个依赖可靠性、有序性数据传输的网络模型，我们还是可以很容易的通过一个自定义的协议基于UDP来发送我们的数据包，并且得到的效果更好。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟二之网络物理部分的视频演示]]></title>
    <url>%2F2019%2F05%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%BA%8C%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E9%83%A8%E5%88%86%E7%9A%84%E8%A7%86%E9%A2%91%E6%BC%94%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[原文出处 Introduction to Networked Physics IntroductionHi, I’m Glenn Fiedler and welcome to the first article in Networked Physics. In this article series we’re going to network a physics simulation three different ways: deterministic lockstep, snapshot interpolation and state synchronization. But before we get to this, let’s spend some time exploring the physics simulation we’re going to network in this article series: Here I’ve setup a simple simulation of a cube in the open source physics engine ODE. The player moves around by applying forces at its center of mass. The physics simulation takes this linear motion and calculates friction as the cube collides with the ground, inducing a rolling and tumbling motion. This is why I chose a cube instead a sphere. I want this complex, unpredictable motion because rigid bodies in general move in interesting ways according to their shape. An Interactive WorldNetworked physics get interesting when the player interacts with other physically simulated objects, especially when those objects push back and affect the motion of the player. So let’s add some more cubes to the simulation: When the player interacts with a cube it turns red. When that cube comes to rest it turns back to grey (non-interacting). While it’s cool to roll around and interact with other cubes, what I really wanted was a way to push lots of cubes around. What I came up with is this: As you can see, interactions aren’t just direct. Red cubes pushed around by the player turn other cubes they touch red as well. This way, interactions fan out to cover all affected objects. A Complicated CaseI also wanted a very complex coupled motion between the player and non-player cubes such they become one system: a group of rigid bodies joined together by constraints. To implement this I thought it would be cool if the player could roll around and create a ball of cubes, like in one of my favorite games Katamari Damacy. Cubes within a certain distance of the player have a force applied towards the center of the cube. These cubes remain physically simulated while in the katamari ball, they are not just “stuck” to the player like in the original game. This is a very difficult situation for networked physics!]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟一之网络物理部分的简介]]></title>
    <url>%2F2019%2F05%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%B8%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E9%83%A8%E5%88%86%E7%9A%84%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 大家好，我是格伦·菲德勒，是一位来自洛杉矶的职业游戏开发者。 我作为一名职业游戏开发者已经有15年了。而在这其中的十年时光里，我都是专门做网络编程的。在这期间我大部分的业余时间一直致力于研究网络物理模拟方面的问题。在这个系列文章中我的目标是分享我已经知道的一切有关网络物理模拟方面的知识。 写这些文章是一个工作量很大的工作，而且我完全是在我的业余时间里面来做这些事情的。如果有了你们的支持，我可以找时间来继续写这个系列的文章。如果可以的话，请通过 patreon 支持我的工作。 网络物理部分的入门是相当困难的。你可能想知道你的物理模拟是否需要确定性以便可以进行网络传输和通信？你应该通过网络来发送物体的物理状态么？或者你应该通过网络来发送一些诸如碰撞时间或者物体相互作用力这些东西么？你到底是应该通过UDP还是TCP来传送数据？你应该使用客户端/服务器通信模型还是点对点的通信模型？ 你是否需要一个专门的服务器？如何隐藏播放动作的时候的延迟？如何不让玩家作弊？ 人们经常问我这些问题。大多数情况下这些问题的正确答案依赖于他们所选用的网络模型。你可能甚至不知道你正在使用或计划使用的网络模型是什么，但事实是你必须选择一个网络模型并且你选择的这个网络模型会使某些事情变得容易，并使得另外一些事情变得困难。 在这个系列文章中我们将使用三种不同的方式来构建一个网络物理模拟的例子，并通过这些例子来对不同的网络模型展开一个讨论。我把这些基本技术称为“同步策略”。他们是：《确定性的帧同步》、《快照插值》和《状态同步》。 带宽是网络物理部分的另外一个重要方面。我该怎么做才能对所有这些对象进行同步状态？因此，我们打算花一整篇文章来进行介绍一个带宽优化的案例，向你展示如何将快照插值技术的带宽从每秒18m减少到每秒256k。 在选择了合适的同步策略以及合理的优化完带宽以后，我们现在准备讨论客户机/服务器与点对点对等网络之间的优劣性了。除了讨论利弊以外，我还将分享在艰苦的发布使用客户机/服务器网络模型的游戏和点对点对等网络模型的游戏的过程中学到的那些经验和教训。这篇文章会有你在其他地方找不到的非常具体、务实的信息。 最后，我们讨论构建在基本同步策略上的不同网络模型，这些网络模型的细节各有不同。在这里。你会发现你的网络物理模拟可以有许多不同的选择：《确定性帧同步网络模型》、《分布式权威网络模型》和《带有客户端预测的服务器仲裁网络模型》。 所以系上安全带准备出发了。我们还有一大堆材料需要学习！ 接下来的这篇文章是：《物理模拟》 为什么格伦·菲德勒在Patreon上寻求自助？ 嘿，大家好，我是格伦·菲德勒，网站gafferongames.com的作者。在过去的10年，我给大家分享了许多游戏开发方面的文章：《如何解决你的时间戳》、《整合基础》、《UDP与TCP》、《每个程序员都需要了解游戏开发网络方面的知识》以及其他一些文章。我还分享了《物理编程技巧》、《网络游戏编程的基础知识》和我的个人项目虚拟围棋。我想继续新的系列文章《网络物理模拟》以及《构建一个游戏网络协议》，但是我需要你的帮助！托管代码和文章需要费用，写这些文章需要花费大量的时间去研究和写作。如果你喜欢gafferongames.com上面的文章，请向我展现出你对我的支持并鼓励我写更多的文章并开放更多的源代码！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 如果你觉得这篇文章有价值，请在 patreon 上支持原作者。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发五之每个游戏开发者都需要知道的游戏网络知识]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%BA%94%E4%B9%8B%E6%AF%8F%E4%B8%AA%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E8%80%85%E9%83%BD%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[原文原文出处 IntroductionHi, I’m Glenn Fiedler and welcome to Networking for Game Programmers. Have you ever wondered how multiplayer games work? From the outside it seems magical: two or more players sharing a consistent experience across the network like they actually exist together in the same virtual world. But as programmers we know the truth of what is actually going on underneath is quite different from what you see. It turns out it’s all an illusion. A massive sleight-of-hand. What you perceive as a shared reality is only an approximation unique to your own point of view and place in time. Peer-to-Peer LockstepIn the beginning games were networked peer-to-peer, with each each computer exchanging information with each other in a fully connected mesh topology. You can still see this model alive today in RTS games, and interestingly for some reason, perhaps because it was the first way - it’s still how most people think that game networking works. The basic idea is to abstract the game into a series of turns and a set of command messages when processed at the beginning of each turn direct the evolution of the game state. For example: move unit, attack unit, construct building. All that is needed to network this is to run exactly the same set of commands and turns on each player’s machine starting from a common initial state. Of course this is an overly simplistic explanation and glosses over many subtle points, but it gets across the basic idea of how networking for RTS games work. You can read more about this networking model here: 1500 Archers on a 28.8: Network Programming in Age of Empires and Beyond. It seems so simple and elegant, but unfortunately there are several limitations. First, it’s exceptionally difficult to ensure that a game is completely deterministic; that each turn plays out identically on each machine. For example, one unit could take slightly a different path on two machines, arriving sooner to a battle and saving the day on one machine, while arriving later on the other and erm. not saving the day. Like a butterfly flapping it’s wings and causing a hurricane on the other side of the world, one tiny difference results in complete desynchronization over time. The next limitation is that in order to ensure that the game plays out identically on all machines it is necessary to wait until all player’s commands for that turn are received before simulating that turn. This means that each player in the game has latency equal to the most lagged player. RTS games typically hide this by providing audio feedback immediately and/or playing cosmetic animation, but ultimately any truly game affecting action may occur only after this delay has passed. The final limitation occurs because of the way the game synchronizes by sending just the command messages which change the state. In order for this to work it is necessary for all players to start from the same initial state. Typically this means that each player must join up in a lobby before commencing play, although it is technically possible to support late join, this is not common due to the difficulty of capturing and transmitting a completely deterministic starting point in the middle of a live game. Despite these limitations this model naturally suits RTS games and it still lives on today in games like “Command and Conquer”, “Age of Empires” and “Starcraft”. The reason being that in RTS games the game state consists of many thousands of units and is simply too large to exchange between players. These games have no choice but to exchange the commands which drive the evolution of the game state. But for other genres, the state of the art has moved on. So that’s it for the deterministic peer-to-peer lockstep networking model. Now lets look at the evolution of action games starting with Doom, Quake and Unreal. Client/ServerIn the era of action games, the limitations of peer-to-peer lockstep became apparent in Doom, which despite playing well over the LAN played terribly over the internet for typical users: Although it is possible to connect two DOOM machines together across the Internet using a modem link, the resulting game will be slow, ranging from the unplayable (e.g. a 14.4Kbps PPP connection) to the marginally playable (e.g. a 28.8Kbps modem running a Compressed SLIP driver). Since these sorts of connections are of only marginal utility, this document will focus only on direct net connections. The problem of course was that Doom was designed for networking over LAN only, and used the peer-to-peer lockstep model described previously for RTS games. Each turn player inputs (key presses etc.) were exchanged with other peers, and before any player could simulate a frame all other player’s key presses needed to be received. In other words, before you could turn, move or shoot you had to wait for the inputs from the most lagged modem player. Just imagine the wailing and gnashing of teeth that this would have resulted in for the sort of folks with internet connections that were “of only marginal utility”. :) In order to move beyond the LAN and the well connected elite at university networks and large companies, it was necessary to change the model. And in 1996, that’s exactly what John Carmack and his team did when he released Quake using client/server instead of peer-to-peer. Now instead of each player running the same game code and communicating directly with each other, each player was now a “client” and they all communicated with just one computer called the “server”. There was no longer any need for the game to be deterministic across all machines, because the game really only existed on the server. Each client effectively acted as a dumb terminal showing an approximation of the game as it played out on the server. In a pure client/server model you run no game code locally, instead sending your inputs such as key presses, mouse movement, clicks to the server. In response the server updates the state of your character in the world and replies with a packet containing the state of your character and other players near you. All the client has to do is interpolate between these updates to provide the illusion of smooth movement and BAM you have a networked game. This was a great step forward. The quality of the game experience now depended on the connection between the client and the server instead of the most lagged peer in the game. It also became possible for players to come and go in the middle of the game, and the number of players increased as client/server reduced the bandwidth required on average per-player. But there were still problems with the pure client/server model: While I can remember and justify all of my decisions about networking from DOOM through Quake, the bottom line is that I was working with the wrong basic assumptions for doing a good internet game. My original design was targeted at &lt; 200ms connection latencies. People that have a digital connection to the internet through a good provider get a pretty good game experience. Unfortunately, 99% of the world gets on with a slip or ppp connection over a modem, often through a crappy overcrowded ISP. This gives 300+ ms latencies, minimum. Client. User’s modem. ISP’s modem. Server. ISP’s modem. User’s modem. Client. God, that sucks.Ok, I made a bad call. I have a T1 to my house, so I just wasn’t familliar with PPP life. I’m addressing it now. The problem was of course latency. What happened next would change the industry forever. Client-Side PredictionIn the original Quake you felt the latency between your computer and the server. Press forward and you’d wait however long it took for packets to travel to the server and back to you before you’d actually start moving. Press fire and you wait for that same delay before shooting. If you’ve played any modern FPS like Call of Duty: Modern Warfare, you know this is no longer what happens. So how exactly do modern FPS games remove the latency on your own actions in multiplayer? When writing about his plans for the soon to be released QuakeWorld, John Carmack said: I am now allowing the client to guess at the results of the users movement until the authoritative response from the server comes through. This is a biiiig architectural change. The client now needs to know about solidity of objects, friction, gravity, etc. I am sad to see the elegant client-as-terminal setup go away, but I am practical above idealistic. So now in order to remove the latency, the client runs more code than it previously did. It is no longer a dumb terminal sending inputs to the server and interpolating between state sent back. Instead it is able to predict the movement of your character locally and immediately in response to your input, running a subset of the game code for your player character on the client machine. Now as soon as you press forward, there is no wait for a round trip between client and server - your character start moving forward right away. The difficulty of this approach is not in the prediction, for the prediction works just as normal game code does - evolving the state of the game character forward in time according to the player’s input. The difficulty is in applying the correction back from the server to resolve cases when the client and server disagree about where the player character should be and what it is doing. Now at this point you might wonder. Hey, if you are running code on the client - why not just make the client authoritative over their player character? The client could run the simulation code for their own character and simply tell the server where they are each time they send a packet. The problem with this is that if each player were able to simply tell the server “here is my current position” it would be trivially easy to hack the client such that a cheater could instantly dodge the RPG about to hit them, or teleport instantly behind you to shoot you in the back. So in FPS games it is absolutely necessary that the server is the authoritative over the state of each player character, in-spite of the fact that each player is locally predicting the motion of their own character to hide latency. As Tim Sweeney writes in The Unreal Networking Architecture: “The Server Is The Man”. Here is where it gets interesting. If the client and the server disagree, the client must accept the update for the position from the server, but due to latency between the client and server this correction is necessarily in the past. For example, if it takes 100ms from client to server and 100ms back, then any server correction for the player character position will appear to be 200ms in the past, relative to the time up to which the client has predicted their own movement. If the client were to simply apply this server correction update verbatim, it would yank the client back in time, completely undoing any client-side prediction. How then to solve this while still allowing the client to predict ahead? The solution is to keep a circular buffer of past character state and input for the local player on the client, then when the client receives a correction from the server, it first discards any buffered state older than the corrected state from the server, and replays the state starting from the corrected state back to the present “predicted” time on the client using player inputs stored in the circular buffer. In effect the client invisibly “rewinds and replays” the last n frames of local player character movement while holding the rest of the world fixed. This way the player appears to control their own character without any latency, and provided that the client and server character simulation code is reasonable, giving roughly exactly the same result for the same inputs on the client and server, it is rarely corrected. It is as Tim Sweeney describes: … the best of both worlds: In all cases, the server remains completely authoritative. Nearly all the time, the client movement simulation exactly mirrors the client movement carried out by the server, so the client’s position is seldom corrected. Only in the rare case, such as a player getting hit by a rocket, or bumping into an enemy, will the client’s location need to be corrected. In other words, only when the player’s character is affected by something external to the local player’s input, which cannot possibly be predicted on the client, will the player’s position need to be corrected. That and of course, if the player is attempting to cheat :) 译文译文出处]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发四之基于UDP的可靠性与排序和避免拥堵]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E5%9B%9B%E4%B9%8B%E5%9F%BA%E4%BA%8EUDP%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%92%8C%E9%81%BF%E5%85%8D%E6%8B%A5%E5%A0%B5%2F</url>
    <content type="text"><![CDATA[原文原文出处 IntroductionHi, I’m Glenn Fiedler and welcome to Networking for Game Programmers. In the previous article, we added our own concept of virtual connection on top of UDP. In this article we’re going to add reliability, ordering and congestion avoidance to our virtual UDP connection. The Problem with TCPThose of you familiar with TCP know that it already has its own concept of connection, reliability-ordering and congestion avoidance, so why are we rewriting our own mini version of TCP on top of UDP? The issue is that multiplayer action games rely on a steady stream of packets sent at rates of 10 to 30 packets per second, and for the most part, the data contained is these packets is so time sensitive that only the most recent data is useful. This includes data such as player inputs, the position, orientation and velocity of each player character, and the state of physics objects in the world. The problem with TCP is that it abstracts data delivery as a reliable ordered stream. Because of this, if a packet is lost, TCP has to stop and wait for that packet to be resent. This interrupts the steady stream of packets because more recent packets must wait in a queue until the resent packet arrives, so packets are received in the same order they were sent. What we need is a different type of reliability. Instead of having all data treated as a reliable ordered stream, we want to send packets at a steady rate and get notified when packets are received by the other computer. This allows time sensitive data to get through without waiting for resent packets, while letting us make our own decision about how to handle packet loss at the application level. It is not possible to implement a reliability system with these properties using TCP, so we have no choice but to roll our own reliability on top of UDP. Sequence NumbersThe goal of our reliability system is simple: we want to know which packets arrive at the other side of the connection. First we need a way to identify packets. What if we had added the concept of a “packet id”? Let’s make it an integer value. We could start this at zero then with each packet we send, increase the number by one. The first packet we send would be packet 0, and the 100th packet sent is packet 99. This is actually quite a common technique. It’s even used in TCP! These packet ids are called sequence numbers. While we’re not going to implement reliability exactly as TCP does, it makes sense to use the same terminology, so we’ll call them sequence numbers from now on. Since UDP does not guarantee the order of packets, the 100th packet received is not necessarily the 100th packet sent. It follows that we need to insert the sequence number somewhere in the packet, so that the computer at the other side of the connection knows which packet it is. We already have a simple packet header for the virtual connection from the previous article, so we’ll just add the sequence number in the header like this: 123[uint protocol id] [uint sequence] (packet data…) Now when the other computer receives a packet it knows its sequence number according to the computer that sent it. AcksNow that we can identify packets using sequence numbers, the next step is to let the other side of the connection know which packets we receive. Logically this is quite simple, we just need to take note of the sequence number of each packet we receive, and send those sequence numbers back to the computer that sent them. Because we are sending packets continuously between both machines, we can just add the ack to the packet header, just like we did with the sequence number: 1234[uint protocol id] [uint sequence] [uint ack] (packet data…) Our general approach is as follows: Each time we send a packet we increase the local sequence number When we receieve a packet, we check the sequence number of the packet against the sequence number of the most recently received packet, called the remote sequence number. If the packet is more recent, we update the remote sequence to be equal to the sequence number of the packet. When we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack. This simple ack system works provided that one packet comes in for each packet we send out. But what if packets clump up such that two packets arrive before we send a packet? We only have space for one ack per-packet, so what do we do? Now consider the case where one side of the connection is sending packets at a faster rate. If the client sends 30 packets per-second, and the server only sends 10 packets per-second, we need at least 3 acks included in each packet sent from the server. Let’s make it even more complex! What if the packet containing the ack is lost? The computer that sent the packet would think the packet got lost but it was actually received! It seems like we need to make our reliability system… more reliable! Reliable AcksHere is where we diverge significantly from TCP. What TCP does is maintain a sliding window where the ack sent is the sequence number of the next packet it expects to receive, in order. If TCP does not receive an ack for a given packet, it stops and resends a packet with that sequence number again. This is exactly the behavior we want to avoid! In our reliability system, we never resend a packet with a given sequence number. We sequence n exactly once, then we send n+1, n+2 and so on. We never stop and resend packet n if it was lost, we leave it up to the application to compose a new packet containing the data that was lost, if necessary, and this packet gets sent with a new sequence number. Because we’re doing things differently to TCP, its now possible to have holes in the set of packets we ack, so it is no longer sufficient to just state the sequence number of the most recent packet we have received. We need to include multiple acks per-packet. How many acks do we need? As mentioned previously we have the case where one side of the connection sends packets faster than the other. Let’s assume that the worst case is one side sending no less than 10 packets per-second, while the other sends no more than 30. In this case, the average number of acks we’ll need per-packet is 3, but if packets clump up a bit, we would need more. Let’s say 6-10 worst case. What about acks that don’t get through because the packet containing the ack is lost? To solve this, we’re going to use a classic networking strategy of using redundancy to defeat packet loss! Let’s include 33 acks per-packet, and this isn’t just going to be up to 33, but always 33. So for any given ack we redundantly send it up to 32 additional times, just in case one packet with the ack doesn’t get through! But how can we possibly fit 33 acks in a packet? At 4 bytes per-ack thats 132 bytes! The trick is to represent the 32 previous acks before “ack” using a bitfield: 12345[uint protocol id] [uint sequence] [uint ack] [uint ack bitfield] (packet data…) We define “ack bitfield” such that each bit corresponds to acks of the 32 sequence numbers before “ack”. So let’s say “ack” is 100. If the first bit of “ack bitfield” is set, then the packet also includes an ack for packet 99. If the second bit is set, then packet 98 is acked. This goes all the way down to the 32nd bit for packet 68. Our adjusted algorithm looks like this: Each time we send a packet we increase the local sequence number When we receive a packet, we check the sequence number of the packet against the remote sequence number. If the packet sequence is more recent, we update the remote sequence number. When we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack. The ack bitfield is calculated by looking into a queue of up to 33 packets, containing sequence numbers in the range [remote sequence - 32, remote sequence]. We set bit n (in [1,32]) in ack bits to 1 if the sequence number remote sequence - n is in the received queue. Additionally, when a packet is received, ack bitfield is scanned and if bit n is set, then we acknowledge sequence number packet sequence - n, if it has not been acked already. With this improved algorithm, you would have to lose 100% of packets for more than a second to stop an ack getting through. And of course, it easily handles different send rates and clumped up packet receives. Detecting Lost PacketsNow that we know what packets are received by the other side of the connection, how do we detect packet loss? The trick here is to flip it around and say that if you don’t get an ack for a packet within a certain amount of time, then we consider that packet lost. Given that we are sending at no more than 30 packets per second, and we are redundantly sending acks roughly 30 times, if you don’t get an ack for a packet within one second, it is very likely that packet was lost. So we are playing a bit of a trick here, while we can know 100% for sure which packets get through, but we can only be reasonably certain of the set of packets that didn’t arrive. The implication of this is that any data which you resend using this reliability technique needs to have its own message id so that if you receive it multiple times, you can discard it. This can be done at the application level. Handling Sequence Number Wrap-AroundNo discussion of sequence numbers and acks would be complete without coverage of sequence number wrap around! Sequence numbers and acks are 32 bit unsigned integers, so they can represent numbers in the range [0,4294967295]. Thats a very high number! So high that if you sent 30 packets per-second, it would take over four and a half years for the sequence number to wrap back around to zero. But perhaps you want to save some bandwidth so you shorten your sequence numbers and acks to 16 bit integers. You save 4 bytes per-packet, but now they wrap around in only half an hour. So how do we handle this wrap around case? The trick is to realize that if the current sequence number is already very high, and the next sequence number that comes in is very low, then you must have wrapped around. So even though the new sequence number is _numerically_lower than the current sequence value, it actually represents a more recent packet. For example, let’s say we encoded sequence numbers in one byte (not recommended btw. :)), then they would wrap around after 255 like this: 1… 252, 253, 254, 255, 0, 1, 2, 3, … To handle this case we need a new function that is aware of the fact that sequence numbers wrap around to zero after 255, so that 0, 1, 2, 3 are considered more recent than 255. Otherwise, our reliability system stops working after you receive packet 255. Here’s a function for 16 bit sequence numbers: 12345inline bool sequence_greater_than( uint16_t s1, uint16_t s2 ) &#123; return ( ( s1 &gt; s2 ) &amp;&amp; ( s1 - s2 &lt;= 32768 ) ) || ( ( s1 &lt; s2 ) &amp;&amp; ( s2 - s1 &gt; 32768 ) ); &#125; This function works by comparing the two numbers and their difference. If their difference is less than 1⁄2 the maximum sequence number value, then they must be close together - so we just check if one is greater than the other, as usual. However, if they are far apart, their difference will be greater than 1⁄2 the max sequence, then we paradoxically consider the sequence number more recent if it is less than the current sequence number. This last bit is what handles the wrap around of sequence numbers transparently, so 0,1,2 are considered more recent than 255. Make sure you include this in any sequence number processing you do. Congestion AvoidanceWhile we have solved reliability, there is still the question of congestion avoidance. TCP provides congestion avoidance as part of the packet when you get TCP reliability, but UDP has no congestion avoidance whatsoever! If we just send packets without some sort of flow control, we risk flooding the connection and inducing severe latency (2 seconds plus!) as routers between us and the other computer become congested and buffer up packets. This happens because routers try very hard to deliver all the packets we send, and therefore tend to buffer up packets in a queue before they consider dropping them. While it would be nice if we could tell the routers that our packets are time sensitive and should be dropped instead of buffered if the router is overloaded, we can’t really do this without rewriting the software for all routers in the world. Instead, we need to focus on what we can actually do which is to avoid flooding the connection in the first place. We try to avoid sending too much bandwidth in the first place, and then if we detect congestion, we attempt to back off and send even less. The way to do this is to implement our own basic congestion avoidance algorithm. And I stress basic! Just like reliability, we have no hope of coming up with something as general and robust as TCP’s implementation on the first try, so let’s keep it as simple as possible. Measuring Round Trip TimeSince the whole point of congestion avoidance is to avoid flooding the connection and increasing round trip time (RTT), it makes sense that the most important metric as to whether or not we are flooding our connection is the RTT itself. We need a way to measure the RTT of our connection. Here is the basic technique: For each packet we send, we add an entry to a queue containing the sequence number of the packet and the time it was sent. Each time we receive an ack, we look up this entry and note the difference in local time between the time we receive the ack, and the time we sent the packet. This is the RTT time for that packet. Because the arrival of packets varies with network jitter, we need to smooth this value to provide something meaningful, so each time we obtain a new RTT we move a percentage of the distance between our current RTT and the packet RTT. 10% seems to work well for me in practice. This is called an exponentially smoothed moving average, and it has the effect of smoothing out noise in the RTT with a low pass filter. To ensure that the sent queue doesn’t grow forever, we discard packets once they have exceeded some maximum expected RTT. As discussed in the previous section on reliability, it is exceptionally likely that any packet not acked within a second was lost, so one second is a good value for this maximum RTT. Now that we have RTT, we can use it as a metric to drive our congestion avoidance. If RTT gets too large, we send data less frequently, if its within acceptable ranges, we can try sending data more frequently. Simple Binary Congestion AvoidanceAs discussed before, let’s not get greedy, we’ll implement a very basic congestion avoidance. This congestion avoidance has two modes. Good and bad. I call it simple binary congestion avoidance. Let’s assume you send packets of a certain size, say 256 bytes. You would like to send these packets 30 times a second, but if conditions are bad, you can drop down to 10 times a second. So 256 byte packets 30 times a second is around 64kbits/sec, and 10 times a second is roughly 20kbit/sec. There isn’t a broadband network connection in the world that can’t handle at least 20kbit/sec, so we’ll move forward with this assumption. Unlike TCP which is entirely general for any device with any amount of send/recv bandwidth, we’re going to assume a minimum supported bandwidth for devices involved in our connections. So the basic idea is this. When network conditions are “good” we send 30 packets per-second, and when network conditions are “bad” we drop to 10 packets per-second. Of course, you can define “good” and “bad” however you like, but I’ve gotten good results considering only RTT. For example if RTT exceeds some threshold (say 250ms) then you know you are probably flooding the connection. Of course, this assumes that nobody would normally exceed 250ms under non-flooding conditions, which is reasonable given our broadband requirement. How do you switch between good and bad? The algorithm I like to use operates as follows: If you are currently in good mode, and conditions become bad, immediately drop to bad mode If you are in bad mode, and conditions have been good for a specific length of time ’t’, then return to good mode To avoid rapid toggling between good and bad mode, if you drop from good mode to bad in under 10 seconds, double the amount of time ’t’ before bad mode goes back to good. Clamp this at some maximum, say 60 seconds. To avoid punishing good connections when they have short periods of bad behavior, for each 10 seconds the connection is in good mode, halve the time ’t’ before bad mode goes back to good. Clamp this at some minimum like 1 second. With this algorithm you will rapidly respond to bad conditions and drop your send rate to 10 packets per-second, avoiding flooding of the connection. You’ll also conservatively try out good mode, and persist sending packets at a higher rate of 30 packets per-second, while network conditions are good. Of course, you can implement much more sophisticated algorithms. Packet loss % can be taken into account as a metric, even the amount of network jitter (time variance in packet acks), not just RTT. You can also get much more greedy with congestion avoidance, and attempt to discover when you can send data at a much higher bandwidth (eg. LAN), but you have to be very careful! With increased greediness comes more risk that you’ll flood the connection. ConclusionOur new reliability system let’s us send a steady stream of packets and notifies us which packets are received. From this we can infer lost packets, and resend data that didn’t get through if necessary. We also have a simple congestion avoidance system that drops from 30 packets per-second to 10 times a second so we don’t flood the connection. 译文译文出处 翻译：艾涛（轻描一个世界） 审校：黄威（横写丶意气风发） 简介嗨，我是格伦-菲德勒，欢迎来到我的游戏程序员网络设计文章系列的第四篇。 在之前的文章里，我们将我们的虚拟连接的概念加入到UDP之上。 现在我们将要给我们的虚拟UDP连接增加可靠性，排序和避免拥堵。 这是迄今为止底层游戏网络设计中最复杂的一面，因此这将是一篇极其热情的文章，跟上我启程出发！ TCP的问题熟悉TCP的你们知道它已经有了自己关于连接、可靠性、排序和避免拥堵的概念，那么为什么我们还要重写我们自己的迷你版本的基于UDP的TCP呢？ 问题是多人动作游戏依靠于一个稳定的每秒发送10到30包的数据包流，而且在大多数情况下，这些数据包中包含的数据对时间是如此敏感以至于只有最新的数据才是有用的。这包括玩家的输入，位置方向和每个玩家角色的速度以及游戏世界中物理对象的状态等数据。 TCP的问题是它提取的是以可靠有序的数据流发送的数据。正因为如此，如果一个数据包丢失了，TCP不得不停止以等待那个数据包重新发送，这打断了这个稳定的数据包流因为更多的最新的数据包在重新发送的数据包到达之前必须在队列中等待，所以数据包必须有序地提供。 我们需要的是一种不同类型的可靠性。我们想要以一个稳定的速度发送数据包而且当数据被其他电脑接收到时我们会得到通知，而不是让所有的数据用一个可靠有效的数据流处理。这样的方法使得那些对时间敏感的数据能够不用等待重新发送的数据包就通过，而让我们自己拿主意怎么在应用层级去处理丢包。 具有TCP这些特性的系统是不可能实现可靠性的，因此我们别无选择只能在UDP的基础上自行努力。 不幸的是，可靠性并不是唯一一个我们必须重写的东西，这是因为TCP也提供避免拥堵功能，这样它就能够动态地衡量数据发送速率以来适应网络连接的性能。例如TCP在28.8k的调制调解器上会比在T1线路上发送更少的数据，而且它在不用事先知道这是什么类型的网络连接的情况下就能这么做！ 序列号现在回到可靠性！ 我们可靠性系统的目标很简单：我们想要知道哪些数据包到了网络连接的另一端。 首先我们得鉴别数据包。 如果我们添加一个“数据包id”的概念会怎么样？让我们先给id赋一个整数值。我们能够从零开始，然后随着我们每发送一个数据包，增加一个数值。我们发送的第一个数据包就是“包0”，发送的第100个数据包就是“包99”。 这实际上是一个相当普遍的技术。甚至于在TCP中也得到了应用！这些数据包id叫做序列号，然而我们并不打算像TCP那样去做来实现可靠性，使用相同的术语是有意义的，因此从现在起我们还将称之为序列号。 因为UDP并不能保证数据包的顺序，所以第100个收到的数据包并不一定是第100个发出的数据包。接下来我们需要在数据包中插入序列号这样网络连接另一端电脑便能够知道是哪个数据包。 我们在前一篇文章中已经有了一个简单的关于虚拟网络连接的数据头，因此我们将只需要像这样在数据头中插入序列号： 12345[uint protocol id][uint sequence](packet data…) 现在当其他电脑收到一个数据包时通过发送数据包的电脑它就能知道数据包的序列号啦。 应答系统既然我们已经能够使用序列号来鉴别数据包，下一步就该是让网络连接的另一端知道我们收到了哪个包了。 逻辑上来说这是非常简单的，我们只需要记录我们收到的每个包的序列号，然后把那些序列号发回发送他们的电脑即可。 因为我们是在两个机器间相互发送数据包，我们只能在数据包头添加上确认字符，就像我们加上序列号一样： 1234567[uint protocol id][uint sequence][uint ack](packet data…) 我们的一般方法如下： 每次我们发送一个数据包我们就增加本地序列号。 当我们接收一个数据包时，我们将这个数据包的序列号与最近收到的数据包的序列号(称之为远程序列号)进行核对。如果这个包时间更近，我们就更新远程序列号使之等于这个数据包的序列号。 当我们编写数据包头时，本地序列号就变成了数据包的序列号，而远程序列号则变成确认字符。 这个简单的应答系统工作条件是每当我们发出一个数据包就会接收到一个数据包。 但如果数据包一起发送这样在我们发送一个数据包之前有两个数据包到达该怎么办呢？我们每个数据包只留了一个确认字符的位置，那我们该怎么处理呢? 现在考虑网络连接中的一端用更快的速率发送数据包这种情况。如果客户端每秒发送30个数据包，而服务器每秒只发送10个数据包，这样从服务器发出的每个数据包我们至少需要3个确认字符。 让我们想得更复杂点！如果数据包留下来了而确认字符丢失了会怎么样？这样发送这个数据包的电脑会认为这个数据包已经丢失了而实际上它已经被收到了！ 貌似我们需要让我们的可靠性系统……更加可靠一点！ 可靠的应答系统这就是我们偏离TCP的地方。 TCP的做法是在确认字符发送的地方给下一个按顺序预期该收到的数据包序列号的位置维持一个移动窗口。如果TCP对于一个已经发出的数据包没有收到确认字符，它将暂停并重新发送那个对应序列号的数据包。这正是我们想要避免的做法！ 因此在我们的可靠性系统里，我们从不为一个已经发出的序列号重新发送数据包，我们精确地只排序一次n，然后我们发送n+1，n+2，依次类推。如果数据包n丢失了我们也从不暂停重新发送它，而是把它留给应用程序来编写一个包含丢失数据的新的数据包，必要的话，这个包还会用一个新的序列号发送。 因为我们工作的方式与TCP不同，它的做法现在可能在我们数据包的确定字符设置中有了个洞，因此现在仅仅陈述最近的数据包的序列号已经远远不够了。 我们需要在每个数据包中包含多个确认字符。 那我们需要多少确认字符呢? 正如之前提到网络连接的一端发包速率比另一端快的情况，让我们假定最糟的情况是一端每秒钟发送不少于10个数据包，而另一端每秒钟发送不多于30个数据包。这种情况下，我们每个数据包需要的平均确认字符数是3个，但是如果数据包发送密集点，我们将需要更多。让我们说6-10个最差的情况。 如果因为包含确认字符的数据包丢失而导致确认字符并没有到达怎么办? 为了解决这个问题，我们将要使用一种经典的使用冗余码的网络设计策略来处理数据包丢失的情况！ 让我们在每个数据包中容纳33个确认字符，而且这不仅是他将要达到33个，而是一直是33个。因此对于每一个发出的确认字符我们多余地把它额外多发送了多达32次，仅仅是以防某个包含确认字符的数据包不能通过！ 但是我们怎么可能在一个数据包里配置33个确认字符呢？每个确认字符4字节那就是132字节了！ 窍门是在“相应确认字符”之前使用一段位域来代表32个之前的确认字符，就像这样： 123456789[uint protocol id][uint sequence][uint ack][uint ack bitfield](packet data…) 我们这样规定“位域”中每一位对应“相应确认字符”之前的32个确认字符。因此让我们说“相应确认字符”是100。如果位域的第一位设置好了，那么这个数据包也包含包99的一个确认字符。如果第二位设置好了，那么它也包含包98的一个确认字符。这样一路下来就到了包68的第32位。 我们调整过的算法看起来就像这样: 每次我们发送一个数据包我们就增加本地序列号。 当我们接收一个数据包时，我们将这个数据包的序列号与最近收到的数据包的序列号(称之为远程序列号)进行核对。如果这个包是更新的，我们就更新远程序列号使之等于数据包的序列号。 当我们编写数据包头时，本地序列号就变成了数据包的序列号，而远程序列号则变成确认字符。 计算确认字符位域是通过寻找一个多达33个数据包的队列，其中包括在[远程序列号-32，远程序列号]范围内的序列号。如果序列号“远程序列号-n”正在接收队列中那就把确认字符位域中的位n（在[1，32]范围内）设置为位1。 此外，当一个数据包被接收了，确认字符位域也被扫描了，如果位n设置好了，那么即使它还没有被应答，我们也认可序列号“远程序列号-n”。 利用这个改善过的算法，你将可能不得不在不止一秒内丢掉100%的数据包而不是让一个数据包停止通过。当然，它能够轻松地处理不同的发包速率和接受一起发送的数据包。 检测丢包既然我们知道网络连接另一端接受的是哪些数据包，那么我们该怎么检测数据包的丢失呢? 这次的窍门是反过来想，如果你在一定时间内还没有收到某个数据包的应答，那么我们可以考虑说那个数据包已经丢失了。 考虑到我们正在以每秒不超过30包的速率发送数据包，而且我们正在多余地发送数据包大概三十次。如果你在一秒内没有收到某个数据包的确认字符，那很有可能就是这个数据包已经丢失了。 因此我们在这儿用了一些小窍门，尽管我们能100%确定哪个数据包通过了，但是我们只能适度地确定那些没有到达的数据包。 这种情况的复杂性在于任何你重新发送的使用了这种可靠性方法的数据需要有它自己的信息id，这样的话在你多次收到它的时候你可以放弃它。这在应用层级是能够做到的。 应对环绕式处理的序列号如果序列号没有环绕式处理覆盖，那么对于序列号和确认字符的讨论是不完整的！ 序列号和确认字符都是32比特的无符号整数，因此它们能够代表在范围[0，4294967295]内的数字。那是一个非常大的数字！那么大以至于如果你每秒发送三十个数据包也将要花费四年半来把这个序列号环绕式处理回零。 但是可能你想要节省一些带宽这样你将你的序列号和确认字符缩减到到16比特整数。你每个数据包节省了4个字节，但现在他们只需要在仅仅半个小时内即可完成环绕式处理！ 所以我们该怎么应对这种环绕式处理的情况呢? 诀窍是要认识到如果当前序列号已经非常高了，而且下一个到达的序列号很低，那么你就必须进行环绕式处理。那么即使新的序列号数值上比当前序列号值更低它也能实际代表一个更新的数据包。 举个例子，让我们假设我们用一个字节编码序列号（顺便说一下并不推荐这样做）。 :))， 之后他们就会在255后面进行环绕式处理，就像这样: 1… 252, 253, 254, 255, 0, 1, 2, 3,… 为了解决这种情况我们需要一个能够意识到在255之后需要环绕式处理回零这样一个事实的新功能，这样0，1，2，3就会被认为比255更新。否则，我们的可靠性系统就会在你收到包255后停止工作。 这就是那个新功能： 123456789bool sequence_more_recent( unsigned int s1, unsigned int s2, unsigned int max )&#123; return ( s1 &gt; s2 ) &amp;&amp; ( s1 - s2 &lt;= max/2 ) || ( s2 &gt; s1 ) &amp;&amp; ( s2 - s1 &gt; max/2 );&#125; 这个功能通过比较两个数字和他们的不同来工作。如果它们之间的差异少于1/2的最大序列号值，那么它们必须靠在一起– 因此我们只需要照常检查某个序列号是否比另一个大。然而，如果它们相差很多，它们之间的差异将会比1/2的最大序列号值大，那么如果它比当前序列号小我们反而认为这个序列号是更新的。 这最后一点是显然需要环绕式处理序列号的地方，那么0，1，2就会被认为比255更新。 多么简洁而巧妙！ 一定要确保你在你所做的任何序列号处理当中包含了这一步！ 避免拥堵当你已经解决了可靠性的问题的时候，还有避免拥堵的问题。当你获得TCP的可靠性的时候TCP已经提供了避免拥堵的功能作为数据包的一部分，但是UDP无论怎样都不会有避免拥堵！ 如果我们仅仅发送数据包而没有某种流量控制，我们正在冒险占满网络连接而且会引起严重的延迟（2秒以上！），正如我们和另外一台电脑之间的路由器会超负荷而缓冲数据包。这个发生是因为路由器很努力地想要尝试传送我们发送的所有数据包，因此在它们考虑丢弃数据包之前会在队列中缓冲数据包。 然而如果我们能告诉路由器我们的数据包是时间敏感的而且如果路由器超载的话这些数据包应该丢弃而不是缓冲这样会很棒的，但只有我们重写世界上所有路由器的软件才能做到这一点！ 那么我们反而需要把重点放在我们实际上能做的是避免占满首位网络连接。 做到这个的方法是实施我们自己的基础避免拥堵算法。我强调基础！就像可靠性，我们并不寄希望于像TCP第一次尝试应用那样普通而粗暴地想出某些东西，那么让我们让它尽可能简单吧。 衡量往返时间因为所有避免拥堵的要点就是避免占满网络连接和避免增加往返时间（RTT），关于我们是不是占满网络的最重要的衡量标准是RTT它本身的观点是有道理的。 我们需要一种方法来衡量我们网络连接的RTT。 这是基础的技巧： 对我们发送的每个数据包，我们对数据包队列中包含的序列号和他们发送的时间添加一个登记。 当我们收到一个应答时，我们找到这个登记, 然后记录我们收到这个应答的时间t1与我们发送数据包的时间的t2的差值(都基于本地时间来计算)。这就是是这个数据包的RTT时间。 因为数据包的到达因网络波动而不同，我们需要缓和这个值来提供某些有意义的东西，这样每次我们获得一个新的RTT我们就移动一个我们当前的RTT和数据包的RTT之间距离的百分比。10%在实践中看起来效果很好。这就叫做一个指数级平滑移动平均值，而且它在用一个低通滤波器的情况下能有效地平滑RTT中的杂音。 为了确保发送队列永不增长，一旦超过某些最大预期RTT值我们就丢弃数据包。正如上一节关于可靠性讨论过的，任何在一秒内未应答的数据包都极有可能丢失了，那么对于最大RTT来说，一秒是个很棒的值。 既然我们有RTT，我们能把它作为一个衡量标准来推动我们的避免拥堵功能。如果RTT变得太大了，我们更缓慢地发送数据，如果它的值低于可接受范围，我们能努力更频繁地发送数据。 简单的好坏机制避免拥堵正如之前讨论的，我们不要那么贪心，我们将要执行一个非常基础的避免拥堵机制。这个避免拥堵机制有两种模式。好和坏。我把它叫做简单的二进制避免拥堵。 让我们假设你在发送一个确定大小的数据包，就假设256字节吧。你想要每秒发送这些数据包30次，但是如果网络条件差，你可以削减为每秒10次。 那么30次256字节的数据包的速率大概是64kbits/sec，每秒10次的话大概20kbits/sec。世界上没有一个宽带连接不能处理至少20kbits/sec的速率，所以我们在这样的假定下继续前进。不像TCP这样对有任何数量的发送/接受带宽的任何设备都完全通用，我们将假设一个设备的最小支持带宽来参与我们的网络连接。 所以基础想法就是这样了。当网络条件好的时候我们每秒发送30个数据包，当网络条件差的时候我们降至每秒10个数据包。 当然，你能随你喜爱定义好和坏，但是仅考虑RTT的时候我已经得到了好的成效。举个例子，如果RTT超过某些极限值（假设250ms）那你就知道你可能已经正占满了网络连接。当然，这里假设一般没人在非占满网络条件下超过250ms，考虑到我们的宽带要求这是合理的。。 好和坏之间你会怎么转换？我喜欢用下列操作的算法: 如果你当前在好模式下，而网络条件突然变坏，立即降至坏模式。 如果你正在坏模式下，而且网络条件已经好了一段特定时长”t”，那么回到好模式。 为了避免好模式和坏模式之间的快速切换，如果你从好模式降至坏模式持续10秒钟以内，从坏模式回到好模式之前的时间是”t”的两倍。在某些最大值中固定这个时间值，假设60秒。 为了避免打击良好的网络连接，当它们有一小段时期的差连接时，每过10秒连接就处于好模式，把坏模式回到好模式之前的时间“t”减半。在某些最小值中固定这个时间值，例如1秒。 利用这个算法，你将对差网络连接迅速反应然后降低你的发送速率至每秒10个数据包，避免占满网络。在网络条件好时，你也将谨慎地尝试好模式，坚持以更高的每秒发送30个数据包的速率发送数据包。 当然，你也能实施复杂得多的算法，丢包率百分比甚至是网络波动（数据包确认字符的时间差异）都可以考虑作为一个衡量标准，而不仅仅是RTT。 对于避免拥堵你还可以更贪心点，并尝试发现什么时候你能以一个更高的带宽（例如LAN）发送数据，但是你必须非常小心！随着贪婪心的增加你占满网络连接的风险也在增大！ 结语我们全新的可靠性系统让我们稳定流畅发送数据包，而且能通知我们收到了什么数据包。从这我们能推断出丢失的数据包，必要的话重新发送没有通过的数据。 基于此我们有了能够取决于网络条件在每秒10次和每秒30次发包速率间轮流切换的一个简单的避免拥堵系统，因此我们不会占满网络连接。 还有很多实施细节因为太具体而不能在这篇文章一一提到，所以务必确保你检查示例源代码来看是否它都被实施了。 这就是关于可靠性，排序和避免拥堵的一切了，或许是低层次网络设计中最复杂的一面了。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发三之基于UDP的虚拟连接]]></title>
    <url>%2F2019%2F05%2F19%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%B8%89%E4%B9%8B%E5%9F%BA%E4%BA%8EUDP%E7%9A%84%E8%99%9A%E6%8B%9F%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[原文原文出处 IntroductionHi, I’m Glenn Fiedler and welcome to Networking for Game Programmers. In the previous article we sent and received packets over UDP. Since UDP is connectionless, one UDP socket can be used to exchange packets with any number of different computers. In multiplayer games however, we usually only want to exchange packets between a small set of connected computers. As the first step towards a general connection system, we’ll start with the simplest case possible: creating a virtual connection between two computers on top of UDP. But first, we’re going to dig in a bit deeper about how the Internet really works! The Internet NOT a series of tubesIn 2006, Senator Ted Stevens made internet history with his famous speech on the net neutrality act: “The internet is not something that you just dump something on. It’s not a big truck. It’s a series of tubes” When I first started using the Internet, I was just like Ted. Sitting in the computer lab in University of Sydney in 1995, I was “surfing the web” with this new thing called Netscape Navigator, and I had absolutely no idea what was going on. You see, I thought each time you connected to a website there was some actual connection going on, like a telephone line. I wondered, how much does it cost each time I connect to a new website? 30 cents? A dollar? Was somebody from the university going to tap me on the shoulder and ask me to pay the long distance charges? :) Of course, this all seems silly now. There is no switchboard somewhere that directly connects you via a physical phone line to the other computer you want to talk to, let alone a series of pneumatic tubes like Sen. Stevens would have you believe. No Direct ConnectionsInstead your data is sent over Internet Protocol (IP) via packets that hop from computer to computer. A packet may pass through several computers before it reaches its destination. You cannot know the exact set of computers in advance, as it changes dynamically depending on how the network decides to route packets. You could even send two packets A and B to the same address, and they may take different routes. On unix-like systems can inspect the route that packets take by calling “traceroute” and passing in a destination hostname or IP address. On windows, replace “traceroute” with “tracert” to get it to work. Try it with a few websites like this: 12345traceroute slashdot.org traceroute amazon.com traceroute google.com traceroute bbc.co.uk traceroute news.com.au Take a look and you should be able to convince yourself pretty quickly that there is no direct connection. How Packets Get DeliveredIn the first article, I presented a simple analogy for packet delivery, describing it as somewhat like a note being passed from person to person across a crowded room. While this analogy gets the basic idea across, it is much too simple. The Internet is not a flat network of computers, it is a network of networks. And of course, we don’t just need to pass letters around a small room, we need to be able to send them anywhere in the world. It should be pretty clear then that the best analogy is the postal service! When you want to send a letter to somebody you put your letter in the mailbox and you trust that it will be delivered correctly. It’s not really relevant to you how it gets there, as long as it does. Somebody has to physically deliver your letter to its destination of course, so how is this done? Well first off, the postman sure as hell doesn’t take your letter and deliver it personally! It seems that the postal service is not a series of tubes either. Instead, the postman takes your letter to the local post office for processing. If the letter is addressed locally then the post office just sends it back out, and another postman delivers it directly. But, if the address is is non-local then it gets interesting! The local post office is not able to deliver the letter directly, so it passes it “up” to the next level of hierarchy, perhaps to a regional post office which services cities nearby, or maybe to a mail center at an airport, if the address is far away. Ideally, the actual transport of the letter would be done using a big truck. Lets be complicated and assume the letter is sent from Los Angeles to Sydney, Australia. The local post office receives the letter and given that it is addressed internationally, sends it directly to a mail center at LAX. The letter is processed again according to address, and gets routed on the next flight to Sydney. The plane lands at Sydney airport where an entirely different postal system takes over. Now the whole process starts operating in reverse. The letter travels “down” the hierarchy, from the general, to the specific. From the mail hub at Sydney Airport it gets sent out to a regional center, the regional center delivers it to the local post office, and eventually the letter is hand delivered by a mailman with a funny accent. Crikey! :) Just like post offices determine how to deliver letters via their address, networks deliver packets according to their IP address. The low-level details of this delivery and the actual routing of packets from network to network is actually quite complex, but the basic idea is that each router is just another computer, with a routing table describing where packets matching sets of addresses should go, as well as a default gateway address describing where to pass packets for which there is no matching entry in the table. It is routing tables, and the physical connections they represent that define the network of networks that is the Internet. The job of configuring these routing tables is up to network administrators, not programmers like us. But if you want to read more about it, then this article from ars technica provides some fascinating insight into how networks exchange packets between each other via peering and transit relationships. You can also read more details about routing tables in this linux faq, and about the border gateway protocol on wikipedia, which automatically discovers how to route packets between networks, making the internet a truly distributed system capable of dynamically routing around broken connectivity. Virtual ConnectionsNow back to connections. If you have used TCP sockets then you know that they sure look like a connection, but since TCP is implemented on top of IP, and IP is just packets hopping from computer to computer, it follows that TCP’s concept of connection must be a virtual connection. If TCP can create a virtual connection over IP, it follows that we can do the same over UDP. Lets define our virtual connection as two computers exchanging UDP packets at some fixed rate like 10 packets per-second. As long as the packets are flowing, we consider the two computers to be virtually connected. Our connection has two sides: One computer sits there and listens for another computer to connect to it. We’ll call this computer the server. Another computer connects to a server by specifying an IP address and port. We’ll call this computer the client. In our case, we only allow one client to connect to the server at any time. We’ll generalize our connection system to support multiple simultaneous connections in a later article. Also, we assume that the IP address of the server is on a fixed IP address that the client may directly connect to. Protocol IDSince UDP is connectionless our UDP socket can receive packets sent from any computer. We’d like to narrow this down so that the server only receives packets sent from the client, and the client only receives packets sent from the server. We can’t just filter out packets by address, because the server doesn’t know the address of the client in advance. So instead, we prefix each UDP packet with small header containing a 32 bit protocol id as follows: 12[uint protocol id] (packet data…) The protocol id is just some unique number representing our game protocol. Any packet that arrives from our UDP socket first has its first four bytes inspected. If they don’t match our protocol id, then the packet is ignored. If the protocol id does match, we strip out the first four bytes of the packet and deliver the rest as payload. You just choose some number that is reasonably unique, perhaps a hash of the name of your game and the protocol version number. But really you can use anything. The whole point is that from the point of view of our connection based protocol, packets with different protocol ids are ignored. Detecting ConnectionNow we need a way to detect connection. Sure we could do some complex handshaking involving multiple UDP packets sent back and forth. Perhaps a client “request connection” packet is sent to the server, to which the server responds with a “connection accepted” sent back to the client, or maybe an “i’m busy” packet if a client tries to connect to server which already has a connected client. Or… we could just setup our server to take the first packet it receives with the correct protocol id, and consider a connection to be established. The client just starts sending packets to the server assuming connection, when the server receives the first packet from the client, it takes note of the IP address and port of the client, and starts sending packets back. The client already knows the address and port of the server, since it was specified on connect. So when the client receives packets, it filters out any that don’t come from the server address. Similarly, once the server receives the first packet from the client, it gets the address and port of the client from “recvfrom”, so it is able to ignore any packets that don’t come from the client address. We can get away with this shortcut because we only have two computers involved in the connection. In later articles, we’ll extend our connection system to support more than two computers in a client/server or peer-to-peer topology, and at this point we’ll upgrade our connection negotiation to something more robust. But for now, why make things more complicated than they need to be? Detecting DisconnectionHow do we detect disconnection? Well if a connection is defined as receiving packets, we can define disconnection as not receiving packets. To detect when we are not receiving packets, we keep track of the number of seconds since we last received a packet from the other side of the connection. We do this on both sides. Each time we receive a packet from the other side, we reset our accumulator to 0.0, each update we increase the accumulator by the amount of time that has passed. If this accumulator exceeds some value like 10 seconds, the connection “times out” and we disconnect. This also gracefully handles the case of a second client trying to connect to a server that has already made a connection with another client. Since the server is already connected it ignores packets coming from any address other than the connected client, so the second client receives no packets in response to the packets it sends, so the second client times out and disconnects. ConclusionAnd that’s all it takes to setup a virtual connection: some way to establish connection, filtering for packets not involved in the connection, and timeouts to detect disconnection. Our connection is as real as any TCP connection, and the steady stream of UDP packets it provides is a suitable starting point for a multiplayer action game. Now that you have your virtual connection over UDP, you can easily setup a client/server relationship for a two player multiplayer game without TCP. 译文译文出处 译者：张华栋(wcby) 审校：崔国军（飞扬971） 序言大家好，我是Glenn Fiedler，欢迎阅读《针对游戏程序员的网络知识》系列教程的第三篇文章。 在之前的文章中，我向你展示了如何使用UDP协议来发送和接收数据包。 由于UDP协议是无连接的传输层协议，一个UDP套接字可以用来与任意数目的不同电脑进行数据包交换。但是在多人在线网络游戏中，我们通常只需要在一小部分互相连接的计算机之间交换数据包。 作为实现通用连接系统的第一步，我们将从最简单的可能情况开始：创建两台电脑之间构建于UDP协议之上的虚拟连接。 但是首先，我们将对互联网到底是如何工作的进行一点深度挖掘！ 互联网不是一连串的管子在2006年，参议院特德·史蒂文斯(Ted Stevens) 用他关于互联网中立（netneutrality）法案的著名演讲创造了互联网的历史： ”互联网不是那种你随便丢点什么东西进去就能运行的东西。它不是一个大卡车。它是一连串的管子“ 当我第一次开始使用互联网的时候，我也像Ted一样无知。那是1995年，我坐在悉尼大学的计算机实验室里，在用一种叫做Netscape的网络浏览器（最早最热门的网页浏览工具）“在网上冲浪（surfing the web）“，那个时候我对发生了什么根本一无所知。 你看那个时候，我觉得每次连到一个网站上就一定有某个真实存在的连接在帮我们传递信息，就像电话线一样。那时候我在想，当我每次连到一个新的网站上需要花费多少钱? 30美分吗?一美元吗? 会有大学里的某个人过来拍拍我的肩膀让我付长途通信的费用么？ 当然，现在回头看那时候一切的想法都非常的愚蠢。 并没有在某个地方存在一个物理交换机用物理电话线将你和你希望通话的某个电脑直接连起来。更不用说像参议院史蒂文斯想让你相信的那样存在一串气压输送管。 没有直接的连接相反你的数据是基于IP协议(InternetProtocol)通过在电脑到电脑之间发送数据包来传递信息的。 一个数据包可能在到达它的目的地之前要经过几个电脑。你没有办法提前知道数据包会经过具体哪些电脑，因为它会依赖当前网络的情况对数据包进行路由来动态的改变路径。甚至有可能给同一个地址发送A和B两个数据包，这两个数据包都采用不同的路由。这就是为什么UDP协议不能保证数据包的到达顺序。（其实这么说稍微容易有点引起误解，TCP协议是能保证数据包的到达顺序的，但是他也是基于IP协议进行数据包的发送，并且往同一个地址发送的两个数据包也有可能采用完全不同的路由，这主要是因为TCP在自己这一层做了一些控制而UDP没有，所以导致TCP协议可以保证数据包的有序性，而UDP协议不能，当然这种保证需要付出性能方面的代价）。在类unix的系统中可以通过调用“traceroute”函数并传递一个目的地主机名或IP地址来检查数据包的路由。 在Windows系统中，可以用“tracert”代替“traceroute”，其他不变，就能检查数据包的路由了。 像下面这样用一些网址来尝试下这种方法： 123456789traceroute slashdot.orgtraceroute amazon.comtraceroute google.comtraceroute bbc.co.uktraceroute news.com.au 运行下看下输出结果，你应该很快就能说服你自己确实连接到了网站上，但是并没有一个直接的连接。 数据包是如何传递到目的地的？在第一篇文章中，我对数据包传递到目的地这个事情做了一个简单的类比，把这个过程描述的有点像在一个拥挤的房间内一个人接着一个人的把便条传递下去。 虽然这个类比的基本思想还是表达出来了，但是它有点过于简单了。互联网并不是电脑组成的一个平面的网络，实际上它是网络的网络。当然，我们不只是要在一个小房间里面传递信件，我们要做的事能够把信息传递到全世界。 这就应该很清楚了，数据包传递到目的地的最好的类比是邮政服务! 当你想给某人写信的时候，你会把你的信件放到邮箱里并且你相信它将正确的传递到目的地。这封信件具体是怎么到达目的地的和你并不是十分相关，尽管它是否正确到达会对你有影响。当然会有某个人在物理上帮你把信件传递到目的地，所以这是怎么做的呢? 首先，邮递员肯定不需要自己去把你的信件送到目的地！看起来邮政服务也不是一串管子。相反，邮递员是把你的信件带到当地的邮政部门进行处理。 如果这封信件是发送给本地的，那么邮政部门就会把这封信件发送回来，另外一个邮递员会直接投递这封信件。但是，如果这封信件不是发送给本地的，那么这个处理过程就有意思了！当地的邮政部门不能直接投递这封信件，所以这封信件会被向上传递到层次结构的上一层，这个上一层也许是地区级的邮政部门它会负责服务附近的几个城市，如果要投递的地址非常远的话，这个上一层也许是位于机场的一个邮件中心。理想情况下，信件的实际运输将通过一个大卡车来完成。 让我们通过一个例子来把上面说的过程具体的走一遍，假设有一封信件要从洛杉矶发送到澳大利亚的悉尼。当地的邮政部门收到信件以后考虑到这封信件是一封跨国投递的信件，所以会直接把它发送到位于洛杉矶机场的邮件中心。在那里，这封信件会再次根据它的地址进行处理，并被安排通过下一个到悉尼的航班投递到悉尼去。 当飞机降落到悉尼机场以后，一个完全不同的邮政系统会负责接管这封信件。现在整个过程开始逆向操作。这封信件会沿着层次结构向下传递，从大的管理部门到具体的投递区域。这封信件会从悉尼机场的邮件中心被送往一个地区级的中心，然后地区级的中心会把这封信件投递到当地的邮政部门，最终这封信件会是由一个操着有趣的本地口音的邮政人员用手投递到真正的目的地的。哎呀! ! 就像邮局是通过信件的地址来决定这些信件是该如何投递的一样，网络也是根据这些数据包的IP地址来决定它们是该如何传递的。投递机制的底层细节以及数据包从网络到网络的实际路由其实都是相当复杂的，但是基本的想法都是一样的，就是每个路由器都只是另外一台计算机，它会携带一张路由表用来描述如果数据包的IP地址匹配了这张表上的某个地址集，那么这个数据包该如何传递，这张表还会记载着默认的网关地址，如果数据包的IP地址和这张路由表上的一个地址都匹配不上，那么这个数据包该传递到默认的网关地址那里。其实是路由表以及它们代表的物理连接定义了网络的网络，也就是互联网（互联网也被称为万维网）。 因特网于1969年诞生于美国。最初名为“阿帕网”（ARPAnet）是一个军用研究系统，后来又成为连接大学及高等院校计算机的学术系统，则已发展成为一个覆盖五大洲150多个国家的开放型全球计算机网络系统，拥有许多服务商。普通电脑用户只需要一台个人计算机用电话线通过调制解调器和因特网服务商连接，便可进入因特网。但因特网并不是全球唯一的互联网络。例如在欧洲，跨国的互联网络就有“欧盟网”（Euronet），“欧洲学术与研究网”（EARN），“欧洲信息网”（EIN），在美国还有“国际学术网”（BITNET），世界范围的还有“飞多网”（全球性的BBS系统）等。但这些网络其实根本就不需要知道，感谢IP协议的帮助，只要知道他们是可以互联互通的就可以。 这些路由表的配置工作是由网络管理员完成的，而不是由像我们这样的程序员来做。但是如果你想要了解这方面的更多内容， 那么来自ars technica的这篇文章将提供网络是如何在端与端之间互联来交换数据包以及传输关系方面一些非常有趣的见解。你还可以通过linux常见问题中路由表（routing tables）方面的文章以及维基百科上面的边界网关协议（border gateway protocol ）的解释来获得更多的细节。边界网关协议是用来自动发现如何在网络之间路由数据包的协议，有了它才真正的让互联网成为一个分布式系统，能够在不稳定的连接里面进行动态的路由。 边界网关协议（BGP）是运行于 TCP 上的一种自治系统的路由协议。 BGP 是唯一一个用来处理像因特网大小的网络的协议，也是唯一能够妥善处理好不相关路由域间的多路连接的协议。 BGP 构建在 EGP 的经验之上。 BGP 系统的主要功能是和其他的 BGP 系统交换网络可达信息。网络可达信息包括列出的自治系统（AS）的信息。这些信息有效地构造了 AS 互联的拓朴图并由此清除了路由环路，同时在 AS 级别上可实施策略决策。 虚拟的连接现在让我们回到连接本身。 如果你已经使用过TCP套接字，那么你会知道它们看起来真的像是一个连接，但是由于TCP协议是在IP协议之上实现的，而IP协议是通过在计算机之间进行跳转来传递数据包的，所以TCP的连接仍然是一个虚拟连接。 如果TCP协议可以基于IP协议建立虚拟连接，那么我们在UDP协议上所做的一切都可以应用于TCP协议上。 让我们给虚拟连接下个定义：两个计算机之间以某个固定频率比如说每秒10个数据包来交换UDP的数据包。只要数据包仍然在传输，我们就认为这两台计算机之间存在一个虚拟连接。 我们的连接有两侧： 一个计算机坐在那儿侦听是否有另一台计算机连接到它。我们称负责监听的这台计算机为服务器（server）。 另一台计算机会通过一个指定的IP地址和端口连接到一个服务器。我们称主动连接的这台电脑为客户端（client）。 在我们的场景里，我们只允许一个客户端在任意的时候连接到服务器。我们将在下一篇文章里面拓展我们的连接系统以支持多个客户端的同时连接。此外，我们假定服务器的IP地址是一个固定的IP地址，客户端可以随时直接连接上来。我们将在后面的文章里面介绍匹配（matchmaking）和NAT打穿（NATpunch-through）。 协议ID由于UDP协议是无连接的传输层协议，所以我们的UDP套接字可以接受来自任何电脑的数据包。 我们想要缩小接收数据包的范围，以便我们的服务器只接收那些从我们的客户端发送出来的数据包，并且我们的客户端只接收那些从我们的服务端发送出来的数据包。我们不能只通过地址来过滤我们的数据包，因为服务器没有办法提前知道客户端的地址。所以，我们会在每一个UDP数据包前面加上一个包含32位协议id的头,如下所示: 123[uint protocol id](packet data…) 协议ID只是一些独特的代表我们的游戏协议的数字。我们的UDP套接字收到的任意数据包首先都要检查数据包的首四位。如果它们和我们的协议ID不匹配的话，这个数据包就会被忽略。如果它们和我们的协议ID匹配的话，我们会剔除数据包的第一个四个字节并把剩下的部分发给我们的系统进行处理。 你只要选择一些非常独特的数字就可以了，这些数字可以是你的游戏名字和协议版本号的散列值。不过说真的，你可以使用任何东西。这种做法的重点是把我们的连接视为基于协议进行通信的连接，如果协议ID不同，那么这样的数据包将被丢弃掉。 检测连接现在我们需要一个方法来检测连接。 当然我们可以实现一些复杂的握手协议，牵扯到多个UDP数据包来回传递。比如说客户端发送一个”请求连接（request connection）“的数据包给服务器，当服务器收到这个数据包的时候会回应一个”连接接受（connection accepted）“的数据包给客户端，或者如果这个服务器已经有超过一个连接的客户端以后，会回复一个“我很忙（i’m busy）”的数据包给客户端。 或者。。我们可以设置我们的服务器，让它以它收到的第一个数据包的协议ID作为正确的协议ID，并在收到第一个数据包的时候就认为连接已经建立起来了。 客户端只是开始给服务器发送数据包，当服务器收到客户端发过来的第一个数据包的时候，它会记录下客户端的IP地址和端口号，然后开始给客户端回包。 客户端已经知道了服务器的地址和端口，因为这些信息是在连接的时候指定的。所以当客户端收到数据包的时候，它会过滤掉任何不是来自于服务器地址的数据包。同样的，一旦服务器收到客户端的第一个数据包，它就会从“recvfrom”函数里面得到客户端的地址和端口号，所以它也可以忽略任何不是发自客户端地址的数据包。 我们可以通过一个捷径来避开这个问题，因为我们的系统只有两台计算机会建立连接。在后面的文章里，我们将拓展我们的连接系统来支持超过两台计算机参与客户端/服务器或者端对端（peer-to-peer，p2p）网络模型，并且在那个时候我们会升级我们的连接协议方式来让它变得更加健壮。 但是现在，为什么我们要让事情变得超出需求的复杂度呢？（作者的意思是因为我们现在不需要解决这个问题，因为我们的场景是面对只有两台计算机的情况，所以我们可以先放过这个问题。） 检测断线的情况我们该如何检测断线（disconnection）的情况？ 那么，如果一个连接被定义为接收数据包，我们可以定义断线为收不到数据包。 为了检测什么时候开始我们收不到数据包，我们要记录上一次我们从连接的另外一侧收到数据包到现在过去了多少秒，我们在连接的两侧都做了这个事情。 每次我们从连接的另外一端收到数据包的时候，我们都会重置我们的计数器为0.0，每一次更新的时候我们都会把这次更新到上一次更新逝去的时间量加到计数器上。 如果计数器的值超过某一个值，比如说10秒，那么我们就认定这个连接“超时”了并且我们会断开连接。 这也可以很优雅的处理当服务器已经与一个客户端建立连接以后，有第二个客户端试图与服务器建立连接的情况。因为服务器已经建立了连接，它会忽略掉不是来自连接的客户端地址发出来的数据包，所以第二个客户端在发出了数据包以后得不到任何回应，这样它就会判断连接超时并断开连接。 总结而这一切都需要设置一个虚拟连接：用某种方法建立一个连接，过滤掉那些不是来自这个连接的数据包，并且如果发现连接超时就断开连接。 我们的连接就跟任何TCP连接一样真实，并且UDP数据包构成的稳定数据流为多人在线动作网络游戏提供了一个很好的起点。 我们还获得了一些互联网是如何路由数据包的见解。举个例子来说，我们现在知道UDP数据包有时候会在到达的时候是乱序的原因是因为它们在IP层传输的时候采用不同的路由！看下互联网的地图，你会不会对你的数据包能够到达正确的目的点感到非常的神奇？如果你想对这个问题进行更加深入的了解，维基百科上的这篇文章(Internet backbone)是一个很好的起点。 现在，既然你已经有了一个基于UDP协议的虚拟连接，你可以轻松的在两个玩家的多人在线游戏里面设置一个客户端/服务器关系而不需要使用TCP协议。 你可以在这篇文章的示例源代码（examplesource code ）找到一个具体实现。 这是一个简单的客户端/服务器程序，每秒交换30个数据包。你可以在任意你喜欢的机器上运行这个服务器，只要给它提供一个公共的IP地址就可以了，需要公共IP地址的原因是我们目前还不支持NAT打穿（NAT punch-through ）。 NAT穿越（NATtraversal）涉及TCP/IP网络中的一个常见问题，即在处于使用了NAT设备的私有TCP/IP网络中的主机之间建立连接的问题。 像这样来运行客户端： 1./Client 205.10.40.50 它会尝试连接到你在命令行输入的地址。如果你不输入地址的话，默认情况下它会连接到127.0.0.1。 当一个客户端已经与服务器建立连接的时候，你可以尝试用另外一个客户端来连接这个服务器，你会注意到这次连接的尝试失败了。这么设计是故意的。因为到目前为止，一次只允许一个客户端连接上服务器。 你也可以在客户端和服务器连接的状态下尝试停止客户端或者服务器，你会注意到10秒以后连接的另外一侧会判断连接超时并断开连接。当客户端超时的时候它会退到shell窗口，但是服务器会退到监听状态为下一次的连接做好准备。 预告下接下来的一篇文章的题目:《基于UDP的可靠、有序和拥塞避免的传输》，欢迎继续阅读。 如果你喜欢这篇文章的话，请考虑对我做一个小小的捐赠。捐款会鼓励我写更多的文章!（原文作者在原文的地址上提供了一个捐赠网址，有兴趣的读者可以在文章开始的地方找到原文地址进行捐赠） 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发二之数据的发送与接收]]></title>
    <url>%2F2019%2F05%2F19%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%BA%8C%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%91%E9%80%81%E4%B8%8E%E6%8E%A5%E6%94%B6%2F</url>
    <content type="text"><![CDATA[原文原文出处 IntroductionHi, I’m Glenn Fiedler and welcome to Networking for Game Programmers. In the previous article we discussed options for sending data between computers and decided to use UDP instead of TCP for time critical data. In this article I am going to show you how to send and receive UDP packets. BSD socketsFor most modern platforms you have some sort of basic socket layer available based on BSD sockets. BSD sockets are manipulated using simple functions like “socket”, “bind”, “sendto” and “recvfrom”. You can of course work directly with these functions if you wish, but it becomes difficult to keep your code platform independent because each platform is slightly different. So although I will first show you BSD socket example code to demonstrate basic socket usage, we won’t be using BSD sockets directly for long. Once we’ve covered all basic socket functionality we’ll abstract everything away into a set of classes, making it easy to you to write platform independent socket code. Platform specificsFirst let’s setup a define so we can detect what our current platform is and handle the slight differences in sockets from one platform to another: 12345678910111213// platform detection #define PLATFORM_WINDOWS 1 #define PLATFORM_MAC 2 #define PLATFORM_UNIX 3 #if defined(_WIN32) #define PLATFORM PLATFORM_WINDOWS #elif defined(APPLE) #define PLATFORM PLATFORM_MAC #else #define PLATFORM PLATFORM_UNIX #endif Now let’s include the appropriate headers for sockets. Since the header files are platform specific, we’ll use the platform #define to include different sets of files depending on the platform: 123456789101112#if PLATFORM == PLATFORM_WINDOWS #include &lt;winsock2.h&gt; #elif PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX #include &lt;sys/socket.h&gt; #include &lt;netinet/in.h&gt; #include &lt;fcntl.h&gt; #endif Sockets are built in to the standard system libraries on unix-based platforms so we don’t have to link to any additonal libraries. However, on Windows we need to link to the winsock library to get socket functionality. Here is a simple trick to do this without having to change your project or makefile: 123#if PLATFORM == PLATFORM_WINDOWS #pragma comment( lib, "wsock32.lib" ) #endif I like this trick because I’m super lazy. You can always link from your project or makefile if you wish. Initializing the socket layerMost unix-like platforms (including macosx) don’t require any specific steps to initialize the sockets layer, however Windows requires that you jump through some hoops to get your socket code working. You must call “WSAStartup” to initialize the sockets layer before you call any socket functions, and “WSACleanup” to shutdown when you are done. Let’s add two new functions: 123456789101112131415161718bool InitializeSockets() &#123; #if PLATFORM == PLATFORM_WINDOWS WSADATA WsaData; return WSAStartup( MAKEWORD(2,2), &amp;WsaData ) == NO_ERROR; #else return true; #endif &#125; void ShutdownSockets() &#123; #if PLATFORM == PLATFORM_WINDOWS WSACleanup(); #endif &#125; Now we have a platform independent way to initialize the socket layer. Creating a socketIt’s time to create a UDP socket, here’s how to do it: 123456789int handle = socket( AF_INET, SOCK_DGRAM, IPPROTO_UDP ); if ( handle &lt;= 0 ) &#123; printf( "failed to create socket\n" ); return false; &#125; Next we bind the UDP socket to a port number (eg. 30000). Each socket must be bound to a unique port, because when a packet arrives the port number determines which socket to deliver to. Don’t use ports lower than 1024 because they are reserved for the system. Also try to avoid using ports above 50000 because they used when dynamically assigning ports. Special case: if you don’t care what port your socket gets bound to just pass in “0” as your port, and the system will select a free port for you. 12345678910111213sockaddr_in address; address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons( (unsigned short) port ); if ( bind( handle, (const sockaddr) &amp;address, sizeof(sockaddr_in) ) &lt; 0 ) &#123; printf( "failed to bind socket\n" ); return false; &#125; Now the socket is ready to send and receive packets. But what is this mysterious call to “htons” in the code above? This is just a helper function that converts a 16 bit integer value from host byte order (little or big-endian) to network byte order (big-endian). This is required whenever you directly set integer members in socket structures. You’ll see “htons” (host to network short) and its 32 bit integer sized cousin “htonl” (host to network long) used several times throughout this article, so keep an eye out, and you’ll know what is going on. Setting the socket as non-blockingBy default sockets are set in what is called “blocking mode”. This means that if you try to read a packet using “recvfrom”, the function will not return until a packet is available to read. This is not at all suitable for our purposes. Video games are realtime programs that simulate at 30 or 60 frames per second, they can’t just sit there waiting for a packet to arrive! The solution is to flip your sockets into “non-blocking mode” after you create them. Once this is done, the “recvfrom” function returns immediately when no packets are available to read, with a return value indicating that you should try to read packets again later. Here’s how put a socket in non-blocking mode: 12345678910111213141516171819202122232425#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX int nonBlocking = 1; if ( fcntl( handle, F_SETFL, O_NONBLOCK, nonBlocking ) == -1 ) &#123; printf( "failed to set non-blocking\n" ); return false; &#125; #elif PLATFORM == PLATFORM_WINDOWS DWORD nonBlocking = 1; if ( ioctlsocket( handle, FIONBIO, &amp;nonBlocking ) != 0 ) &#123; printf( "failed to set non-blocking\n" ); return false; &#125; #endif Windows does not provide the “fcntl” function, so we use the “ioctlsocket” function instead. Sending packetsUDP is a connectionless protocol, so each time you send a packet you must specify the destination address. This means you can use one UDP socket to send packets to any number of different IP addresses, there’s no single computer at the other end of your UDP socket that you are connected to. Here’s how to send a packet to a specific address: 12345678910111213int sent_bytes = sendto( handle, (const char)packet_data, packet_size, 0, (sockaddr)&amp;address, sizeof(sockaddr_in) ); if ( sent_bytes != packet_size ) &#123; printf( "failed to send packet\n" ); return false; &#125; Important! The return value from “sendto” only indicates if the packet was successfully sent from the local computer. It does not tell you whether or not the packet was received by the destination computer. UDP has no way of knowing whether or not the the packet arrived at its destination! In the code above we pass a “sockaddr_in” structure as the destination address. How do we setup one of these structures? Let’s say we want to send to the address 207.45.186.98:30000 Starting with our address in this form: 12345unsigned int a = 207; unsigned int b = 45; unsigned int c = 186; unsigned int d = 98; unsigned short port = 30000; We have a bit of work to do to get it in the form required by “sendto”: 123456789unsigned int address = ( a &lt;&lt; 24 ) | ( b &lt;&lt; 16 ) | ( c &lt;&lt; 8 ) | d; sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_addr.s_addr = htonl( address ); addr.sin_port = htons( port ); As you can see, we first combine the a,b,c,d values in range [0,255] into a single unsigned integer, with each byte of the integer now corresponding to the input values. We then initialize a “sockaddr_in” structure with the integer address and port, making sure to convert our integer address and port values from host byte order to network byte order using “htonl” and “htons”. Special case: if you want to send a packet to yourself, there’s no need to query the IP address of your own machine, just pass in the loopback address 127.0.0.1 and the packet will be sent to your local machine. Receiving packetsOnce you have a UDP socket bound to a port, any UDP packets sent to your sockets IP address and port are placed in a queue. To receive packets just loop and call “recvfrom” until it fails with EWOULDBLOCK indicating there are no more packets to receive. Since UDP is connectionless, packets may arrive from any number of different computers. Each time you receive a packet “recvfrom” gives you the IP address and port of the sender, so you know where the packet came from. Here’s how to loop and receive all incoming packets: 1234567891011121314151617181920212223242526272829303132while ( true ) &#123; unsigned char packet_data[256]; unsigned int max_packet_size = sizeof( packet_data ); #if PLATFORM == PLATFORM_WINDOWS typedef int socklen_t; #endif sockaddr_in from; socklen_t fromLength = sizeof( from ); int bytes = recvfrom( socket, (char)packet_data, max_packet_size, 0, (sockaddr)&amp;from, &amp;fromLength ); if ( bytes &lt;= 0 ) break; unsigned int from_address = ntohl( from.sin_addr.s_addr ); unsigned int from_port = ntohs( from.sin_port ); // process received packet &#125; Any packets in the queue larger than your receive buffer will be silently discarded. So if you have a 256 byte buffer to receive packets like the code above, and somebody sends you a 300 byte packet, the 300 byte packet will be dropped. You will not receive just the first 256 bytes of the 300 byte packet. Since you are writing your own game network protocol, this is no problem at all in practice, just make sure your receive buffer is big enough to receive the largest packet your code could possibly send. Destroying a socketOn most unix-like platforms, sockets are file handles so you use the standard file “close” function to clean up sockets once you are finished with them. However, Windows likes to be a little bit different, so we have to use “closesocket” instead: 123456#if PLATFORM == PLATFORM\_MAC || PLATFORM == PLATFORM\_UNIX close( socket ); #elif PLATFORM == PLATFORM\_WINDOWS closesocket( socket ); #endif Hooray windows. Socket classSo we’ve covered all the basic operations: creating a socket, binding it to a port, setting it to non-blocking, sending and receiving packets, and destroying the socket. But you’ll notice most of these operations are slightly platform dependent, and it’s pretty annoying to have to remember to #ifdef and do platform specifics each time you want to perform socket operations. We’re going to solve this by wrapping all our socket functionality up into a “Socket” class. While we’re at it, we’ll add an “Address” class to make it easier to specify internet addresses. This avoids having to manually encode or decode a “sockaddr_in” structure each time we send or receive packets. So let’s add a socket class: 1234567891011121314151617181920212223242526class Socket &#123; public: Socket(); ~Socket(); bool Open( unsigned short port ); void Close(); bool IsOpen() const; bool Send( const Address &amp; destination, const void data, int size ); int Receive( Address &amp; sender, void * data, int size ); private: int handle; &#125;; and an address class: 1234567891011121314151617181920212223242526272829class Address &#123; public: Address(); Address( unsigned char a, unsigned char b, unsigned char c, unsigned char d, unsigned short port ); Address( unsigned int address, unsigned short port ); unsigned int GetAddress() const; unsigned char GetA() const; unsigned char GetB() const; unsigned char GetC() const; unsigned char GetD() const; unsigned short GetPort() const; private: unsigned int address; unsigned short port; &#125;; Here’s how to to send and receive packets with these classes: 123456789101112131415161718192021222324252627282930313233// create socket const int port = 30000; Socket socket; if ( !socket.Open( port ) ) &#123; printf( "failed to create socket!\n" ); return false; &#125; // send a packet const char data[] = "hello world!"; socket.Send( Address(127,0,0,1,port), data, sizeof( data ) ); // receive packets while ( true ) &#123; Address sender; unsigned char buffer[256]; int bytes_read = socket.Receive( sender, buffer, sizeof( buffer ) ); if ( !bytes_read ) break; // process packet &#125; As you can see it’s much simpler than using BSD sockets directly. As an added bonus the code is the same on all platforms because everything platform specific is handled inside the socket and address classes. ConclusionYou now have a platform independent way to send and receive packets. Enjoy :) 译文译文出处]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发一之TCPvsUDP]]></title>
    <url>%2F2019%2F05%2F18%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%B8%80%E4%B9%8BTCPvsUDP%2F</url>
    <content type="text"><![CDATA[原文原文出处 IntroductionHi, I’m Glenn Fiedler and welcome to Networking for Game Programmers. In this article we start with the most basic aspect of network programming: sending and receiving data over the network. This is perhaps the simplest and most basic part of what network programmers do, but still it is quite intricate and non-obvious as to what the best course of action is. You have most likely heard of sockets, and are probably aware that there are two main types: TCP and UDP. When writing a network game, we first need to choose what type of socket to use. Do we use TCP sockets, UDP sockets or a mixture of both? Take care because if you get this wrong it will have terrible effects on your multiplayer game! The choice you make depends entirely on what sort of game you want to network. So from this point on and for the rest of this article series, I assume you want to network an action game. You know, games like Halo, Battlefield 1942, Quake, Unreal, CounterStrike and Team Fortress. In light of the fact that we want to network an action game, we’ll take a very close look at the properties of each protocol, and dig a bit into how the internet actually works. Once we have all this information, the correct choice is clear. TCP/IPTCP stands for “transmission control protocol”. IP stands for “internet protocol”. Together they form the backbone for almost everything you do online, from web browsing to IRC to email, it’s all built on top of TCP/IP. If you have ever used a TCP socket, then you know it’s a reliable connection based protocol. This means you create a connection between two machines, then you exchange data much like you’re writing to a file on one side, and reading from a file on the other. TCP connections are reliable and ordered. All data you send is guaranteed to arrive at the other side and in the order you wrote it. It’s also a stream protocol, so TCP automatically splits your data into packets and sends them over the network for you. IPThe simplicity of TCP is in stark contrast to what actually goes on underneath TCP at the IP or “internet protocol” level. Here there is no concept of connection, packets are simply passed from one computer to the next. You can visualize this process being somewhat like a hand-written note passed from one person to the next across a crowded room, eventually, reaching the person it’s addressed to, but only after passing through many hands. There is also no guarantee that this note will actually reach the person it is intended for. The sender just passes the note along and hopes for the best, never knowing whether or not the note was received, unless the other person decides to write back! Of course IP is in reality a little more complicated than this, since no one computer knows the exact sequence of computers to pass the packet along to so that it reaches its destination quickly. Sometimes IP passes along multiple copies of the same packet and these packets make their way to the destination via different paths, causing packets to arrive out of order and in duplicate. This is because the internet is designed to be self-organizing and self-repairing, able to route around connectivity problems rather than relying on direct connections between computers. It’s actually quite cool if you think about what’s really going on at the low level. You can read all about this in the classic book TCP/IP Illustrated. UDPInstead of treating communications between computers like writing to files, what if we want to send and receive packets directly? We can do this using UDP. UDP stands for “user datagram protocol” and it’s another protocol built on top of IP, but unlike TCP, instead of adding lots of features and complexity, UDP is a very thin layer over IP. With UDP we can send a packet to a destination IP address (eg. 112.140.20.10) and port (say 52423), and it gets passed from computer to computer until it arrives at the destination or is lost along the way. On the receiver side, we just sit there listening on a specific port (eg. 52423) and when a packet arrives from _any_computer (remember there are no connections!), we get notified of the address and port of the computer that sent the packet, the size of the packet, and can read the packet data. Like IP, UDP is an unreliable protocol. In practice however, most packets that are sent will get through, but you’ll usually have around 1-5% packet loss, and occasionally you’ll get periods where no packets get through at all (remember there are lots of computers between you and your destination where things can go wrong…) There is also no guarantee of ordering of packets with UDP. You could send 5 packets in order 1,2,3,4,5 and they could arrive completely out of order like 3,1,2,5,4. In practice, packets tend to arrive in order most of the time, but you cannot rely on this! UDP also provides a 16 bit checksum, which in theory is meant to protect you from receiving invalid or truncated data, but you can’t even trust this, since 16 bits is just not enough protection when you are sending UDP packets rapidly over a long period of time. Statistically, you can’t even rely on this checksum and must add your own. So in short, when you use UDP you’re pretty much on your own! TCP vs. UDPWe have a decision to make here, do we use TCP sockets or UDP sockets? Lets look at the properties of each: TCP: Connection based Guaranteed reliable and ordered Automatically breaks up your data into packets for you Makes sure it doesn’t send data too fast for the internet connection to handle (flow control) Easy to use, you just read and write data like its a file UDP: No concept of connection, you have to code this yourself No guarantee of reliability or ordering of packets, they may arrive out of order, be duplicated, or not arrive at all! You have to manually break your data up into packets and send them You have to make sure you don’t send data too fast for your internet connection to handle If a packet is lost, you need to devise some way to detect this, and resend that data if necessary You can’t even rely on the UDP checksum so you must add your own The decision seems pretty clear then, TCP does everything we want and its super easy to use, while UDP is a huge pain in the ass and we have to code everything ourselves from scratch. So obviously we just use TCP right? Wrong! Using TCP is the worst possible mistake you can make when developing a multiplayer game! To understand why, you need to see what TCP is actually doing above IP to make everything look so simple. How TCP really worksTCP and UDP are both built on top of IP, but they are radically different. UDP behaves very much like the IP protocol underneath it, while TCP abstracts everything so it looks like you are reading and writing to a file, hiding all complexities of packets and unreliability from you. So how does it do this? Firstly, TCP is a stream protocol, so you just write bytes to a stream, and TCP makes sure that they get across to the other side. Since IP is built on packets, and TCP is built on top of IP, TCP must therefore break your stream of data up into packets. So, some internal TCP code queues up the data you send, then when enough data is pending the queue, it sends a packet to the other machine. This can be a problem for multiplayer games if you are sending very small packets. What can happen here is that TCP may decide it’s not going to send data until you have buffered up enough data to make a reasonably sized packet to send over the network. This is a problem because you want your client player input to get to the server as quickly as possible, if it is delayed or “clumped up” like TCP can do with small packets, the client’s user experience of the multiplayer game will be very poor. Game network updates will arrive late and infrequently, instead of on-time and frequently like we want. TCP has an option to fix this behavior called TCP_NODELAY. This option instructs TCP not to wait around until enough data is queued up, but to flush any data you write to it immediately. This is referred to as disabling Nagle’s algorithm. Unfortunately, even if you set this option TCP still has serious problems for multiplayer games and it all stems from how TCP handles lost and out of order packets to present you with the “illusion” of a reliable, ordered stream of data. How TCP implements reliabilityFundamentally TCP breaks down a stream of data into packets, sends these packets over unreliable IP, then takes the packets received on the other side and reconstructs the stream. But what happens when a packet is lost? What happens when packets arrive out of order or are duplicated? Without going too much into the details of how TCP works because its super-complicated (please refer to TCP/IP Illustrated) in essence TCP sends out a packet, waits a while until it detects that packet was lost because it didn’t receive an ack (or acknowledgement), then resends the lost packet to the other machine. Duplicate packets are discarded on the receiver side, and out of order packets are resequenced so everything is reliable and in order. The problem is that if we were to send our time critical game data over TCP, whenever a packet is dropped it has to stop and wait for that data to be resent. Yes, even if more recent data arrives, that new data gets put in a queue, and you cannot access it until that lost packet has been retransmitted. How long does it take to resend the packet? Well, it’s going to take at least round trip latency for TCP to work out that data needs to be resent, but commonly it takes 2*RTT, and another one way trip from the sender to the receiver for the resent packet to get there. So if you have a 125ms ping, you’ll be waiting roughly 1/5th of a second for the packet data to be resent at best, and in worst case conditions you could be waiting up to half a second or more (consider what happens if the attempt to resend the packet fails to get through?). What happens if TCP decides the packet loss indicates network congestion and it backs off? Yes it actually does this. Fun times! Never use TCP for time critical dataThe problem with using TCP for realtime games like FPS is that unlike web browsers, or email or most other applications, these multiplayer games have a real time requirement on packet delivery. What this means is that for many parts of a game, for example player input and character positions, it really doesn’t matter what happened a second ago, the game only cares about the most recent data. TCP was simply not designed with this in mind. Consider a very simple example of a multiplayer game, some sort of action game like a shooter. You want to network this in a very simple way. Every frame you send the input from the client to the server (eg. keypresses, mouse input controller input), and each frame the server processes the input from each player, updates the simulation, then sends the current position of game objects back to the client for rendering. So in our simple multiplayer game, whenever a packet is lost, everything has to stop and wait for that packet to be resent. On the client game objects stop receiving updates so they appear to be standing still, and on the server input stops getting through from the client, so the players cannot move or shoot. When the resent packet finally arrives, you receive this stale, out of date information that you don’t even care about! Plus, there are packets backed up in queue waiting for the resend which arrive at same time, so you have to process all of these packets in one frame. Everything is clumped up! Unfortunately, there is nothing you can do to fix this behavior, it’s just the fundamental nature of TCP. This is just what it takes to make the unreliable, packet-based internet look like a reliable-ordered stream. Thing is we don’t want a reliable ordered stream. We want our data to get as quickly as possible from client to server without having to wait for lost data to be resent. This is why you should never use TCP when networking time-critical data! Wait? Why can’t I use both UDP and TCP?For realtime game data like player input and state, only the most recent data is relevant, but for other types of data, say perhaps a sequence of commands sent from one machine to another, reliability and ordering can be very important. The temptation then is to use UDP for player input and state, and TCP for the reliable ordered data. If you’re sharp you’ve probably even worked out that you may have multiple “streams” of reliable ordered commands, maybe one about level loading, and another about AI. Perhaps you think to yourself, “Well, I’d really not want AI commands to stall out if a packet is lost containing a level loading command - they are completely unrelated!”. You are right, so you may be tempted to create one TCP socket for each stream of commands. On the surface, this seems like a great idea. The problem is that since TCP and UDP are both built on top of IP, the underlying packets sent by each protocol will affect each other. Exactly how they affect each other is quite complicated and relates to how TCP performs reliability and flow control, but fundamentally you should remember that TCP tends to induce packet loss in UDP packets. For more information, read this paper on the subject. Also, it’s pretty complicated to mix UDP and TCP. If you mix UDP and TCP you lose a certain amount of control. Maybe you can implement reliability in a more efficient way that TCP does, better suited to your needs? Even if you need reliable-ordered data, it’s possible, provided that data is small relative to the available bandwidth to get that data across faster and more reliably that it would if you sent it over TCP. Plus, if you have to do NAT to enable home internet connections to talk to each other, having to do this NAT once for UDP and once for TCP (not even sure if this is possible…) is kind of painful. ConclusionMy recommendation is not only that you use UDP, but that you only use UDP for your game protocol. Don’t mix TCP and UDP! Instead, learn how to implement the specific features of TCP that you need inside your own custom UDP based protocol. Of course, it is no problem to use HTTP to talk to some RESTful services while your game is running. I’m not saying you can’t do that. A few TCP connections running while your game is running isn’t going to bring everything down. The point is, don’t split your game protocol across UDP and TCP. Keep your game protocol running over UDP so you are fully in control of the data you send and receive and how reliability, ordering and congestion avoidance are implemented. The rest of this article series show you how to do this, from creating your own virtual connection on top of UDP, to creating your own reliability, flow control and congestion avoidance. 译文译文出处]]></content>
      <categories>
        <category>Multiplayer</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
        <tag>TCP</tag>
      </tags>
  </entry>
</search>
